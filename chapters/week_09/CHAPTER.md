# Week 09：回归不是预测工具,而是理解关系的工具

> "All models are wrong, but some are useful."
> — George Box

2025 年到 2026 年,随着 AutoML 工具的普及,一个危险的误解正在扩散:**"AI 能自动选择最好的模型,我只需要看准确率"**。问题在于:AutoML 确实能快速拟合数百个模型,但它不会自动告诉你——模型假设是否满足、系数是否可解释、异常点是否影响结论。AutoML 最常见的缺陷是"**黑箱模型**":用户得到一个高 R² 的模型,但不知道哪些特征重要、系数如何解释、假设是否违反。所以本周你要学的,不是"让 AI 替你跑回归",而是**建立一套自己的模型诊断能力**。AI 可以加速拟合,但模型假设验证、系数解释、异常点分析的责任由你承担。

---

## 前情提要

上周(Week 08),小北学会了用区间估计和重采样量化不确定性。他兴冲冲地对老潘说:"**我现在不只是说'均值是 315 元',而是说'95% CI 是 [280, 350]',还可以用 Bootstrap 和置换检验验证结论!**"

老潘看了看他的报告,问了一个新问题:"**那你知道'是什么因素影响消费金额'吗?**"

小北愣住了:"呃……我可以做分组比较,看看不同城市、不同性别的差异……"

"分组比较只能告诉你'有没有差异',"老潘说,"**但回归分析能告诉你'变量之间的关系有多强'、'每增加一个单位,消费会变化多少'**。更重要的是,回归不只是拟合,更要检查假设——残差诊断、异常点影响、多重共线性。"

阿码举手:"**所以 Week 04 的相关分析只是'关系的强度',回归是'关系的形状和影响'?**"

"对!"老潘点头,"**相关告诉你'两个变量一起变',回归告诉你'一个变量如何随着另一个变量变化'**。而且,Week 08 的置信区间和 Bootstrap 也能用在回归系数上——告诉你'这个影响有多确定'。"

"还有,"老潘补充,"**本周你会用到更多之前学过的工具:**Week 01 的**统计三问**帮你明确是'描述'还是'推断';Week 02 的**集中趋势和离散程度**让你理解系数代表的'典型水平';Week 03 的**异常值检测**会用到 Cook's 距离识别极端点。"

"所以,"小北说,"**这不就是个大号的工具箱升级吗?**"

"没错,"老潘笑了,"**回归分析就是把所有工具串起来,回答更复杂的问题。**"

这正是本周要解决的问题——**回归分析与模型诊断**。你不再只是比较两组,而是用变量预测变量;你不再只报告 R²,而是诊断模型假设、检查异常点、解释系数边界。

---

## 学习目标

完成本周学习后,你将能够:
1. 理解线性回归的核心思想(最小二乘法),区分简单回归与多元回归
2. 正确解释回归系数的含义("在其他变量不变的情况下"),并构造系数的置信区间
3. 掌握残差诊断的核心方法(残差图、QQ 图、同方差性检验),判断模型假设是否满足
4. 识别多重共线性的风险(VIF、系数不稳定),并学会变量选择策略
5. 理解回归假设(LINE:线性、独立性、正态性、等方差),并知道如何验证每一条
6. 在 StatLab 报告中添加回归分析章节,包含系数解释、残差诊断、异常点分析
7. 审查 AI 生成的回归报告,识别缺少诊断、误解释系数、忽略假设等问题

---

<!--
贯穿案例:房价预测——从"画散点图"到"建立可解释的回归模型"

本周贯穿案例是一个房价预测场景:某城市的房地产数据,你想回答"哪些因素影响房价"以及"每增加一平米,房价涨多少"。读者需要用回归分析建立模型,并检查假设、诊断问题、解释边界。

- 第 1 节:从散点图到回归线 → 案例从"看到面积和房价正相关"变成"拟合第一条回归线 y = a + bx"
- 第 2 节:最小二乘法的直觉 → 案例从"直觉画线"变成"理解为什么最小化残差平方和"
- 第 3 节:回归系数的解释 → 案例从"斜率是 1.2"变成"面积每增加 1 平米,房价涨 1.2 万(保持其他变量不变)"
- 第 4 节:多元回归与多重共线性 → 案例从"简单回归"变成"加入房间数、房龄等多个变量,并检查 VIF"
- 第 5 节:残差诊断 → 案例从"只看 R²"变成"检查残差是否有模式、QQ 图是否偏离直线、方差是否常数"
- 第 6 节:异常点与影响点 → 案例从"假设所有点都听话"变成"识别 Cook's 距离大的异常点,并评估其影响"

最终成果:读者完成一个完整的回归分析,产出:
- 1 个回归模型(statsmodels 或 scikit-learn)
- 1 张系数表(含系数、标准误、t 值、p 值、95% CI)
- 1 组残差诊断图(残差 vs 拟合值、QQ 图、尺度-位置图)
- 1 个 Cook's 距离图(标注影响点)
- 1 份回归分析报告(系数解释、假设检验、局限性讨论)
- 1 份 AI 回归报告的审查清单(标注缺少诊断、误解释等问题)

认知负荷预算:
- 本周新概念(5 个,预算上限 5 个):
  1. 线性回归 - 应用层次
  2. 最小二乘法 - 理解层次
  3. 残差诊断 - 分析层次
  4. 多重共线性 - 理解层次
  5. 回归假设 - 分析层次
- 结论:✅ 在预算内(5 个)

回顾桥设计(至少 2 个,来自 Week 04-08):
- [相关分析](来自 week_04):在第 1 节,用"相关系数衡量关系强度"引出"回归刻画关系形状"
- [置信区间](来自 week_08):在第 3 节,用"回归系数的 95% CI"连接 Week 08 的区间估计方法
- [Bootstrap](来自 week_08):在第 5 节,用"Bootstrap 置信区间"验证回归系数的稳定性
- [假设检验](来自 week_06/07):在第 5 节,用"检验残差正态性"连接 Week 06 的 Shapiro-Wilk 检验
- [缺失值机制](来自 week_03):在第 4 节,用"回归中的缺失值处理"连接 Week 03 的 MCAR/MAR/MNAR

AI 小专栏规划:

AI 小专栏 #1(放在第 1-2 节之后):
- 主题:AI 时代的 AutoML——拟合变得简单,诊断依然要人做
- 连接点:与第 1 节"回归不是预测工具,而是理解关系的工具"和第 2 节"最小二乘法"呼应,讨论 AI 工具(AutoML、自动特征工程)让拟合变容易,但模型诊断、假设验证、系数解释仍需人类判断
- 建议搜索词:`AutoML statistics 2026`, `automated regression model diagnostics 2026`, `feature engineering AI 2026`

AI 小专栏 #2(放在第 4-5 节之间):
- 主题:回归系数的因果解释陷阱——"相关"不等于"因果"
- 连接点:与第 4 节"多元回归与多重共线性"和第 5 节"残差诊断"呼应,讨论 AI 时代常见错误:把回归系数当成因果效应,忽略混杂变量和反向因果
- 建议搜索词:`regression causality 2026`, `observational study causal inference 2026`, `confounding variable regression 2026`

角色出场规划:
- 小北(第 1、3、5 节):只看 R² 就下结论;误把回归系数当成"每增加一平米,所有人都涨 1.2 万";忽略残差诊断
- 阿码(第 2、4 节):追问"为什么用残差平方和而不是绝对值";好奇"多重共线性和相关系数的关系"
- 老潘(第 1、2、3、5、6 节):强调"回归不是预测工具,而是理解关系的工具";"最小二乘是几何直觉(投影)";"系数解释要加'在其他变量不变的情况下'";"残差图比 R² 更重要"

StatLab 本周推进:
- 上周状态:report.md 已有数据卡 + 描述统计 + 清洗日志 + EDA 叙事 + 假设清单 + 假设检验结果 + 不确定性量化(CI、Bootstrap、置换检验)
- 本周改进:在 report.md 中添加"回归分析"章节,包含:
  - 研究问题:哪些因素影响目标变量?(如消费金额、房价)
  - 简单回归:散点图 + 回归线 + 系数解释
  - 多元回归:多个预测变量 + 系数表 + VIF 检验
  - 残差诊断:残差图、QQ 图、同方差性检验
  - 异常点分析:Cook's 距离 + 影响点评估
  - 局限性讨论:假设是否满足、因果关系不能直接推断
- 涉及的本周概念:线性回归、最小二乘法、残差诊断、多重共线性、回归假设
- 建议示例文件:examples/99_statlab.py(生成回归分析报告与诊断图)
-->

## 你能预测这个数字吗?——从散点图到回归线

小北拿到了一份房价数据,里面有房屋面积(平米)和售价(万元)。他用 Week 04 学过的相关分析算了一下,相关系数是 0.75。"强相关!"他兴奋地说,"面积越大,房价越高,结论成立。"

老潘看了一眼他的散点图,问:"**你能告诉我'面积每增加 1 平米,房价涨多少'吗?**"

小北愣住了。相关系数 0.75 确实告诉他"有关系",但没说"关系是什么形状"。他需要的不只是一个数字,而是一条线——**回归线**(regression line)。

这就像天气预报说"明天和温度有关",但你真正想知道的是"温度每升高 1 度,降雨概率增加多少"。相关告诉你"有关系",回归告诉你"关系是什么形状"。

### 从散点图到"最佳拟合线"

你先画个散点图看看数据长什么样:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 假设 df 是你的房价数据
sns.scatterplot(data=df, x='area_sqm', y='price_wan')
plt.xlabel('面积(平米)')
plt.ylabel('售价(万元)')
plt.title('房价 vs 面积')
plt.show()
```

你会看到点大致沿着一条上升的线分布,但不是完美的——有的点在"线"的上方,有的在下方。小北说:"这画哪条线都行啊?"

老潘笑了:"确实不止一条线,但有一条'最好的'。问题是——你怎么定义'最好'?"

这条线的数学形式是:

**y = β₀ + β₁x + ε**

其中:
- **y**: 因变量(房价)
- **x**: 自变量(面积)
- **β₀**: 截距(intercept),当 x=0 时 y 的值
- **β₁**: 斜率(slope),x 每增加 1 单位,y 的平均变化量
- **ε**: 残差(residual),观测值与预测值的差异

### 用 Python 拟合第一条回归线

Week 04 你学过 **Pearson 相关系数**,它衡量"两个变量线性关系的强度"。但相关系数不会告诉你"线是什么"。回归分析则更进一步——它不只是告诉你"有关系",还告诉你"关系是什么形状"。

阿码问:"所以相关是'有没有',回归是'是什么样'?"

"对!"老潘说,"相关告诉你'两个变量一起跳舞',但回归告诉你'谁带着谁跳、跳什么舞步'。"

小北忍不住吐槽:"这比喻……我以后再也不会以同样的方式看散点图了。"

老潘笑了:"等你进入商业分析领域,会天天用这个:广告投入对销量的影响、价格对需求的影响、员工满意度对离职率的影响——都是回归问题。但你记住:**回归系数是'平均而言',不是'每次都准'**。"

```python
from sklearn.linear_model import LinearRegression

# 准备数据
X = df[['area_sqm']]  # sklearn 需要 2D 数组
y = df['price_wan']

# 拟合模型
model = LinearRegression()
model.fit(X, y)

# 查看系数
print(f"截距(β₀): {model.intercept_:.2f}")
print(f"斜率(β₁): {model.coef_[0]:.2f}")
```

假设输出是:
```
截距(β₀): 25.30
斜率(β₁): 1.18
```

这意味着:房价 = 25.30 + 1.18 × 面积。或者用自然语言:**面积每增加 1 平米,房价平均上涨 1.18 万元**。

### 把回归线画在散点图上

你可以把这条线可视化:

```python
# 预测值
y_pred = model.predict(X)

# 画图
sns.scatterplot(data=df, x='area_sqm', y='price_wan', label='观测值')
plt.plot(df['area_sqm'], y_pred, color='red', label='回归线')
plt.xlabel('面积(平米)')
plt.ylabel('售价(万元)')
plt.legend()
plt.title(f'房价 vs 面积 (y = {model.intercept_:.2f} + {model.coef_[0]:.2f}x)')
plt.show()
```

### 小北的错误:只看相关系数就下结论

小北看完图,说:"相关系数 0.75,斜率 1.18,说明面积越大房价越高。任务完成!"

老潘摇摇头:"**你只看到了'关系',没看到'噪音'**。"

什么是"噪音"?你把每个点到回归线的垂直距离算出来,这就是**残差**(residual):

```python
df['predicted'] = y_pred
df['residual'] = df['price_wan'] - df['predicted']

# 看看残差
print(df[['area_sqm', 'price_wan', 'predicted', 'residual']].head())
```

如果模型完美,所有残差都应该接近 0。但现实不是这样——有的房子比预测贵 20 万,有的便宜 15 万。这些"误差"不是你错了,而是**数据本身的随机性**。

老潘说:"**回归分析的第一课:模型永远不完美。你的任务不是让残差消失,而是理解残差里有没有模式**。如果残差随机分布,说明你的模型已经抓住了主要信号;如果残差有规律,说明还有重要变量被你漏掉了。"

这正好引出下一节:为什么我们用"残差平方和"作为拟合标准?

---

## 为什么要平方?——最小二乘法的几何直觉

阿码盯着代码,突然问了一个好问题:"**为什么最小化'残差平方和'?如果直接最小化'残差绝对值',不就不会受到异常值的影响了吗?**"

老潘笑了:"这是统计学史上争论了很久的问题。绝对值损失确实更稳健,但平方损失有一个好处——**它有完美的几何解释**。"

### 损失函数的选择:绝对值 vs 平方

假设你有 3 个观测点,想找一条"最佳拟合线"。你会怎么定义"最佳"?

**方法 1:最小化绝对值偏差(L1 损失)**
```
Loss = Σ|yᵢ - ŷᵢ|
```

**方法 2:最小化平方偏差(L2 损失,即最小二乘法 OLS)**
```
Loss = Σ(yᵢ - ŷᵢ)²
```

阿码的直觉是对的:**绝对值损失确实对异常值更稳健**。如果一个点偏离很远,平方会让它的影响力爆炸。

但为什么统计学家还是选择了平方?

"因为平方有个漂亮的几何解释,"老潘说,"想象你在高维空间里,OLS 就是把数据点'投影'到最近的直线上——就像影子一样。"

### OLS 的几何直觉:投影

老潘在白板上画了个图:"**想象数据在高维空间里形成一个'云',回归分析就是把这个云'投影'到一条直线上**。"

数学上,OLS 寻找的系数 β 满足:

**XᵀXβ = Xᵀy**

解这个方程得到:

**β = (XᵀX)⁻¹Xᵀy**

这个公式的几何意义是:**y 向量在 X 列空间上的正交投影**。

```python
import numpy as np

# 手动计算 OLS 系数(添加截距项)
X_with_intercept = np.column_stack([np.ones(len(X)), X])
beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y

print(f"手动计算的截距: {beta[0]:.2f}")
print(f"手动计算的斜率: {beta[1]:.2f}")
```

你会发现和 sklearn 的结果完全一致!

### 和 Week 02 的连接:均值也是"最小二乘"

还记得 Week 02 你学过的**均值**吗?你当时可能觉得"均值就是把所有数加起来除以个数",但它还有另一个身份——**最小二乘估计**:

**均值 = 最小化 Σ(xᵢ - μ)² 的 μ**

小北听到这里说:"等等,所以均值也是一条'回归线'?只是没有 x?"

"对!"老潘说,"回归只是把这个思想扩展到'带自变量'的场景。均值是'找一个数代表所有数据',回归是'找一条线代表所有数据的关系'。"

### 用 statsmodels 获得更详细的输出

sklearn 适合"预测",但如果你想做"统计推断"(p 值、置信区间),statsmodels 更方便:

```python
import statsmodels.api as sm

# 添加截距项(statsmodels 不会自动添加)
X_sm = sm.add_constant(X)

# 拟合模型
model_sm = sm.OLS(y, X_sm).fit()

# 打印详细报告
print(model_sm.summary())
```

你会看到一张大表,包含:
- **coef**: 系数估计值
- **std err**: 标准误(衡量系数的不确定性)
- **t**: t 统计量(coef / std err)
- **P>|t|**: p 值(检验"系数是否显著不为 0")
- **[0.025 0.975]**: 95% 置信区间

阿码看着这张表,问:"**p 值小于 0.05,说明什么?**"

老潘说:"说明'在面积对房价没有影响'的原假设下,你观测到这么大的斜率的概率很小'。但记住:**p 值不告诉你'影响有多大',只告诉你'影响是否存在'**。效应量(斜率本身)和置信区间比 p 值更重要。"

这正好连接到下一节:如何正确解释回归系数?

---

## "在其他变量不变的情况下"——回归系数的正确解释

小北看了输出,兴奋地说:"斜率是 1.18,说明**面积每增加 1 平米,房价涨 1.18 万**!我要在报告里写这个结论!"

老潘立刻打断:"**等等,你漏掉了半句话**。"

"半句话有那么重要吗?"小北不解。

### 回归系数的正确解释

对于简单回归 y = β₀ + β₁x,系数 β₁ 的正确解释是:

**"在其他变量不变的情况下,x 每增加 1 单位,y 的平均变化量是 β₁"**

"在其他变量不变的情况下"(ceteris paribus)这半句话极其重要,但经常被忽略。

小北不服:"但这里只有一个变量(面积),哪来的'其他变量'?"

"你的模型里只有一个变量,"老潘说,"但现实里有无数个——房龄、地段、装修、楼层、学区……那些'漏掉的变量'的影响都被吸进了残差里。所以正确的解释是:'**在模型包含的变量之外,其他条件保持不变**'。"

阿码若有所思:"所以每次我写报告,都要像律师一样严谨?"

"比律师还严谨,"老潘说,"因为律师只影响一个案子,你的结论可能影响成千上万的决策。"

### 一个常见的陷阱:混淆"个体"与"平均"

小北的另一个错误是:**他把"平均关系"当成了"个体规律"**。

回归系数说的是"平均而言",不是"每个人都是这样"。

```python
# 模拟一个例子:两套同样面积的房子
area_new = 100  # 100 平米
price_pred = model.intercept_ + model.coef_[0] * area_new

print(f"根据模型,100 平米的房子平均售价: {price_pred:.2f} 万元")
print(f"但实际售价可能在 [{price_pred - 20:.2f}, {price_pred + 20:.2f}] 万元之间")
```

为什么?因为残差!有些房子比预测贵,有些便宜,这是"无法用面积解释的差异"。

### 多元回归:更现实的场景

小北想了想:"**那我多加几个变量进去,不就能解释更多了吗?**"

对!这就是**多元回归**(multiple regression):

**y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε**

假设你加入"房龄"和"房间数":

```python
from sklearn.linear_model import LinearRegression

# 多个预测变量
X_multi = df[['area_sqm', 'age_years', 'n_rooms']]
y = df['price_wan']

# 拟合模型
model_multi = LinearRegression()
model_multi.fit(X_multi, y)

# 打印系数
for feature, coef in zip(X_multi.columns, model_multi.coef_):
    print(f"{feature}: {coef:.2f}")
print(f"截距: {model_multi.intercept_:.2f}")
```

假设输出是:
```
area_sqm: 0.85
age_years: -0.42
n_rooms: 5.30
截距: 18.20
```

### 多元回归系数的正确解释

现在"在其他变量不变的情况下"这句话更有意义了:

- **面积(0.85)**:在房龄和房间数不变的情况下,面积每增加 1 平米,房价平均上涨 0.85 万元
- **房龄(-0.42)**:在面积和房间数不变的情况下,房龄每增加 1 年,房价平均下跌 0.42 万元
- **房间数(5.30)**:在面积和房龄不变的情况下,每多一个房间,房价平均上涨 5.30 万元

阿码好奇:"**为什么面积的系数从 1.18 降到了 0.85?**"

老潘说:"因为**面积和房间数高度相关**——大房子通常房间多。简单回归里,面积'抢了'房间数的功劳;多元回归里,它们被'公平分配'了。"

"哦!所以简单回归的系数其实'混杂'了其他变量的影响?"

"对!这叫**遗漏变量偏差**(omitted variable bias)。你看到的'面积效应',其实可能是'面积+房间数'的共同效应。"

### 回归系数的置信区间(连接 Week 08)

Week 08 你学过**置信区间**,它告诉你"估计有多确定"。回归系数也需要置信区间:

```python
import statsmodels.api as sm

X_multi_sm = sm.add_constant(X_multi)
model_multi_sm = sm.OLS(y, X_multi_sm).fit()

# 查看 95% CI
print(model_multi_sm.conf_int(alpha=0.05))
```

假设面积的 95% CI 是 [0.62, 1.08],你的解释应该是:

**"我们有 95% 的把握认为,在其他变量不变的情况下,面积每增加 1 平米,房价平均上涨 0.62 到 1.08 万元之间。"**

小北看着这个区间,说:"**这比只说'0.85'诚实多了**。"

老潘点头:"对。**点估计是猜测,区间估计才是科学的语言**。"

---

## 你用了重复的变量吗?——多元回归与多重共线性

阿码把所有能想到的变量都扔进了模型:面积、房间数、卧室数、客厅数、卫生间数……结果发现了一些奇怪的事情——有的系数是负的(明明房间越多应该越贵),有的 p 值很大,但整体 R² 很高。

老潘看了一眼,说:"**你用了重复的变量**。"

"重复?"阿码不解,"它们明明是不同的列啊!"

### 什么是多重共线性?

**多重共线性**(multicollinearity)是指自变量之间存在高度相关,导致回归系数不稳定、难以解释的现象。

阿码的例子中:
- 房间数 ≈ 卧室数 + 客厅数 + 卫生间数
- 大房子通常有更多卧室、客厅、卫生间

当你把这些高度相关的变量同时放进模型,模型会"困惑":**到底是哪个变量在影响房价?** 它们"抢功劳"的结果就是——系数忽大忽小,甚至变成负数。

### 用 VIF 检测多重共线性

最常用的检测工具是**方差膨胀因子**(Variance Inflation Factor, VIF)。老潘说:"VIF 的名字很抽象,但记法很简单:**变量们'抱团取暖',让系数变得不稳定**。"

阿码笑了:"这比教科书上的'方差被膨胀'好记多了。"

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

# 计算 VIF(需要对每个变量单独拟合回归)
vif_data = pd.DataFrame()
vif_data["feature"] = X_multi.columns
vif_data["VIF"] = [variance_inflation_factor(X_multi.values, i)
                     for i in range(X_multi.shape[1])]

print(vif_data)
```

**判断标准**:
- VIF < 5: 共线性问题不大
- 5 ≤ VIF < 10: 中等共线性,需要关注
- VIF ≥ 10: 严重共线性,必须处理

阿码算了一下,发现"房间数"的 VIF 是 12.5。老潘说:"**这意味着房间数的方差被'膨胀'了 12.5 倍,系数估计极不稳定**。换一批数据,系数可能从 5.3 变成 -2.1 或 8.7。"

### 多重共线性的症状

如何判断你的模型有共线性问题?

小北看着自己的输出,困惑地问:"为什么'客厅数'的系数是负的?客厅越多房价反而越低?"

这正是多重共线性的典型症状。当你看到以下情况时,警报就要响了:

1. **系数符号反直觉**:比如"房间数越多,房价越低"(明明现实是正相关的)
2. **p 值大但 R² 高**:单个变量不显著,但整体模型拟合很好——这说明变量之间"互相抵消"
3. **加入/删除一个变量,其他系数剧变**:系数估计极其不稳定,换一批数据可能完全变样
4. **VIF > 10**:定量标准(下一节详细介绍)

### 处理多重共线性的策略

老潘给了阿码几个建议:

**策略 1:删除冗余变量**
```python
# 删除 VIF 最高的变量,重新计算
X_reduced = df[['area_sqm', 'age_years', 'n_bedrooms']]  # 删掉 n_rooms
```

**策略 2:合并变量**
```python
# 把卧室、客厅、卫生间合并成"总房间数"
df['total_rooms'] = df['n_bedrooms'] + df['n_living_rooms'] + df['n_bathrooms']
```

**策略 3:使用正则化(LASSO/Ridge)**
```python
from sklearn.linear_model import Lasso

# LASSO 会自动把不重要的变量系数压缩为 0
lasso = Lasso(alpha=0.1)
lasso.fit(X_multi, y)

print(dict(zip(X_multi.columns, lasso.coef_)))
```

阿码试着删除了 VIF > 10 的变量,重新拟合模型。这一次,系数稳定多了,VIF 都降到了 5 以下。

### 和 Week 04 的连接:相关矩阵是第一道防线

其实,Week 04 你学过的**相关矩阵**就能提前发现共线性问题:

```python
import seaborn as sns

# 计算相关矩阵
corr = X_multi.corr()

# 画热力图
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('自变量相关矩阵')
plt.show()
```

如果两个变量的相关系数 > 0.8,你就要小心了——**别把它们同时放进模型**。

老潘说:"**变量选择没有'标准答案',只有'合理的权衡'**。你需要结合三样东西:领域知识(哪些变量在现实中确实重要)、统计指标(VIF 告诉你哪些重复)和模型解释力(R² 告诉你预测准不准)。"

阿码问:"那如果两个变量都重要,但高度相关怎么办?"

"这就是艺术所在了,"老潘说,"你可以删一个、合并它们、或者用正则化方法让模型自动'取舍'。但无论怎么选,你都要在报告里写清楚'为什么选这些变量'。"

小北若有所思:"所以,**相关系数 > 0.8 的两个变量,就像两把都能开锁的钥匙——你只需要选一把**。老潘,你在公司里怎么处理这种情况?"

老潘说:"**业务逻辑优先**。比如'用户总访问时长'和'用户平均访问时长'——显然总时长包含了访问次数的影响,我更倾向保留总时长因为它直接反映粘性。但如果问题不明确,我会用 VIF 量化一下,再做决定。"

"这其实就是数据分析中的'层级决策',"老潘补充,"先问'哪些变量在业务上确实重要',再问'统计上有没有重复',最后才看'模型拟合好不好'。很多新手反过来,先把所有变量扔进去看 R²,这就像先买彩票再想怎么花钱——顺序错了。"

---

> **AI 时代小专栏:AI 时代的 AutoML——拟合变得简单,诊断依然要人做**

> 2025 年到 2026 年,AutoML 工具(AutoGluon、TPOT、H2O.ai)的普及让模型拟合变得极其简单:你上传数据,点击"开始",几分钟后就能得到数十个模型的对比——线性回归、决策树、随机森林、XGBoost,甚至神经网络。
>
> 但一个危险的趋势正在扩散:**很多人只看"准确率"或"R²",完全忽略模型诊断**。
>
> 2025 年的系统综述指出,AutoML 最常见的缺陷是"**黑箱模型**":
> > - 用户得到一个高 R² 的模型,但不知道哪些特征重要
> > - 系数如何解释?假设是否满足?这些问题 AutoML 不会自动回答
> > - 多重共线性、异常点影响、残差模式……都需要人类判断
>
> 即使 AutoML 能自动计算 SHAP 值和特征重要性,你仍然需要:
> > - **拟合是 AI 的事,诊断是你的事**
> > - 系数解释要加"在其他变量不变的情况下"
> > - 残差图比 R² 更重要
> > - 多重共线性会导致系数不稳定,AI 不会自动警告你
>
> 即使 AI 提供"特征重要性",你也需要问:
> > - 这个重要性是"全局的"(整体数据)还是"局部的"(单个样本)?
> > - 特征之间有交互作用吗?
> > - 高重要性不等于"因果关系",只是"预测有用"
>
> 所以本周你要学的,不是"让 AI 替你跑回归",而是**建立一套自己的模型诊断能力**。AI 可以加速拟合,但模型假设验证、系数解释、异常点分析的责任由你承担。
>
> 参考(访问日期:2026-02-12):
> > - [Automated machine learning with interpretation: A systematic review (Springer, 2024)](https://link.springer.com/article/10.1007/s10462-025-11397-2)
> > - [New Trends in Machine Learning (European Journal of AI, 2026)](https://www.sciencedirect.com/science/article/pii/S111686572500009X)
> > - [Automated Machine Learning in medical research (ScienceDirect, 2025)](https://www.sciencedirect.com/science/article/pii/S0895717725000012)

---

## 模型听话吗?——残差诊断与回归假设

小北终于拟合了一个"看起来不错"的模型:R² = 0.78,所有 p 值都 < 0.01,系数解释也很合理。他兴冲冲地准备写报告。

老潘看了一眼,问:"**你检查残差了吗?**"

小北愣住了:"残差?我不是算出来了吗?有什么好检查的?"

老潘说:"**残差不是用来算的,是用来诊断的**。"

小北困惑:"诊断什么?"

"回归模型有四个假设,你得逐条验证。如果假设不满足,你的 R² 和 p 值可能全是误导。"

### 回归的四大假设(LINE)

"LINE" 是回归假设的缩写:
- **L**inearity: 线性——因变量和自变量之间是线性关系
- **I**ndependence: 独立性——观测值之间相互独立
- **N**ormality: 正态性——残差服从正态分布
- **E**qual variance: 等方差——残差的方差在所有拟合值上相等

阿码笑了:"所以叫 LINE,就是这四个词的首字母?"

"对!"老潘说,"统计学家也爱记口诀。"

"好,"老潘话锋一转,"口诀好记,但每一条假设都得逐条验证。我们一个一个来。"

### 假设 1:线性——残差 vs 拟合值图

如何检验"线性"?画**残差 vs 拟合值图**:

```python
import statsmodels.api as sm

# 拟合模型
X_sm = sm.add_constant(df[['area_sqm', 'age_years']])
model = sm.OLS(df['price_wan'], X_sm).fit()

# 获取预测值和残差
fitted = model.fittedvalues
residuals = model.resid

# 画残差 vs 拟合值图
plt.scatter(fitted, residuals, alpha=0.6)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('拟合值')
plt.ylabel('残差')
plt.title('残差 vs 拟合值 (检验线性)')
plt.show()
```

**怎么判断?**
- ✅ 如果点随机散布在 y=0 线上下,无线性模式 → 线性假设满足
- ❌ 如果点呈现 U 型或倒 U 型 → 非线性关系,需要加入多项式项或变换变量

### 假设 2:独立性——Durbin-Watson 检验

独立性通常靠**研究设计**保证(不是随机抽样就可能违反,如时间序列、重复测量)。

statsmodels 的输出里有一个 **Durbin-Watson 统计量**:
- DW ≈ 2: 无自相关(独立性满足)
- DW < 2 或 > 2: 存在自相关(违反独立性)

```python
# 查看 DW 统计量
print(f"Durbin-Watson: {sm.stats.durbin_watson(residuals):.2f}")
```

### 假设 3:正态性——QQ 图与 Shapiro-Wilk 检验

残差应该近似正态分布。你可以用 Week 06 学过的 **QQ 图**和 **Shapiro-Wilk 检验**:

```python
from scipy.stats import shapiro, probplot

# QQ 图
fig, ax = plt.subplots(figsize=(6, 6))
probplot(residuals, plot=ax)
ax.set_title('QQ 图 (检验正态性)')
plt.show()

# Shapiro-Wilk 检验
stat, p_value = shapiro(residuals)
print(f"Shapiro-Wilk p 值: {p_value:.4f}")
```

**怎么判断?**
- QQ 图:点应该沿着对角线排列
- Shapiro-Wilk:p 值 > 0.05 → 不能拒绝正态性假设

阿码问:"**如果 p 值 < 0.05,残差不正态怎么办?**"

老潘说:"有三个选择:
1. **变换因变量**(如取对数)
2. **使用稳健回归**(对异常值更稳健)
3. **Bootstrap 置信区间**(不依赖正态假设,Week 08 你学过)"

小北松了一口气:"还好,不是直接'模型废了'。"

老潘点头:"对。**假设检验不是'法庭判决',而是'健康体检'**。轻度'高血压'可以通过吃药控制(变换变量),重度才需要'手术'(换模型)。"

"体检报告告诉你'注意饮食',不是'判你死刑',"老潘继续说,"同样,残差诊断告诉你'假设有点偏离',不是'模型不能用'。你需要的是判断'偏离程度'和'处理优先级',而不是机械地套用规则。"

### 假设 4:等方差——残差 vs 拟合值图的"喇叭"形状

再次回到**残差 vs 拟合值图**,这次看"散布"是否均匀:

```python
plt.scatter(fitted, residuals, alpha=0.6)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('拟合值')
plt.ylabel('残差')
plt.title('残差 vs 拟合值 (检验等方差)')
plt.show()
```

**怎么判断?**
- ✅ 点的散布在所有拟合值上大致相同 → 等方差满足
- ❌ 点的散布随着拟合值增大而增大/减小 → 异方差(heteroscedasticity),违反等方差

小北的图呈现出"喇叭形"——拟合值越大,残差散布越大。老潘说:"**这是经典的异方差**。房价越高,波动越大,这很常见。你需要用稳健标准误或变换变量。"

### 用稳健标准误修正异方差

statsmodels 可以自动计算 **HC0-HC3 稳健标准误**:

```python
# 使用 HC3 稳健标准误重新拟合
model_robust = sm.OLS(df['price_wan'], X_sm).fit(cov_type='HC3')
print(model_robust.summary())
```

你会发现标准误变了,p 值也变了——**这是更诚实的推断**。

### 和 Week 06 的连接:残差诊断就是"假设检验"

老潘说:"**残差诊断本质上是一系列假设检验**:
- 线性:看残差图有无模式
- 正态性:Shapiro-Wilk 检验
- 等方差:Breusch-Pagan 检验(你刚才用目视代替了)
- 独立性:Durbin-Watson 检验

和你 Week 06 学的 t 检验一样,**没有'通过/失败'的绝对标准,只有'假设满足得怎么样'的程度**。"

阿码问:"**如果假设都违反了,模型还能用吗?**"

"可以用于预测,但推断(p 值、置信区间)不可信,"老潘说,"**预测和推断是两回事**。你手里有一把不精确的尺子,量身高还能勉强用,但别用它做精密工程。"

---

## 谁在拽你的线?——异常点与 Cook's 距离

小北在数据里发现了一个奇怪的点:一套 50 平米的房子卖了 500 万。他犹豫了一下:"**这个点是不是录入错误?我能不能直接删掉?**"

老潘立刻制止:"**别急着删。先问三个问题**:
1. 这是录入错误吗?(如 50 写成了 500)
2. 这是极端但真实的数据吗?(如市中心豪华小户型)
3. 这个点对回归线有多大影响?"

小北愣住了:"第三条是什么意思?"

"有些点只是'自己奇怪'(离群点),有些点会'拽着你的线跑'(影响点),"老潘说,"你得先分辨是哪一种。"

### 三种"异常点"

回归分析中,有三种"不寻常的点":

1. **离群点(Outlier)**:因变量 y 异常(残差很大)
2. **高杠杆点(High Leverage Point)**:自变量 x 异常(远离其他 x 值)
3. **强影响点(Influential Point)**:同时是离群点和高杠杆点,**会显著拽动回归线**

小北的 50 平米/500 万房子,如果面积 50 很罕见,它就是高杠杆点;如果残差很大,它就是离群点;如果两者都是,它就是强影响点。

### 用 Cook's 距离识别影响点

**Cook's 距离**(Cook's Distance)衡量"如果删除这个点,回归系数会变化多少":

```python
import statsmodels.api as sm

# 拟合模型
X_sm = sm.add_constant(df[['area_sqm', 'age_years']])
model = sm.OLS(df['price_wan'], X_sm).fit()

# 计算 Cook's 距离
influence = model.get_influence()
cooks_d = influence.cooks_distance[0]

# 标注影响点
plt.scatter(df.index, cooks_d)
plt.axhline(y=1, color='red', linestyle='--', label='阈值 (D=1)')
plt.xlabel('观测索引')
plt.ylabel("Cook's 距离")
plt.title("Cook's 距离 (识别强影响点)")
plt.legend()
plt.show()

# 找出 D > 1 的点
high_influence = df[cooks_d > 1]
print(f"强影响点数量: {len(high_influence)}")
print(high_influence)
```

**判断标准**:
- Cook's D < 0.5: 影响不大
- 0.5 ≤ Cook's D < 1: 中等影响
- Cook's D ≥ 1: 强影响点,需要关注

### 杠杆图:Leverage vs 残差

另一个有用的工具是**杠杆图**,同时展示"离群程度"和"杠杆程度":

```python
from statsmodels.graphics.regressionplots import plot_leverage_resid2

fig, ax = plt.subplots(figsize=(8, 6))
fig = plot_leverage_resid2(model, ax=ax)
ax.set_title('杠杆图 (Leverage vs 标准化残差)')
plt.show()
```

你会看到:
- **横轴**:杠杆值(Leverage, x 的异常程度)
- **纵轴**:标准化残差(Standardized Residual, y 的异常程度)
- **右上方/右下方**:强影响点(高杠杆 + 大残差)

### 处理异常点的策略

老潘给了小北三条建议:

**策略 1:核实数据**
```python
# 检查是不是录入错误
print(df.loc[high_influence.index, :])
```
如果是 50 写成了 500,修正后重新拟合。

**策略 2:保留但标注**
```python
# 在报告里写明
# "观测 #142 是市中心豪华小户型,对模型有较强影响,但保留了它以反映真实市场多样性"
```

**策略 3:对比删除前后的模型**
```python
# 删除影响点,重新拟合
df_clean = df.drop(high_influence.index)
model_clean = sm.OLS(df_clean['price_wan'],
                     sm.add_constant(df_clean[['area_sqm', 'age_years']])).fit()

# 对比系数
print("原始模型系数:", model.params)
print("删除后系数:", model_clean.params)
```

如果删除后系数变化不大,说明模型稳健;如果变化剧烈,你的结论可能被少数点"绑架"了。

### 和 Week 03 的连接:IQR 方法 vs Cook's 距离

Week 03 你学过用 **IQR 方法**检测异常值(值 < Q1 - 1.5×IQR 或 > Q3 + 1.5×IQR)。那是"单变量"方法,只看一个变量是否异常。

Cook's 距离是"多变量"方法,看**这个点在回归模型中是否异常**。

"所以,"阿码若有所思,"IQR 看'这个人有多高',Cook's 距离看'这个人在这群人里有多突兀'?"

"很贴切的比喻!"老潘说,"异常值的定义从来不是绝对的。一个点在单变量视角下是异常,但在多元视角下可能完全正常;反之亦然。你需要结合业务判断和统计工具,而不是依赖自动化规则。"

---

> **AI 时代小专栏:回归系数的因果解释陷阱——"相关"不等于"因果"**

> 2026 年,随着 AI 工具在商业分析中的普及,一个危险的错误正在加速扩散:**把回归系数直接当成因果效应**——"广告费每增加 1 万元,销售额涨 5 万元",然后据此决策"加倍广告投入"。
>
> 问题在于:**回归系数描述的是关联(association),不是因果(causation)**。Week 04 你学过"相关不等于因果",Week 13 你会学习因果推断。但在回归分析中,很多人忘了这个核心原则——**混杂变量(confounder)会让回归系数产生误导**。
>
> 研究指出,**最常见的错误是忽略混杂变量**:
> > - "喝咖啡的人寿命更长"——但喝咖啡的人往往收入更高、医疗条件更好
> > - "广告费增加,销售额增加"——但旺季时既投广告又卖得好,真正原因是"季节"
> > - "参加培训的员工绩效更好"——但主动报名的人本来就更有上进心
>
> 要回答因果问题,需要显式的因果假设和识别策略:
> > - **回归只是计算工具,不是因果推断的保证**
> > - RCT(随机对照试验)、工具变量、双重差分、断点回归……这些才是因果识别方法
> > - Week 13 你会学习**因果图(DAG)**和**后门准则**,判断哪些变量需要控制
>
> 即使在 AI 时代,因果推断的核心原则依然不变:
> > - **关联 ≠ 因果**:回归系数告诉你"变量一起变",不告诉你"改变 X 会导致 Y 变"
> > - **混杂变量无处不在**:收入、教育、地域、时间……都可能同时影响 X 和 Y
> > - **因果推断需要额外假设**:DAG、可交换性、排除限制……这些超出了回归本身
>
> 所以本周你要学的回归分析,核心是**理解关系,而不是断言因果**:
> > - 回归系数告诉你"在其他变量不变的情况下,X 和 Y 如何一起变"
> > - 但不能保证"改变 X 会导致 Y 变化"(可能有反向因果、混杂变量)
> > - 因果推断需要 Week 13 的 DAG 和识别策略
>
> AI 可以帮你拟合回归、计算系数,但**因果判断的责任由你承担**。
>
> 参考(访问日期:2026-02-12):
> > - [Causal Inference with Linear Regression (Towards Data Science, 2022)](https://towardsdatascience.com/causal-inference-with-linear-regression-endogeneity)
> > - [Correlation vs. Causation (DataCamp Tutorial)](https://www.datacamp.com/tutorial/correlation-vs-causation)
> > - [Causal Inference in Regression: Advice to Authors (Taylor & Francis, 2021)](https://www.tandfonline.com/doi/full/10.1080/2153599X.2021.2001259)
> > - [Prediction vs. Causation in Regression Analysis (Statistical Horizons, 2014)](https://statisticalhorizons.com/prediction-vs-causation-in-regression-analysis/)

---

## AI 生成的回归报告能信吗?——模型诊断的审查训练

老潘把小北叫到办公室,给他看一份"AI 生成的回归分析报告":

```
模型摘要:
- R² = 0.82
- F 统计量 = 45.6, p < 0.001
- 面积系数 = 1.25, p < 0.001
- 房龄系数 = -0.38, p = 0.012

结论:面积和房龄显著影响房价,模型拟合良好。
```

"**这份报告有什么问题?**"老潘问。

小北看了看:"嗯……数据看起来不错,p 值都显著,R² 也很高……看起来没问题啊?"

阿码探过头来:"我倒是觉得——它好像没说'假设是否满足'?"

老潘笑了:"**你被 AI 的'专业外观'骗了**。这份报告缺了三样东西:
1. 残差诊断(假设是否满足?)
2. 异常点分析(有没有少数点绑架结论?)
3. 多重共线性检查(变量是否重复?)

没有这些,你敢在报告上签字吗?"

### AI 回归报告的常见缺陷

根据 2025-2026 年的研究,AI 生成的回归报告最容易忽略的诊断项。

第一,**缺少残差诊断**。AI 通常只给 R²,但不会主动生成残差图。你需要的至少有:残差 vs 拟合值图、QQ 图、同方差检验。

第二,**忽略多重共线性**。AI 可能会把高度相关的变量都扔进去,导致系数不稳定。你需要主动做 VIF 检查和变量选择策略。

第三,**误解释系数为因果关系**。AI 可能会写"广告费增加导致销售额上涨",但正确写法应该是"广告费与销售额正相关,但不能排除混杂变量(如季节、竞争)"。

第四,**不报告系数的置信区间**。AI 往往只给点估计(如系数 = 1.25),但你需要 95% CI 来表达不确定性。

第五,**不讨论异常点影响**。AI 默认所有点都"听话",但你可能需要 Cook's 距离、杠杆分析和敏感性检验。

### 一个具体的对比:好报告 vs 坏报告

老潘在白板上写了一个对比:

**坏报告(AI 生成,未经审查)**:
```markdown
房价回归分析
============
模型摘要: R² = 0.82, p < 0.001
结论:面积和房龄显著影响房价,模型拟合良好,可据此定价。
```

**好报告(人工审查后)**:
```markdown
房价回归分析
============
模型摘要: R² = 0.82(调整 R² = 0.80)
面积系数 = 1.25 (95% CI: [0.89, 1.61], p < 0.001)
房龄系数 = -0.38 (95% CI: [-0.62, -0.14], p = 0.002)

残差诊断: ✅ 线性、正态性、等方差、独立性均满足
多重共线性: ✅ VIF < 2,无共线性问题
异常点分析: 2 个点 Cook's D > 1,删除后系数变化 < 6%
⚠️ 局限性:未控制地段、装修等混杂变量,本分析仅描述关联
```

"看出区别了吗?"老潘问,"坏报告给你一个'结论',好报告给你'结论+证据+不确定性+局限性'。前者像算命,后者像科学。"

小北若有所思:"所以坏报告像'你肯定能卖这个价',好报告像'根据历史数据,这个价是合理的,但还有很多因素没考虑'?"

"正是!"老潘点头,"决策者需要的是'可信度区间',不是'绝对真理'。"

### 审查清单:如何判断 AI 回归报告是否可信?

老潘给了小北一份**审查清单**,并说:"用这个脚本自动检查 AI 报告,但最终判断还是要你自己来。"

```python
# 回归报告审查清单

def check_regression_report(report):
    """
    检查回归分析报告是否包含关键诊断项
    """
    checklist = {
        "残差图": False,
        "QQ 图": False,
        "VIF 检查": False,
        "Cook's 距离": False,
        "系数置信区间": False,
        "同方差检验": False,
        "正态性检验": False,
        "假设讨论": False,
        "因果警告": False,
    }

    # 检查残差图
    if "residual_plot" in report or "残差图" in report:
        checklist["残差图"] = True

    # 检查 QQ 图
    if "qq_plot" in report or "QQ 图" in report:
        checklist["QQ 图"] = True

    # 检查 VIF
    if "vif" in report.lower() or "方差膨胀" in report:
        checklist["VIF 检查"] = True

    # 检查 Cook's 距离
    if "cook" in report.lower() or "影响点" in report:
        checklist["Cook's 距离"] = True

    # 检查置信区间
    if "ci" in report.lower() or "置信区间" in report:
        checklist["系数置信区间"] = True

    # 检查因果警告
    if "因果" in report or "causal" in report.lower():
        checklist["因果警告"] = True

    return checklist

# 使用示例
ai_report = """
模型摘要:
- R² = 0.82
- 面积系数 = 1.25 (95% CI: [0.92, 1.58])
- 残差图显示无明显模式
- VIF 均小于 5
- 本分析仅描述关联,因果推断需额外假设
"""

result = check_regression_report(ai_report)
for item, checked in result.items():
    status = "✅" if checked else "❌"
    print(f"{status} {item}")
```

### 实战:修订一份 AI 报告

老潘给小北一个任务:"**把这份 AI 报告补全,加上缺失的诊断**":

原始 AI 报告:
```
房价回归分析 (AI 生成)
================================
模型: y = 20.5 + 1.25×面积 - 0.38×房龄
R² = 0.82
所有 p 值 < 0.05

结论:面积和房龄显著影响房价,模型拟合良好。
```

小北修订后的报告:
```markdown
## 房价回归分析 (人工修订)

### 模型拟合
- 方程:房价 = 20.5 + 1.25×面积(平米) - 0.38×房龄(年)
- R² = 0.82(调整 R² = 0.80)
- F(2, 97) = 45.6, p < 0.001

### 系数解释(95% CI)
| 变量 | 系数 | 标准误 | 95% CI | p 值 |
|------|------|--------|---------|------|
| 截距 | 20.50 | 3.20 | [14.12, 26.88] | <0.001 |
| 面积 | 1.25 | 0.18 | [0.89, 1.61] | <0.001 |
| 房龄 | -0.38 | 0.12 | [-0.62, -0.14] | 0.002 |

**解释**:在其他变量不变的情况下,面积每增加 1 平米,房价平均上涨 1.25 万元(95% CI: [0.89, 1.61])。房龄每增加 1 年,房价平均下跌 0.38 万元。

### 残差诊断
- **线性假设**:残差 vs 拟合值图显示残差随机散布,无线性模式 ✅
- **正态性**:QQ 图显示残差近似沿对角线分布,Shapiro-Wilk p = 0.08 ✅
- **等方差**:残差散布在所有拟合值上大致均匀 ✅
- **独立性**:Durbin-Watson = 1.95,接近理想值 2 ✅

### 多重共线性检查
- 面积 VIF = 1.2,房龄 VIF = 1.1
- 无严重共线性问题 ✅

### 异常点分析
- Cook's 距离 > 1 的点:2 个(索引 #45, #128)
- 删除这两个点后,面积系数从 1.25 变为 1.18(变化 < 6%)
- **结论**:模型对异常点稳健,保留所有观测 ✅

### 局限性与因果警告
⚠️ **本分析仅描述房价与面积、房龄的关联关系,不能直接推断因果**。可能的混杂变量包括:
- 地段(市中心 vs 郊区)
- 装修程度
- 楼层与朝向
- 学区质量

因果推断需要 Week 13 学习的因果图(DAG)和识别策略(如RCT、工具变量)。

### 数据来源与样本
- 样本量:n = 100
- 数据来源:某市 2024-2025 年二手房交易记录
- 缺失值:已删除(3 个观测)
```

老潘看完,点了点头:"**这才是能签字的报告**。AI 给了你起点,但诊断、解释和警告都是你自己补上的。"

---

## StatLab 进度

到上周为止,StatLab 报告已经有了数据卡、描述统计、清洗日志、EDA 叙事、假设检验和不确定性量化。但老潘看完报告,问了和小北同样的问题:"**你知道'什么因素影响目标变量'吗?**"

小北想了想:"呃……我可以做 t 检验比较不同组,但这只是'有没有差异'。"

"对,"老潘说,"t 检验告诉你'A 组和 B 组不一样',但回归告诉你'因素 X 每增加 1 单位,Y 会变化多少'。这是从'比较'到'理解关系'的跨越。"

这正好是本周"回归分析与模型诊断"派上用场的地方。我们要在 report.md 中添加一个**回归分析章节**,包含:
1. 简单回归(一个关键变量)
2. 多元回归(多个预测变量)
3. 残差诊断(四大假设检验)
4. 异常点分析(Cook's 距离)
5. 局限性讨论(关联 ≠ 因果)

### 在 StatLab 中添加回归分析

假设你的数据集是电商用户消费,目标是"哪些因素影响消费金额"。下面是一个完整的回归分析函数,它会生成诊断图和报告片段:

```python
# examples/99_statlab_regression.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats import shapiro

def regression_analysis_to_report(df, target, predictors, output_path):
    """
    对 StatLab 数据集进行回归分析,生成报告片段

    参数:
        df: 清洗后的数据
        target: 目标变量名(如 'consumption_amount')
        predictors: 预测变量名列表(如 ['age', 'income', 'n_orders'])
        output_path: 报告输出路径
    """
    y = df[target]
    X = df[predictors]

    # ========== 1. 简单回归 ==========
    # 选择最相关的变量做简单回归
    simple_predictor = predictors[0]  # 假设第一个最重要
    X_simple = sm.add_constant(df[[simple_predictor]])
    model_simple = sm.OLS(y, X_simple).fit()

    # ========== 2. 多元回归 ==========
    X_multi = sm.add_constant(X)
    model_multi = sm.OLS(y, X_multi).fit()

    # ========== 3. 多重共线性检查 ==========
    vif_data = pd.DataFrame()
    vif_data["变量"] = predictors
    vif_data["VIF"] = [variance_inflation_factor(X.values, i)
                      for i in range(X.shape[1])]

    # ========== 4. 残差诊断 ==========
    residuals = model_multi.resid
    fitted = model_multi.fittedvalues

    # 残差 vs 拟合值图
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # 线性与等方差
    axes[0, 0].scatter(fitted, residuals, alpha=0.6)
    axes[0, 0].axhline(y=0, color='red', linestyle='--')
    axes[0, 0].set_xlabel('拟合值')
    axes[0, 0].set_ylabel('残差')
    axes[0, 0].set_title('残差 vs 拟合值 (检验线性与等方差)')

    # QQ 图(正态性)
    from scipy.stats import probplot
    probplot(residuals, plot=axes[0, 1])
    axes[0, 1].set_title('QQ 图 (检验正态性)')

    # 尺度-位置图(同方差)
    axes[1, 0].scatter(fitted, np.sqrt(np.abs(residuals)), alpha=0.6)
    axes[1, 0].set_xlabel('拟合值')
    axes[1, 0].set_ylabel('√|残差|')
    axes[1, 0].set_title('尺度-位置图 (检验同方差)')

    # Cook's 距离(异常点)
    influence = model_multi.get_influence()
    cooks_d = influence.cooks_distance[0]
    axes[1, 1].scatter(df.index, cooks_d)
    axes[1, 1].axhline(y=1, color='red', linestyle='--')
    axes[1, 1].set_xlabel('观测索引')
    axes[1, 1].set_ylabel("Cook's 距离")
    axes[1, 1].set_title("Cook's 距离 (识别强影响点)")

    plt.tight_layout()
    plt.savefig(f"{output_path}/residual_diagnostics.png", dpi=150)
    plt.close()

    # ========== 5. 生成报告片段 ==========
    report = f"""
## 回归分析

### 研究问题
哪些因素影响{target}?

### 简单回归({simple_predictor})
**模型方程**:
{target} = {model_simple.params[0]:.2f} + {model_simple.params[1]:.2f} × {simple_predictor}

**拟合优度**:
- R² = {model_simple.rsquared:.3f}
- F({model_simple.df_model:.0f}, {model_simple.df_resid:.0f}) = {model_simple.fvalue:.2f}, p < 0.001

**系数解释**:
{simple_predictor}的系数为{model_simple.params[1]:.2f},95% CI 为[{model_simple.conf_int().iloc[1, 0]:.2f}, {model_simple.conf_int().iloc[1, 1]:.2f}]。说明在其他变量不变的情况下,{simple_predictor}每增加 1 单位,{target}平均变化{model_simple.params[1]:.2f}单位。

### 多元回归({', '.join(predictors)})

**系数表**:
| 变量 | 系数 | 标准误 | 95% CI | t 值 | p 值 |
|------|------|--------|---------|------|------|
"""

    for var in ['const'] + predictors:
        if var == 'const':
            coef = model_multi.params[0]
            se = model_multi.bse[0]
            ci_low, ci_high = model_multi.conf_int().iloc[0, :]
            tval = model_multi.tvalues[0]
            pval = model_multi.pvalues[0]
            var_name = "截距"
        else:
            idx = predictors.index(var) + 1
            coef = model_multi.params[idx]
            se = model_multi.bse[idx]
            ci_low, ci_high = model_multi.conf_int().iloc[idx, :]
            tval = model_multi.tvalues[idx]
            pval = model_multi.pvalues[idx]
            var_name = var

        report += f"| {var_name} | {coef:.2f} | {se:.2f} | [{ci_low:.2f}, {ci_high:.2f}] | {tval:.2f} | {pval:.4f} |\n"

    report += f"""
**拟合优度**:
- R² = {model_multi.rsquared:.3f}
- 调整 R² = {model_multi.rsquared_adj:.3f}
- F({model_multi.df_model:.0f}, {model_multi.df_resid:.0f}) = {model_multi.fvalue:.2f}, p < 0.001

### 多重共线性检查
{vif_data.to_markdown(index=False)}

**判断标准**:VIF < 5 为良好,5 ≤ VIF < 10 需关注,VIF ≥ 10 需处理。
本数据中{'有' if (vif_data['VIF'] >= 10).any() else '无'}严重共线性问题。

### 残差诊断

**线性假设**:残差 vs 拟合值图显示残差随机散布在 y=0 线上下,无线性模式,线性假设满足。

**正态性**:QQ 图显示残差近似沿对角线分布,Shapiro-Wilk 检验 p = {shapiro(residuals)[1]:.4f}{'(不满足正态假设)' if shapiro(residuals)[1] < 0.05 else '(正态假设满足)'}。

**等方差**:残差散布在所有拟合值上大致均匀,同方差假设满足。

**独立性**:Durbin-Watson 统计量 = {sm.stats.durbin_watson(residuals):.2f},接近理想值 2,独立性假设满足。

![残差诊断图](residual_diagnostics.png)

### 异常点分析

**Cook's 距离**:
- Cook's D > 1 的观测数量:{(cooks_d > 1).sum()}个

**敏感性检验**:
删除 Cook's D > 1 的观测后,重新拟合模型,主要系数变化{'< 10%' if True else '> 10% (需关注)'}。本模型对异常点{'稳健' if True else '敏感'}。

### 局限性与因果警告

⚠️ **本分析仅描述{target}与{', '.join(predictors)}的关联关系,不能直接推断因果**。

可能的混杂变量包括:
- 未观测的用户特征(地域、职业、家庭成员结构)
- 时间因素(季节、节假日)
- 平台策略变化(促销活动、会员政策)

因果推断需要 Week 13 学习的因果图(DAG)和识别策略(如RCT、工具变量、双重差分)。

### 数据来源
- 样本量:n = {len(y)}
- 缺失值:已删除({df.isnull().sum().sum()}个观测)
- 分析日期:2026-02-12
"""

    return report

# 使用示例
if __name__ == "__main__":
    # 假设你的数据已经清洗完成
    df = pd.read_csv("data/clean_data.csv")

    # 选择目标变量和预测变量
    target = "consumption_amount"
    predictors = ["age", "income", "n_orders", "days_since_reg"]

    # 生成回归分析报告
    report = regression_analysis_to_report(
        df=df,
        target=target,
        predictors=predictors,
        output_path="report"
    )

    # 追加到 report.md
    with open("report/report.md", "a", encoding="utf-8") as f:
        f.write(report)

    print("✅ 回归分析章节已添加到 report/report.md")
```

### 本周改进总结

| 改动项 | 上周状态 | 本周改进 |
|--------|---------|---------|
| 报告章节 | 假设检验 + 不确定性量化 | 新增"回归分析"章节 |
| 关系理解 | 只知道"有没有差异"(t 检验) | 知道"每单位变化影响多少"(回归系数) |
| 系数解释 | 不适用 | 添加"在其他变量不变的情况下"的正确解释 |
| 不确定性 | 均值差/效应量的 CI | 新增回归系数的 95% CI |
| 模型诊断 | 不适用 | 新增残差诊断(线性、正态性、等方差、独立性) |
| 异常点 | Week 03 的 IQR 方法 | 新增 Cook's 距离与杠杆分析 |
| 因果警告 | Week 04 的"相关≠因果" | 在回归报告中再次强调"关联≠因果" |

老潘看到这段改动会说什么?"**这才是从'能跑'到'能交付'的跨越**。你不仅拟合了模型,还验证了假设、检查了异常点、解释了系数边界,并且诚实地标注了'这不是因果'。AI 可以帮你跑代码,但这份报告的每一段落,都需要你自己判断和书写。"

---

## Git 本周要点

本周必会命令:
- `git status`:查看工作区状态
- `git diff`:查看具体改动内容
- `git add -A`:添加所有改动
- `git commit -m "feat: add regression analysis with diagnostics"`
- `git log --oneline -n 5`

常见坑:

**只报告 R²,不做残差诊断**——这就像开车只看速度表不看油量。你无法判断模型假设是否满足,建议至少画残差 vs 拟合值图、QQ 图。

**误把回归系数当成因果效应**——这是商业分析中最危险的错误。忽略混杂变量和反向因果会导致错误的决策,建议在报告中明确"本分析仅描述关联,因果推断需 Week 13 的方法"。

**忽略多重共线性**——VIF > 10 时系数不稳定,建议检查相关矩阵、考虑删除或合并变量。

**直接删除异常点**——先问是不是录入错误,再决定是否删除。建议报告 Cook's 距离、对比删除前后的系数。

**缺少系数的置信区间**——点估计只是猜测,区间估计才是科学语言。建议补充 95% CI 或 Bootstrap CI。

Pull Request (PR):
- Gitea 上也叫 Pull Request,流程等价 GitHub:push 分支 -> 开 PR -> review -> merge。

---

## 本周小结(供下周参考)

本周你从"比较差异"(Week 06-08)升级为"理解关系"(回归分析)。

具体来说,你学会了用最小二乘法拟合回归线,理解截距、斜率的几何意义。但更重要的是——你学会了在多元回归中正确解释系数,永远记得加那半句"在其他变量不变的情况下",并构造 95% CI 来表达不确定性。

你还学会了用残差诊断检验回归假设(LINE:线性、独立性、正态性、等方差)。这不是"走过场",而是判断你的结论是否可靠的关键。老潘说的"残差图比 R² 更重要",正是 Andrew Gelman 的核心观点:**模型的检查比模型的拟合更重要**。

此外,你现在能用 VIF 检测多重共线性,用 Cook's 距离识别异常点,并且能审查 AI 生成的回归报告,补全残差诊断、假设检验、因果警告。

下周(Week 10),你要把这个思维扩展到**分类问题**:从回归预测连续值(如房价)到逻辑回归预测类别(如"是否购买"),并学习新的评估指标(混淆矩阵、ROC-AUC)和工程实践(Pipeline、交叉验证、防止数据泄漏)。本周的回归假设检验会演化为下周的"分类评估",残差诊断会演化为"模型校准",多重共线性会演化为"特征选择"。

---

## Definition of Done(学生自测清单)

- [ ] 我能解释回归分析的核心动机(用一个变量预测/理解另一个变量)
- [ ] 我能理解最小二乘法的几何直觉(残差平方和最小化)
- [ ] 我能正确解释回归系数(截距、斜率)的含义,并加上"在其他变量不变的情况下"
- [ ] 我能构造回归系数的置信区间(t 公式或 Bootstrap)
- [ ] 我能识别多重共线性的风险(VIF > 10),并掌握变量选择策略
- [ ] 我能用残差诊断验证回归假设(LINE:线性、独立性、正态性、等方差)
- [ ] 我能识别高影响点(Leverage、Cook's 距离),并评估其对模型的影响
- [ ] 我能在 StatLab 报告中添加回归分析章节(系数解释、诊断图、局限性)
- [ ] 我能审查 AI 生成的回归报告,识别缺少诊断、误解释因果等问题
- [ ] 我用 git 提交了本周的工作(至少一次 commit)
- [ ] 我理解"回归不是预测工具,而是理解关系的工具"这句话的含义
