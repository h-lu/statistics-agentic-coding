"""
ç¤ºä¾‹ï¼šæœ€å°äºŒä¹˜æ³•çš„å‡ ä½•ç›´è§‰â€”â€”æ‰‹åŠ¨è®¡ç®— OLS ç³»æ•°

æœ¬ä¾‹æ¼”ç¤ºæœ€å°äºŒä¹˜æ³•(OLS)çš„æ•°å­¦åŸç†ï¼ŒåŒ…æ‹¬ï¼š
1. æ‰‹åŠ¨è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆæ®‹å·®å¹³æ–¹å’Œï¼‰
2. ç”¨çŸ©é˜µå…¬å¼è®¡ç®— OLS ç³»æ•°ï¼šÎ² = (X'X)^(-1)X'y
3. å¯¹æ¯” sklearn ç»“æœéªŒè¯ä¸€è‡´æ€§

è¿è¡Œæ–¹å¼ï¼špython3 chapters/week_09/examples/02_ols_intuition.py
é¢„æœŸè¾“å‡ºï¼š
- æŸå¤±å‡½æ•°çš„å€¼ï¼ˆæ®‹å·®å¹³æ–¹å’Œï¼‰
- æ‰‹åŠ¨è®¡ç®—å’Œ sklearn è®¡ç®—çš„ç³»æ•°å¯¹æ¯”
- å¯è§†åŒ–ï¼šæ®‹å·®çš„å¹³æ–¹ï¼ˆæ˜¾ç¤ºå¤§è¯¯å·®è¢«æ”¾å¤§ï¼‰
"""
from __future__ import annotations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

np.random.seed(42)


def generate_simple_data() -> pd.DataFrame:
    """ç”Ÿæˆç®€å•çš„çº¿æ€§å…³ç³»æ•°æ®"""
    x = np.array([1, 2, 3, 4, 5], dtype=float)
    y = 2 * x + 3 + np.array([0.5, -0.3, 0.8, -0.6, 0.4])  # æ·»åŠ å°å™ªéŸ³
    return pd.DataFrame({'x': x, 'y': y})


def compute_loss(y_true: np.ndarray, y_pred: np.ndarray,
                 loss_type: str = 'mse') -> float:
    """
    è®¡ç®—æŸå¤±å‡½æ•°

    å‚æ•°:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        loss_type: 'mse'(å‡æ–¹è¯¯å·®) æˆ– 'mae'(å¹³å‡ç»å¯¹è¯¯å·®)

    è¿”å›:
        æŸå¤±å€¼
    """
    residuals = y_true - y_pred
    if loss_type == 'mse':
        return np.mean(residuals ** 2)
    elif loss_type == 'mae':
        return np.mean(np.abs(residuals))
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")


def manual_ols(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    """
    æ‰‹åŠ¨è®¡ç®— OLS ç³»æ•°ï¼ˆçŸ©é˜µå…¬å¼ï¼‰

    å…¬å¼: Î² = (X'X)^(-1)X'y

    å‚æ•°:
        X: è‡ªå˜é‡çŸ©é˜µï¼ˆå«æˆªè·é¡¹ï¼‰
        y: å› å˜é‡å‘é‡

    è¿”å›:
        ç³»æ•°å‘é‡ Î²
    """
    # Î² = (X'X)^(-1)X'y
    XtX = X.T @ X
    Xty = X.T @ y
    beta = np.linalg.inv(XtX) @ Xty
    return beta


def visualize_residuals_squared(df: pd.DataFrame, model: LinearRegression) -> None:
    """
    å¯è§†åŒ–æ®‹å·®çš„å¹³æ–¹ï¼ˆå±•ç¤ºå¹³æ–¹æŸå¤±çš„æ”¾å¤§æ•ˆåº”ï¼‰

    å‚æ•°:
        df: åŒ…å« x, y, predicted, residual çš„æ•°æ®
        model: æ‹Ÿåˆçš„æ¨¡å‹
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # å·¦å›¾ï¼šæ®‹å·®ï¼ˆç»å¯¹å€¼ï¼‰
    axes[0].bar(df.index, np.abs(df['residual']), color='steelblue', alpha=0.7)
    axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1)
    axes[0].set_xlabel('è§‚æµ‹ç´¢å¼•', fontsize=12)
    axes[0].set_ylabel('|æ®‹å·®|', fontsize=12)
    axes[0].set_title('æ®‹å·®ç»å¯¹å€¼ (MAE)', fontsize=14)
    axes[0].grid(True, alpha=0.3)

    # å³å›¾ï¼šæ®‹å·®å¹³æ–¹ï¼ˆå¹³æ–¹æ”¾å¤§æ•ˆåº”ï¼‰
    axes[1].bar(df.index, df['residual'] ** 2, color='coral', alpha=0.7)
    axes[1].set_xlabel('è§‚æµ‹ç´¢å¼•', fontsize=12)
    axes[1].set_ylabel('æ®‹å·®Â²', fontsize=12)
    axes[1].set_title('æ®‹å·®å¹³æ–¹ (MSE) - å¤§è¯¯å·®è¢«æ”¾å¤§', fontsize=14)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('ols_loss_comparison.png', dpi=150, bbox_inches='tight')
    print("âœ… æŸå¤±å‡½æ•°å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º ols_loss_comparison.png")
    plt.close()


def main() -> None:
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤º OLS å‡ ä½•ç›´è§‰"""
    print("=" * 60)
    print("ç¤ºä¾‹2: æœ€å°äºŒä¹˜æ³•çš„å‡ ä½•ç›´è§‰")
    print("=" * 60)

    # 1. ç”Ÿæˆæ•°æ®
    df = generate_simple_data()
    print(f"\nğŸ“Š åŸå§‹æ•°æ®:")
    print(df)

    # 2. ç”¨ sklearn æ‹Ÿåˆ
    X_sklearn = df[['x']]
    y = df['y']
    model_sklearn = LinearRegression()
    model_sklearn.fit(X_sklearn, y)

    print(f"\nğŸ“ˆ sklearn ç»“æœ:")
    print(f"  æˆªè·: {model_sklearn.intercept_:.4f}")
    print(f"  æ–œç‡: {model_sklearn.coef_[0]:.4f}")
    print(f"  MSE (æŸå¤±): {compute_loss(y, model_sklearn.predict(X_sklearn), 'mse'):.4f}")
    print(f"  MAE (å¯¹æ¯”): {compute_loss(y, model_sklearn.predict(X_sklearn), 'mae'):.4f}")

    # 3. æ‰‹åŠ¨è®¡ç®— OLS ç³»æ•°
    X_with_intercept = np.column_stack([np.ones(len(df)), df['x']])
    beta_manual = manual_ols(X_with_intercept, y)

    print(f"\nğŸ§® æ‰‹åŠ¨è®¡ç®— (çŸ©é˜µå…¬å¼ Î² = (X'X)^(-1)X'y):")
    print(f"  X'X çŸ©é˜µ:")
    print(X_with_intercept.T @ X_with_intercept)
    print(f"\n  (X'X)^(-1):")
    print(np.linalg.inv(X_with_intercept.T @ X_with_intercept))
    print(f"\n  ç³»æ•° Î²:")
    print(f"    æˆªè·: {beta_manual[0]:.4f}")
    print(f"    æ–œç‡: {beta_manual[1]:.4f}")

    # 4. éªŒè¯ä¸€è‡´æ€§
    print(f"\nâœ… éªŒè¯:")
    intercept_match = np.isclose(beta_manual[0], model_sklearn.intercept_)
    slope_match = np.isclose(beta_manual[1], model_sklearn.coef_[0])
    print(f"  æˆªè·ä¸€è‡´: {intercept_match}")
    print(f"  æ–œç‡ä¸€è‡´: {slope_match}")
    print(f"  ç»“è®º: æ‰‹åŠ¨è®¡ç®—ä¸ sklearn ç»“æœ{'ä¸€è‡´ âœ“' if intercept_match and slope_match else 'ä¸ä¸€è‡´ âœ—'}")

    # 5. å¯è§†åŒ–æ®‹å·®
    df['predicted'] = model_sklearn.predict(X_sklearn)
    df['residual'] = df['y'] - df['predicted']
    visualize_residuals_squared(df, model_sklearn)

    # 6. å‡å€¼ä¹Ÿæ˜¯"æœ€å°äºŒä¹˜"
    print(f"\nğŸ”— å›é¡¾: å‡å€¼ä¹Ÿæ˜¯æœ€å°äºŒä¹˜ä¼°è®¡")
    print(f"  y çš„å‡å€¼: {y.mean():.4f}")
    print(f"  æœ€å°åŒ– Î£(yi - Î¼)Â² çš„ Î¼: {y.mean():.4f}")
    print(f"  â†’ å›å½’åªæ˜¯è¿™ä¸ªæ€æƒ³æ‰©å±•åˆ°'å¸¦è‡ªå˜é‡'çš„åœºæ™¯")

    print("\n" + "=" * 60)
    print("âœ… ç¤ºä¾‹2å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
