# Week 12 评分标准（Rubric）

## 评分维度与权重

| 维度 | 权重 | 说明 |
|------|------|------|
| SHAP 可解释性分析 | 30% | 正确计算 SHAP 值、区分全局/局部解释、可视化清晰 |
| 公平性指标计算 | 30% | 准确计算公平性指标、识别群体性能差异、理解不同公平性定义 |
| 伦理审查清单 | 25% | 完整的风险评估、代理变量检测、可复现性检查 |
| 偏见来源与差分隐私 | 10% | 理解偏见三个来源、实现差分隐私机制 |
| AI SHAP 代码审查 | 5% | 识别 AI 代码中的缺失项和工程陷阱 |

---

## 详细评分标准

### 1. SHAP 可解释性分析（30 分）

| 评分项 | 分值 | 优秀（A） | 良好（B） | 合格（C） | 不合格（F） |
|--------|------|-----------|-----------|-----------|-------------|
| 模型训练正确 | 5 | API 使用正确、参数设置合理、性能评估完整 | 拟合基本正确 | 拟合有瑕疵 | 拟合错误 |
| 特征重要性计算和局限性分析 | 7 | 准确计算重要性、清晰分析局限性（无法解释单个样本） | 计算基本正确、分析基本清晰 | 计算有瑕疵或分析不完整 | 计算错误或无局限性分析 |
| SHAP 值计算正确 | 8 | 使用正确解释器（Tree/Linear）、SHAP 值提取准确、区分二分类正负类 | 计算基本正确 | 计算有瑕疵 | 计算错误 |
| SHAP 可视化清晰 | 6 | summary_plot 和 force_plot 完整、标注清晰、解读准确 | 可视化基本完整 | 可视化有瑕疵 | 可视化缺失或混乱 |
| 与 Week 08 置信区间连接 | 4 | 准确连接 SHAP 不确定性和 Bootstrap 置信区间 | 连接基本正确 | 连接有瑕疵 | 未连接或连接错误 |

**验证方式**：
- 检查 `shap_explanation.md` 中的 SHAP 值计算
- 验证特征重要性和 SHAP 值的区别分析
- 评估可视化质量（summary_plot + force_plot）
- 检查与 Week 08 置信区间的连接

**常见错误**：
- ❌ 混淆特征重要性和 SHAP 值（认为它们是一回事）
- ❌ SHAP 使用了错误的解释器（如 LinearExplainer 用于随机森林）
- ❌ 未区分二分类的正负类 SHAP 值（`shap_values[1]` vs `shap_values[0]`）
- ❌ force_plot 未标注基准值和贡献（无法解释预测构成）
- ❌ 直接解释 SHAP 值为因果（"收入贡献 +0.3，所以提高收入会降低违约"）
- ❌ 未说明 SHAP 值的不确定性（假装精确）

---

### 2. 公平性指标计算（30 分）

| 评分项 | 分值 | 优秀（A） | 良好（B） | 合格（C） | 不合格（F） |
|--------|------|-----------|-----------|-----------|-------------|
| 群体性能计算正确 | 8 | 准确计算 AUC、准确率、召回率、样本量 | 计算基本正确 | 计算有瑕疵 | 计算错误 |
| 差异影响比计算和诊断 | 7 | 计算正确、诊断准确（是否 < 0.8）、连接 80% 规则 | 计算基本正确、诊断基本准确 | 计算有瑕疵或诊断不完整 | 计算错误或无诊断 |
| 平等机会计算和诊断 | 7 | 召回率（TPR）计算正确、差异诊断准确、含义解释清晰 | 计算基本正确、诊断基本准确 | 计算有瑕疵或诊断不完整 | 计算错误或无诊断 |
| 均等几率检查完整 | 6 | TPR 和 FPR 都检查、阈值判断合理（< 0.05） | 检查基本完整 | 检查有瑕疵 | 检查缺失或错误 |
| 与 Week 10 混淆矩阵连接 | 2 | 准确连接混淆矩阵和公平性指标的关系 | 连接基本正确 | 连接有瑕疵 | 未连接或连接错误 |

**验证方式**：
- 检查 `fairness_metrics.md` 中的群体性能对比表
- 验证三项公平性指标的计算公式
- 评估对"差异影响比 < 0.8"的诊断
- 检查与 Week 10 混淆矩阵的连接分析

**常见错误**：
- ❌ 只计算 AUC，未按敏感特征分组（无法发现偏见）
- ❌ 差异影响比计算错误（分子分母搞反）
- ❌ 平等机会差异计算错误（用假阳性率代替召回率）
- ❌ 均等几率只检查 TPR，未检查 FPR（不完整）
- ❌ 差异影响比 < 0.8，却说"模型公平"（未识别法律风险）
- ❌ 未连接 Week 10 的混淆矩阵（知识孤立）

---

### 3. 伦理审查清单（25 分）

| 评分项 | 分值 | 优秀（A） | 良好（B） | 合格（C） | 不合格（F） |
|--------|------|-----------|-----------|-----------|-------------|
| 审查清单完整 | 10 | 所有 7 项都有答案+证据、每项都有具体分析 | 大部分项目完整 | 部分项目完整或证据不足 | 清单缺失或大部分项目未完成 |
| 代理变量检测正确 | 8 | 相关性分析完整（>0.5）、SHAP 依赖图验证、解释清晰 | 检测基本正确 | 检测有瑕疵 | 检测错误或缺失 |
| 隐私风险评估合理 | 4 | 识别模型反演攻击风险、给出缓解建议、分析深入 | 风险评估基本合理 | 风险评估有瑕疵 | 风险评估缺失或错误 |
| 可复现性检查完整 | 3 | 检查 random_state、代码记录、数据来源、参数记录 | 检查基本完整 | 检查有瑕疵 | 检查缺失 |
| 结论边界说明清晰 | 5 | 明确说明"模型能/不能回答什么"、包含因果关系、特殊场景、伦理判断 | 说明基本清晰 | 说明有瑕疵 | 说明缺失或模糊 |

**验证方式**：
- 检查 `ethics_review_checklist.md` 中的审查清单完整性
- 验证代理变量检测的相关性阈值（>0.5）
- 评估隐私风险评估的深度（是否识别模型反演攻击）
- 检查可复现性检查项（random_state、代码、数据来源）
- 评估结论边界说明的清晰度

**常见错误**：
- ❌ 审查清单只是打勾，没有证据支撑（如"存在偏见"但无数据支持）
- ❌ 代理变量检测阈值错误（用 0.3 而不是 0.5）
- ❌ 未用 SHAP 依赖图验证代理变量（只用相关性，不够）
- ❌ 隐私风险评估只说"加差分隐私"，未分析攻击场景
- ❌ 可复现性只检查 random_state，未检查代码记录、数据来源
- ❌ 结论边界只说"模型有局限性"，未具体说明"不能回答什么"

---

### 4. 偏见来源与差分隐私（10 分）

| 评分项 | 分值 | 优秀（A） | 良好（B） | 合格（C） | 不合格（F） |
|--------|------|-----------|-----------|-----------|-------------|
| 数据偏见分析深入 | 3 | 准确识别历史歧视、样本不平衡、MNAR 风险 | 分析基本准确 | 分析有瑕疵 | 分析错误或缺失 |
| 算法偏见分析准确 | 2 | 对比线性/树模型性能差异、解释复杂模型如何放大数据 | 分析基本准确 | 分析有瑕疵 | 分析错误或缺失 |
| 反馈循环模拟合理 | 2 | 场景描述清晰、缓解策略可行 | 模拟基本合理 | 模拟有瑕疵 | 模拟缺失或错误 |
| 差分隐私实现正确 | 3 | 拉普拉斯机制实现正确、ε 对比完整、隐私验证合理 | 实现基本正确 | 实现有瑕疵 | 实现错误或缺失 |

**验证方式**：
- 检查 `bias_source_analysis.md` 中的三个偏见来源分析
- 验证反馈循环场景和缓解策略
- 检查 `differential_privacy.md` 中的拉普拉斯机制实现
- 评估不同 ε 值的对比分析

**常见错误**：
- ❌ 只分析数据偏见，未分析算法偏见和反馈循环（不完整）
- ❌ 反馈循环场景模糊（未说明"拒绝→无标签→更拒绝"的循环）
- ❌ 缓解策略不可行（如"删除所有偏见"，不可能）
- ❌ 差分隐私实现错误（噪声尺度 = ε/Δf 而不是 Δf/ε）
- ❌ ε 对比只用一个值（未展示隐私-实用性权衡）

---

### 5. AI SHAP 代码审查（5 分）

| 评分项 | 分值 | 优秀（A） | 良好（B） | 合格（C） | 不合格（F） |
|--------|------|-----------|-----------|-----------|-------------|
| 审查全面性 | 2 | 识别出所有主要缺失项（因果警告、不确定性、向客户解释、代理变量） | 识别出大部分问题 | 识别出部分问题 | 仅识别少量问题 |
| 问题分类合理 | 1 | 按严重性分类（高/中/低），分类准确 | 分类基本合理 | 分类有瑕疵 | 分类混乱 |
| 修正版代码正确 | 1 | 修正代码包含缺失项、可运行、符合最佳实践 | 修正代码基本正确 | 修正代码有瑕疵 | 修正代码错误 |
| 反思深入 | 1 | 分析 AI 的优缺点、提出有价值的人类必须检查项 | 反思基本合理 | 反思有瑕疵 | 反思缺失或肤浅 |

**验证方式**：
- 检查 `ai_shap_review.md` 中的审查清单
- 验证对 AI 代码缺失项的识别（因果警告、不确定性等）
- 评估修正版代码的完整性
- 检查反思部分的深度

**常见错误**：
- ❌ 未检查 AI 是否警告"SHAP ≠ 因果关系"（最重要）
- ❌ 未检查 AI 是否说明 SHAP 值的不确定性（Bootstrap）
- ❌ 未检查 AI 是否提供"向客户解释"的模板（非技术读者）
- ❌ 未检查 AI 是否检测代理变量（与敏感特征相关）
- ❌ 反思只说"AI 很好"或"AI 很差"，无具体分析

---

## 加分/扣分项

### 加分项

| 项目 | 加分 | 说明 |
|------|------|------|
| StatLab 集成完整 | +5% | 将模型解释与伦理审查结果正确整合到 StatLab 报告中，包含三个版本的解释（客户、产品经理、合规部门） |
| 代码质量优秀 | +3% | 代码注释清晰、结构良好、可复现（固定随机种子）、有函数封装 |
| 超额完成挑战作业 | +5% | 完成完整 XAI 流水线且质量优秀（SHAP + 公平性 + 伦理 + 非技术解释） |
| 深入反思 AI 协作 | +2% | AI 审查报告包含深入反思，提出有价值的改进建议 |

### 扣分项

| 项目 | 扣分 | 说明 |
|------|------|------|
| SHAP 值解释为因果 | -10% | 说"收入贡献 +0.3，所以提高收入会降低违约"（严重错误） |
| 未识别群体性能差异 | -10% | 只看整体 AUC，未按敏感特征分组（遗漏偏见） |
| 差异影响比 < 0.8，却说"公平" | -10% | 未识别法律风险（严重） |
| 向非技术读者用术语 | -5% | 对客户说"SHAP 值"或"AUC"（对方听不懂） |
| 未设置 random_state | -5% | SHAP 值不可复现 |
| 审查清单无证据 | -5% | 只打勾，没有数据或分析支撑 |
| 未检测代理变量 | -5% | 删除敏感变量以为解决偏见（未检测间接编码） |

---

## 成绩等级标准

| 等级 | 总分范围 | 对应 GPA | 说明 |
|------|----------|----------|------|
| A | 90-100 | 4.0 | 优秀 |
| B | 80-89 | 3.0-3.7 | 良好 |
| C | 70-79 | 2.0-2.7 | 合格 |
| F | 0-69 | 0.0-1.7 | 不合格 |

---

## 学习目标对照

| 学习目标 | 对应任务 | 评分标准 |
|---------|---------|----------|
| 理解全局解释（特征重要性）与局部解释（SHAP）的区别 | 任务 1 | 特征重要性和 SHAP 值的区别分析准确 |
| 能用 SHAP summary_plot 解释"哪些特征最重要" | 任务 1 | summary_plot 解读清晰、能识别特征影响方向 |
| 能用 SHAP force_plot 解释"为什么这个样本被这样预测" | 任务 1 | force_plot 解读准确、能说明基准值和贡献 |
| 理解模型偏见的三个来源（数据、算法、代理变量） | 任务 4 | 三个来源分析深入、连接反馈循环 |
| 能计算常用公平性指标（差异影响比、平等机会、均等几率） | 任务 2 | 三项指标计算正确、诊断准确 |
| 理解差分隐私的基本原理（添加噪声保护隐私） | 任务 5 | 拉普拉斯机制实现正确、ε 对比完整 |
| 能使用伦理审查清单评估模型风险（偏差、公平性、隐私、可复现性） | 任务 3 | 审查清单完整、每项都有证据 |
| 在 StatLab 报告中添加模型解释章节（SHAP、公平性、伦理审查） | StatLab 集成 | XAI 结果正确整合、包含三个版本的解释 |
| 向非技术读者解释模型（客户、产品经理、合规部门） | 任务 3, 6 | 三个版本解释清晰、用客户能懂的语言 |
| 识别 AI 生成的可解释性分析中的虚假解释 | AI 协作练习 | 识别 AI 代码缺失项、修正代码正确 |

---

## 提交检查清单

### 基础作业
- [ ] `shap_explanation.md` 已完成（SHAP 值计算 + 可视化 + 与 Week 08 连接）
- [ ] `fairness_metrics.md` 已完成（三项公平性指标 + 诊断）
- [ ] `ethics_review_checklist.md` 已完成（7 项风险检查 + 证据）

### 进阶作业
- [ ] `bias_source_analysis.md` 已完成（三个偏见来源分析）
- [ ] `differential_privacy.md` 已完成（拉普拉斯机制实现 + ε 对比）

### 挑战作业
- [ ] `report.md` 已更新（添加"模型解释与伦理审查"章节）
- [ ] `xai_ethics.py` 已提交（代码实现）
- [ ] 可视化图表已保存（SHAP summary_plot、force_plot、公平性对比图）

### AI 协作练习
- [ ] `ai_original_shap_code.txt` 已保存（AI 原始输出）
- [ ] `ai_shap_review.md` 已完成（审查清单 + 修正代码 + 反思）

### 通用要求
- [ ] 所有 Markdown 文档格式规范（清晰的章节结构）
- [ ] SHAP 值未解释为因果关系（有警告）
- [ ] 群体性能差异已检查（按敏感特征分组）
- [ ] 差异影响比已诊断（是否 < 0.8）
- [ ] 代理变量已检测（相关性 > 0.5）
- [ ] 伦理审查清单有证据支撑（不是只打勾）
- [ ] 向非技术读者有三个版本的解释（客户、产品经理、合规部门）
- [ ] 所有代码设置了 random_state（确保可复现）
- [ ] 可视化图表清晰可读（标注完整、图例清晰）

---

## 质量闸门标准

### 必须满足（阻塞项）
- 所有基础作业任务（1-3）已完成
- SHAP 值计算正确，可视化清晰
- 三项公平性指标计算正确，诊断准确
- 伦理审查清单完整（7 项都有答案+证据）
- 未将 SHAP 值解释为因果关系（有警告）
- 向非技术读者有清晰的解释（至少一个版本）

### 建议满足（非阻塞但影响成绩）
- 进阶作业至少完成一项（任务 4 或 5）
- 代理变量检测使用 SHAP 依赖图验证（不只是相关性）
- 差分隐私实现包含 ε 对比（展示隐私-实用性权衡）
- StatLab 报告包含三个版本的解释（客户、产品经理、合规部门）

---

## 评分流程建议

### 快速检查（5 分钟）
1. 检查文件是否齐全（基础作业必做）
2. 检查 SHAP 可视化是否存在（summary_plot + force_plot）
3. 检查公平性指标是否计算（差异影响比、平等机会、均等几率）
4. 检查伦理审查清单是否完整（7 项都有答案）
5. 快速扫描是否有"SHAP 值 = 因果"的严重错误

### 详细评分（20-30 分钟）
1. **SHAP 可解释性分析**（30 分）
   - 模型训练和性能评估（5 分）
   - 特征重要性和局限性分析（7 分）
   - SHAP 值计算和可视化（8 分）
   - 与 Week 08 置信区间连接（4 分）

2. **公平性指标计算**（30 分）
   - 群体性能计算（8 分）
   - 三项公平性指标和诊断（20 分）
   - 与 Week 10 混淆矩阵连接（2 分）

3. **伦理审查清单**（25 分）
   - 审查清单完整性（10 分）
   - 代理变量检测（8 分）
   - 隐私风险和可复现性（7 分）

4. **进阶作业**（10 分）
   - 偏见来源分析（7 分）
   - 差分隐私实现（3 分）

5. **AI 协作练习**（5 分）
   - 审查全面性和反思（5 分）

### 反馈建议
- 对优秀学生：建议深入挑战作业（完整 StatLab 集成）
- 对合格学生：指出薄弱环节（如代理变量检测、非技术解释）
- 对不合格学生：提供具体改进建议（如"重新学习 SHAP 值的计算"）
