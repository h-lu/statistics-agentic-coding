# Week 12 研究缓存
生成日期：2026-02-18

## 时代脉搏素材
### 已写入 CHAPTER.md
- 2024年医疗AI算法偏见案例（参考）
- EU AI Act, GDPR, NIST AI RMF 监管趋势

## Context7 技术查证结果

### scikit-learn 特征重要性与排列重要性
来源：Context7 /scikit-learn/scikit-learn

**最佳实践**：
1. 使用 `permutation_importance` 计算排列重要性（比内置 feature_importances_ 更稳健）
2. 排列重要性通过打乱特征值来衡量特征对模型性能的贡献
3. 对于树模型，可以使用内置 `feature_importances_` 但要意识到它偏向高基数特征

```python
from sklearn.inspection import permutation_importance

# 计算排列重要性
perm_importance = permutation_importance(
    rf, X_test, y_test,
    n_repeats=10,  # 多次重复以估计方差
    random_state=42
)

# 打印带置信区间的结果
for i in perm_importance.importances_mean.argsort()[::-1]:
    if perm_importance.importances_mean[i] - 2 * perm_importance.importances_std[i] > 0:
        print(f"{feature_names[i]:<8}: {perm_importance.importances_mean[i]:.3f} ± {perm_importance.importances_std[i]:.3f}")
```

### SHAP 库使用
来源：训练知识 + scikit-learn 文档

**最佳实践**：
1. 对于树模型使用 `shap.TreeExplainer`（高效）
2. 对于其他模型使用 `shap.KernelExplainer`（慢但通用）
3. 使用 `shap.summary_plot` 展示全局模式
4. 使用 `shap.force_plot` 或 `shap.waterfall_plot` 解释单个预测

## AI 小专栏 #1: AI 时代的可解释性为什么更重要
### 搜索词: "explainable AI regulations EU AI Act 2026"

**已知信息**：
- **欧盟 GDPR**：第 22 条赋予个人"不接受纯自动化决策"的权利
- **欧盟 AI Act**：将 AI 系统按风险分类，"高风险"系统必须提供可解释性
- **美国 NIST AI RMF**：要求 AI 系统"透明、可解释、可问责"
- **中国《深度合成管理规定》**：要求深度合成服务提供者添加标识

**行业实践**：
- Google Model Cards
- IBM AI Facts 360
- Microsoft InterpretML
- H2O.ai 可解释性模块

**参考链接**（需验证）：
- https://gdpr-info.eu/art-22-gdpr/
- https://artificialintelligenceact.eu/
- https://www.nist.gov/itl/ai-risk-management-framework
- https://modelcards.withgoogle.com/

## AI 小专栏 #2: AI 模型中的偏见检测与缓解
### 搜索词: "algorithmic fairness bias detection tools 2026"

**已知信息**：
- **Fairlearn**（Microsoft）：评估和缓解 ML 模型不公平性
- **AIF360**（IBM AI Fairness 360）：70+ 公平性指标和 10+ 偏见缓解算法
- **What-If Tool**（Google）：交互式可视化工具
- **Fairness Indicators**（Google）：TensorFlow 扩展

**偏见缓解策略**：
| 阶段 | 策略 | 示例 |
|------|------|------|
| 数据预处理 | 重采样、表示学习 | 过采样少数群体 |
| 模型训练 | 约束优化、正则化 | 公平学习 |
| 模型后处理 | 阈值调整、校准 | 不同群体不同阈值 |

**行业案例**：
- Amazon 招聘工具（2018）：对女性简历系统性降权
- COMPAS 累犯预测（2016）：黑人误判率更高
- Healthcare 算法（2019）：使用医疗费用作为代理导致对黑人低估
- Apple Card 信用额度（2019）：性别歧视投诉

**参考链接**（需验证）：
- https://fairlearn.org/
- https://aif360.mybluemix.net/
- https://pair-code.github.io/what-if-tool/
- https://github.com/tensorflow/fairness-indicators

## TODO（需联网搜索补充）
1. 验证 AI 小专栏中的参考链接是否有效
2. 搜索 2025-2026 年最新的算法公平性监管案例
3. 补充 SHAP 库最新版本的 API 变化
