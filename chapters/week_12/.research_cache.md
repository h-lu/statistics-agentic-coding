# Week 12 研究缓存
生成日期：2026-02-12

## 时代脉搏素材

### 搜索词: "EU AI Act explainability requirements 2025 2026"

- 事实：欧盟《AI法案》于2024年8月1日正式生效，2026年8月2日高风险AI系统需完全符合透明度要求（包括可解释性）
- 来源：搜索结果 - EU AI Act Explorer, European Commission AI Act Service Desk
- 关键时间节点：
  - 2024年8月1日：EU AI Act生效
  - 2025年2月2日：不可接受风险的AI系统禁令生效
  - 2025年8月2日：通用AI（GPAI）的透明度义务开始
  - 2026年8月2日：高风险AI系统完全合规截止（Article 13透明度与可解释性要求）

### 搜索词: "GDPR right to explanation automated decision making 2025 2026"

- 事实：GDPR第22条规定数据主体有权获得"关于自动化决策的逻辑"的有意义信息
- 来源：搜索结果 - Advocate General Opinion Article 22 (Schufa 2024)
- 技术实施挑战：法案要求透明度但提供的技术指导有限

### 搜索词: "algorithmic fairness bias hiring credit 2025 2026 cases"

- 事实：2026年1月，Eightfold AI面临集体诉讼（FCRA和加州CCRAA指控），其AI评估工具使用申请人无法访问的数据
- 事实：Workday AI招聘偏见诉讼——AI算法导致40岁以上申请人被不成比例地排除
- 事实：Harper v. Sirius XM（2025年10月）——挑战算法招聘歧视的案例
- 事实：FTC在2025-2026年加强使用联邦法解决算法招聘和信贷中的歧视行为
- 事实：VoxDev研究（2025年5月）发现AI招聘工具系统性地偏好女性申请人而非黑人男性
- 来源：搜索结果 - Forbes分析、法律案例报道

## AI 小专栏 #1: GDPR 的"解释权"——为什么可解释AI从学术问题变成法律要求

### 搜索词: "EU AI Act 2024 explainability"
- 数据点：2024年欧盟AI法案正式生效，Article 13要求高风险AI系统必须"设计并开发得足够透明"
- 数据点：高风险AI系统包括信用评估、招聘、医疗——需要用户能解读系统输出、理解系统能力与限制
- 数据点：技术实施挑战——缺乏具体实施标准
- 数据点：实施策略——可解释模型、事后解释、技术文档、用户培训、人工监督
- 参考（来源：EU AI Act搜索结果）：
  - EU AI Act Explorer
  - European Commission AI Act Service Desk
  - Trilateral Research legal analyses (2024-2026)

### 搜索词: "GDPR right to explanation 2025"
- 数据点：GDPR第22条规定自动化决策的"解释权"
- 数据点：2024年Schufa总检察长意见（Article 22）讨论了有意义的解释权
- 参考（来源：Advocate General Opinion）：
  - Advocate General Opinion Article 22 (Schufa 2024)

## AI 小专栏 #2: 算法公平性——从技术指标到社会正义

### 搜索词: "algorithmic fairness 2025 2026 cases"
- 数据点：Eightfold AI集体诉讼（2026年1月）——AI评估工具使用不可访问数据
- 数据点：Workday招聘偏见诉讼——AI排除40岁以上申请人
- 数据点：FTC执法行动（2025-2026）——用联邦法解决算法歧视
- 数据点：VoxDev研究（2025年5月）——AI招聘工具系统性偏好女性
- 数据点：Forbes分析（2026）——放缓的招聘将压力测试招聘系统，暴露更多AI偏见案例
- 参考（来源：搜索结果）：
  - Forbes分析（2026）
  - 法律案例报道（Harper v. Sirius XM, 2025）
  - VoxDev研究（2025年5月）

### 搜索词: "AI fairness metrics machine learning 2026"
- 数据点：公平性指标包括差异影响、平等机会、均等几率
- 数据点：公平性评估需按群体分别计算性能指标

## 技术参考：SHAP 库

### 搜索词: "SHAP SHAPley values model explanation Python tutorial 2026"

- 来源1：Towards Data Science - "When Shapley Values Break: A Guide to Robust Model Explainability"（2026年1月15日）
  - URL: https://towardsdatascience.com/when-shapley-values-break-a-guide-to-robust-model-explainability/
  - 关键点：SHAP值可能具有误导性，需要控制ground truth来验证

- 来源2：Michael Brenndoerfer - "SHAP Complete Guide"（2025年7月15日）
  - URL: https://mbrenndoerfer.com/writing/shap-shapley-additive-explanations-complete-guide-model-interpretability-feature-attribution
  - 关键点：SHAP是行业标准，但需要理解其局限性

- 来源3：Medium - "SHAP Values Explained"（2024年9月19日）
  - URL: https://medium.com/biased-algorithms/shap-values-explained-08764ab16466
  - 关键点：基于Shapley值思想，每个特征 deserve credit

- 来源4：SHAP官方文档
  - URL: https://shap.readthedocs.io/en/stable
  - 关键点：TreeExplainer用于树模型，支持summary_plot和force_plot

## 技术参考：差分隐私

### 搜索词: "differential privacy epsilon delta explain 2026"

- 来源1：Programming Differential Privacy
  - URL: https://programming-dp.com/chapter3.html
  - 关键点：ε参数称为隐私参数/隐私预算，小值=更高隐私，大值=更少隐私
  - 关键点：经验规则：ε应在1或更小，>10可能保护效果有限

- 来源2：IEEE Xplore论文 "Explaining Epsilon"（2025年11月13日）
  - URL: https://ieeexplore.ieee.org/document/9583708/
  - 关键点：ε设置没有通用答案，实际应用中需要仔细权衡

## 技术参考：scikit-learn 置换特征重要性

- 来源：scikit-learn文档（Context7查询结果）
  - 函数：sklearn.inspection.permutation_importance
  - 关键点：模型无关的方法，通过打乱特征值后性能下降来衡量特征重要性
  - 适用：任何拟合的预测模型，特别适用于非线性或不透明估计器

---

**注**：以上搜索结果已整理供prose-polisher使用。部分搜索工具遇到限流，数据可能不完整，prose-polisher可以补充搜索。
