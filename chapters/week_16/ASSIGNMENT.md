# Week 16 作业：从分析到交付——终稿报告与展示

> "可复现性不是'能跑'，而是'任何人跑一遍都能得到相同结果'。"
> — 老潘

## 作业概述

本周作业将带你完成一个完整的**可复现报告交付流程**：从零散的 Jupyter Notebook 到脚本化的分析流水线，从手工复制粘贴到一键生成 Markdown/HTML 报告，从"感觉差不多了"到系统化的审计清单，从堆砌结果到清晰的展示叙事。

你将把 16 周的 StatLab 超级线收敛成一份**可复现、可审计、可对外展示**的终稿分析报告。

---

## 基础层（必做）

### 任务 1：可复现性自查——你的报告经得起审查吗？

小北本周把所有 Jupyter Notebook 导出成 PDF，订成一叠就说："这是我的报告。"

老潘的回应只有一个问题："**如果我现在给你同一份数据，你能一键跑出这份报告吗？**"

**你的任务**：

1. 回答以下问题：
   - 什么是可复现报告流水线？它包含哪些核心要素？
   - Jupyter Notebook 为什么不适合作为最终交付物？
   - 什么是"能重跑"和"可复现"的区别？

2. 检查你当前的 StatLab 项目：
   - 列出你目前的文件结构（如 Notebook、脚本、数据文件等）
   - 指出哪些地方不符合可复现性要求
   - 写出改进计划

**输入示例**（仅供参考格式）：

```
问题 1：什么是可复现报告流水线？

可复现报告流水线是一个系统，它把数据分析从"人工流程"变成"自动化脚本"。

核心要素：
1. 数据：数据来源明确、版本记录清晰
2. 代码：分析脚本模块化、有明确输入输出
3. 环境：依赖版本记录（requirements.txt）
4. 随机种子：固定所有随机操作
5. 决策记录：写清楚数据清洗、模型选择的理由
6. 报告生成：脚本自动生成，不是手工编辑

问题 2：为什么 Notebook 不适合交付？

- 执行顺序不保证：单元格可能乱序执行
- 状态可能混乱：中间变量可能被修改
- 版本控制困难：输出结果也被计入 diff
- 不适合自动化：难以集成到 CI/CD 流程

问题 3："能重跑" vs "可复现"

- 能重跑：同一台机器、同一个人、手工操作能跑出来
- 可复现：换一台机器、换一个人、用脚本一键执行，能得到完全相同的结论

检查我的当前项目：

当前问题：
- 有 15 个 Jupyter Notebook，没有统一的入口脚本
- 没有固定随机种子（每次运行结果不同）
- 没有记录依赖版本
- 图表保存路径不统一

改进计划：
1. 创建 generate_report.py 作为入口脚本
2. 在脚本开头固定所有随机种子
3. 添加 requirements.txt
4. 统一图表保存路径到 figures/ 目录
```

**提交物**：
- 一段文字（500-700 字）回答上述问题
- 你当前项目的文件结构清单
- 改进计划（3-5 条）

**评分点**：
- [ ] 正确解释了可复现报告流水线的核心要素
- [ ] 说明了 Notebook 不适合交付的原因
- [ ] 区分了"能重跑"和"可复现"
- [ ] 列出了当前项目的问题
- [ ] 给出了具体的改进计划

**常见错误**：
- 认为"能跑"就是"可复现"
- 没有识别随机种子问题
- 改进计划太笼统（如"整理代码"而不是"创建入口脚本"）
- 没有检查依赖版本记录

---

### 任务 2：构建报告生成脚本——从原始数据到 Markdown

阿码本周发现：每次重新跑分析后，他要花 40 分钟更新报告——截图、复制表格、调整格式。

老潘说："**让代码来做这件事。这不是分析，这是排版。**"

**你的任务**：

1. 创建一个报告生成脚本 `generate_report.py`：
   - 从原始数据开始，加载并清洗
   - 按顺序调用 16 周的分析函数
   - 用 f-string 或 Jinja2 生成 Markdown 内容
   - 将结果写入 `report.md`

2. 在报告开头添加"可复现信息"章节：
   - 数据来源
   - 依赖版本
   - 随机种子
   - 报告生成时间

**输入示例**（仅供参考格式，不要直接复制）：

```python
# 伪代码示例，展示结构
import pandas as pd
import numpy as np
from datetime import datetime

# 固定随机种子
np.random.seed(42)

# 记录元信息
REPORT_INFO = {
    "data_source": "https://example.com/data.csv",
    "data_date": "2026-02-01",
    "report_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "random_seed": 42,
}

# 1. 数据加载与清洗
df = load_and_clean_data("data/raw/data.csv")

# 2. 描述统计
desc_stats = compute_descriptive_stats(df)

# 3. 统计检验
test_results = run_hypothesis_tests(df)

# 4. 建模与评估
model_results = train_and_evaluate_model(df)

# 5. 生成 Markdown
markdown = f"""# {REPORT_TITLE}

## 可复现信息

- 数据来源：{REPORT_INFO['data_source']}
- 数据日期：{REPORT_INFO['data_date']}
- 报告生成时间：{REPORT_INFO['report_date']}
- 随机种子：{REPORT_INFO['random_seed']}

## 数据概览

样本数：{len(df)}
特征数：{df.shape[1]}

{generate_table(desc_stats)}

## 统计检验

{generate_table(test_results)}

## 建模与评估

模型准确率：{model_results['accuracy']:.1%}

![ROC 曲线](figures/roc_curve.png)

## 结论与限制

{generate_conclusion(model_results)}
"""

# 6. 写入文件
with open("report.md", "w") as f:
    f.write(markdown)
```

**输出示例**（仅供参考格式）：

生成的 `report.md` 应该包含：

```markdown
# 客户流失分析报告

## 可复现信息

- 数据来源：https://example.com/data.csv
- 数据日期：2026-02-01
- 报告生成时间：2026-02-21 14:30:00
- 随机种子：42

## 数据概览

样本数：5000
特征数：15

...

## 统计检验

...

## 建模与评估

模型准确率：82.3%

![ROC 曲线](figures/roc_curve.png)

## 结论与限制

...
```

**提交物**：
- 报告生成脚本
- 生成的 `report.md` 文件
- 一段文字说明你的脚本结构（200-300 字）

**评分点**：
- [ ] 脚本可以从上到下线性执行
- [ ] 固定了随机种子
- [ ] 包含可复现信息章节
- [ ] 自动生成 Markdown，不是手工编辑
- [ ] 图表路径引用正确

**常见错误**：
- 没有固定随机种子
- 脚本需要交互式输入
- 图表路径硬编码，不统一
- 没有记录依赖版本
- 手工编辑 report.md（脚本重新生成时会覆盖）

---

### 任务 3：创建审计清单——系统化检查报告质量

"我觉得报告写得差不多了。"阿码把 `report.md` 往桌上一推。

老潘反问："**如果我是你的评审，我能在一小时内找出三个你没想到的问题，你敢不敢赌？**"

**你的任务**：

1. 创建一个审计清单 `audit_checklist.md`：
   - 数据与可复现性（4 项）
   - 统计假设与方法（4 项）
   - 诚实性与透明度（4 项）
   - 叙事与结构（4 项）

2. 用脚本自动检查部分项目：
   - 检查"数据来源"是否在报告中
   - 检查"random_seed"是否在报告中
   - 检查"置信区间"是否在报告中
   - 检查"假设"或"残差"是否在报告中

3. 完成自检并记录问题：
   - 逐项勾选审计清单
   - 记录发现的问题
   - 说明如何修复

**输入示例**（仅供参考格式）：

```python
# 自动检查脚本示例
def audit_reproducibility(report_path: str) -> dict:
    """检查报告的可复现性。"""
    with open(report_path) as f:
        content = f.read()

    checks = {
        "data_source_mentioned": "数据来源" in content or "data source" in content,
        "random_seed_fixed": "random_seed" in content or "seed" in content or "随机种子" in content,
        "confidence_intervals": "置信区间" in content or "95% CI" in content or "confidence interval" in content,
        "assumptions_checked": "假设" in content or "残差" in content or "residual" in content,
        "limitations_mentioned": "限制" in content or "局限" in content or "limitation" in content,
    }

    return checks

if __name__ == "__main__":
    checks = audit_reproducibility("report.md")
    print("## 可复现性审计")
    for item, passed in checks.items():
        status = "✅" if passed else "❌"
        print(f"{status} {item}")
```

**审计清单示例**（仅供参考格式）：

```markdown
# 审计清单

## 可复现性
- [ ] 数据来源明确
- [ ] 随机种子固定
- [ ] 依赖版本记录
- [ ] 代码可运行

## 统计假设
- [ ] 检验前提验证（正态性、方差齐性）
- [ ] 模型诊断（残差图、QQ 图）
- [ ] 置信区间报告
- [ ] 多重比较校正（如适用）

## 诚实性
- [ ] 图表样本量标注
- [ ] 缺失处理说明
- [ ] 因果声明边界（不说"证明"，说"支持"）
- [ ] 模型限制说明

## 叙事与结构
- [ ] 研究问题清晰
- [ ] 方法可追溯
- [ ] 结果与讨论分离
- [ ] 结论不夸大
```

**提交物**：
- 审计清单文件
- 自动检查脚本
- 自检结果（勾选的清单 + 发现的问题 + 修复说明）

**评分点**：
- [ ] 审计清单覆盖 4 个维度
- [ ] 实现了自动检查脚本
- [ ] 完成了自检并记录问题
- [ ] 说明了如何修复发现的问题

**常见错误**：
- 审计清单太简略（每项只有一句话）
- 没有实现自动检查
- 只勾选没有记录问题
- 没有说明修复方案

---

## 进阶层（推荐完成）

### 任务 4：HTML 导出与样式美化——让报告更专业

阿码问："**能不能把 Markdown 转成 HTML？这样更方便分享。**"

老潘说："**可以。而且 HTML 可以加样式，看起来更专业。**"

**你的任务**：

1. 将 `report.md` 转换为 `report.html`：
   - 使用 Pandoc 或 Python 库（markdown + jinja2）
   - 添加自定义 CSS 样式
   - 确保图表正常显示

2. 在报告中添加导航：
   - 目录（Table of Contents）
   - 返回顶部按钮
   - 章节锚点

3. 回答：HTML 版本相比 Markdown 版本有什么优势？

**输入示例**（仅供参考格式）：

```bash
# 使用 Pandoc 转换
pandoc report.md -o report.html --standalone \
  --toc --toc-depth=3 \
  --css=style.css \
  --highlight-style=pygments
```

或使用 Python：

```python
import markdown
from jinja2 import Template

# 读取 Markdown
with open("report.md") as f:
    md_content = f.read()

# 转换为 HTML
html_content = markdown.markdown(
    md_content,
    extensions=['tables', 'fenced_code', 'toc']
)

# 用模板包装
template = Template("""
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="style.css">
    <title>分析报告</title>
</head>
<body>
    <nav class="toc">{{ toc }}</nav>
    <main>{{ content }}</main>
    <a href="#" class="back-to-top">↑ 返回顶部</a>
</body>
</html>
""")

html_output = template.render(
    content=html_content,
    toc=markdown.markdown(md_content, extensions=['toc'])
)

with open("report.html", "w") as f:
    f.write(html_output)
```

**CSS 示例**（仅供参考格式）：

```css
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
    line-height: 1.6;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
}

h1, h2, h3 {
    color: #333;
}

table {
    border-collapse: collapse;
    width: 100%;
}

th, td {
    border: 1px solid #ddd;
    padding: 8px;
}

img {
    max-width: 100%;
}

.back-to-top {
    position: fixed;
    bottom: 20px;
    right: 20px;
}
```

**提交物**：
- `report.html` 文件
- CSS 样式文件（如果使用）
- 转换脚本或命令
- 一段文字说明 HTML 版本的优势（200-300 字）

**评分点**：
- [ ] 成功转换 Markdown 为 HTML
- [ ] 添加了自定义样式
- [ ] 图表正常显示
- [ ] 说明了 HTML 版本的优势

**常见错误**：
- 图表路径不正确（HTML 中相对路径问题）
- CSS 样式没有应用
- 中文编码问题
- 没有添加导航

---

### 任务 5：展示叙事结构——准备 5 分钟演讲

小北把 16 周的所有内容都做进了 PPT，一共 127 页。

老潘说："**5 分钟，听众只能记住 3 个点。你需要的是故事，不是百科全书。**"

**你的任务**：

1. 按照"问题-方法-发现-边界-反思"结构准备展示材料：
   - 问题（1-2 分钟）：为什么要做这个分析？
   - 方法（2-3 分钟）：你怎么分析？用什么方法？
   - 发现（4-5 分钟）：你发现了什么？用图表
   - 边界（1-2 分钟）：这个结论有什么限制？
   - 反思（1 分钟）：这个分析意味着什么？

2. 为每张幻灯片写"听众应该看到什么"的说明

3. 回答：如何避免"堆砌结果"？

**展示结构示例**（仅供参考格式）：

```markdown
# 展示脚本

## 开场（30 秒）
"我们公司的客户流失率在上升。产品经理问：哪些客户最容易流失？如果能提前预测，可以采取什么措施？这份分析就是为了回答这两个问题。"

## 问题（1 分钟）
"我们要回答两个问题：
1. 哪些客户最容易流失？
2. 如果能提前预测，可以采取什么措施？"

## 方法（2 分钟）
"我们分析了 5000 个客户的行为数据，包括使用时长、消费金额、客服联系次数等 15 个变量。
我们用了逻辑回归预测流失，用 SHAP 值解释模型。
在分析前，我们检查了数据的缺失机制。"

## 发现（4 分钟）
"这张图是我们最重要的发现（指向 ROC 曲线）。
模型的 AUC 是 0.82，这意味着如果我们用模型识别前 20% 高风险客户，能捕获 60% 的实际流失者。

更重要的是，SHAP 值告诉我们（指向特征重要性图）：'使用时长'和'客服联系次数'是最大的预测因子。
这给我们一个可操作的建议：主动联系低活跃客户。"

## 边界（1 分钟）
"这个分析有三个限制：
1. 数据代表性：数据来自 2025 年 Q1-Q2，季节性变化可能影响外推性
2. 因果限制：我们发现'客服联系次数'与流失相关，但不知道因果关系方向
3. 模型不确定性：置信区间显示，对于新客户预测的不确定性仍然较大"

## 反思（1 分钟）
"回到最开始的问题：哪些客户最容易流失？我们可以提前做什么？
分析给出了三个建议：
1. 优先干预前 20% 高风险客户
2. 关注使用时长和客服联系次数
3. 下一步做 A/B 测试验证'主动联系'的效果"
```

**提交物**：
- 展示脚本
- 关键图表清单（每张图一句话说明"听众应该看到什么"）
- 一段文字说明如何避免"堆砌结果"（200-300 字）

**评分点**：
- [ ] 遵循"问题-方法-发现-边界-反思"结构
- [ ] 每个环节有时长控制
- [ ] 为每张图写了说明
- [ ] 说明了如何避免堆砌结果

**常见错误**：
- 开头介绍工具而不是问题
- 堆砌所有图表
- 没有说明不确定性
- 没有回到原始问题收束

---

## 挑战层（可选）

### 任务 6：同行评审——交换报告并审计

老潘说："**所有作者都有盲区。你需要一个系统化的检查清单，不是靠'感觉差不多了'**。"

**你的任务**：

1. 与同学交换报告：
   - 你审 A 的，A 审你的
   - 按审计清单逐项检查
   - 至少指出 2 个可改进点

2. 写评审意见：
   - 具体指出问题（如"报告没有说明随机种子"）
   - 给出改进建议（如"在可复现信息章节添加随机种子"）

3. 作者回应：
   - 接受或拒绝建议
   - 说明理由

**评审意见模板**（仅供参考格式）：

```markdown
# 同行评审意见

## 审查对象
报告作者：XXX
审查日期：2026-02-21

## 审查结果

### ✅ 通过的项
- [x] 数据来源明确
- [x] 随机种子固定
- [x] 图表清晰

### ⚠️ 需要改进的项

**问题 1：报告没有说明数据清洗步骤**
- 位置："数据概览"章节
- 建议：添加一个"数据清洗"小节，说明缺失值处理、异常值处理策略
- 理由：可复现性要求写清楚所有决策

**问题 2：模型没有报告置信区间**
- 位置："建模与评估"章节
- 建议：为模型预测添加置信区间（如用 Bootstrap）
- 理由：不确定性量化是统计分析的核心

### 其他建议
- 建议在"结论与限制"中补充模型的适用范围
- 建议添加"下一步"小节，说明如何验证结论

## 总体评价
报告结构清晰，分析完整。主要改进点是补充数据清洗说明和不确定性量化。
```

**作者回应模板**（仅供参考格式）：

```markdown
# 作者回应

### 接受的建议
- [x] 添加"数据清洗"小节
- [x] 为模型预测添加置信区间

### 拒绝的建议
- [ ] 添加"下一步"小节
  - 理由：当前报告已经超过预期长度，下一步工作可以作为后续研究

### 修改记录
- 2026-02-21: 添加了"数据清洗"小节
- 2026-02-21: 用 Bootstrap 计算了置信区间
```

**提交物**：
- 你写给同学的评审意见
- 同学写给你的评审意见（可匿名）
- 你的回应（接受/拒绝 + 理由）
- 修改后的报告（如果有修改）

**评分点**：
- [ ] 按审计清单逐项检查
- [ ] 至少指出 2 个可改进点
- [ ] 给出了具体的改进建议
- [ ] 作者回应清晰（接受/拒绝 + 理由）

**常见错误**：
- 评审意见太笼统（如"写得不错"）
- 没有给出具体的改进建议
- 作者没有回应所有建议
- 评审不客观（过于严厉或过于宽松）

---

### 任务 7：CI/CD 集成——自动化报告生成

老潘说："**在公司里，每次数据更新都要重新跑分析，生成报告。这个流程应该自动化。**"

**你的任务**：

1. 创建一个 GitHub Actions workflow（或类似的 CI/CD 配置）：
   - 每次推送代码时自动运行分析
   - 生成报告并部署到 GitHub Pages（或其他平台）

2. 或创建一个简单的 Makefile/shell 脚本：
   - 一条命令完成：数据加载 → 分析 → 报告生成

**GitHub Actions 示例**（仅供参考格式）：

```yaml
# .github/workflows/generate_report.yml
name: Generate Report

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run analysis
      run: |
        python generate_report.py

    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./output
```

**Makefile 示例**（仅供参考格式）：

```makefile
.PHONY: all clean data report

all: report

data:
	python scripts/download_data.py

report: data
	python generate_report.py

clean:
	rm -rf output/* figures/*.png

deploy: report
	@echo "Deploying report..."
	@# 添加部署命令
```

**提交物**：
- CI/CD 配置文件或 Makefile
- 一段文字说明如何使用（200-300 字）
- 自动生成的报告截图（如果成功）

**评分点**：
- [ ] 创建了自动化脚本
- [ ] 一条命令完成报告生成
- [ ] 说明了如何使用
- [ ] （可选）成功部署到网页

**常见错误**：
- 脚本只能在本地运行，不能在 CI 环境运行
- 没有处理依赖安装
- 路径问题（相对路径在 CI 环境中可能不同）
- 没有说明如何使用

---

## AI 协作练习（可选，主导期）

老潘说："**AI 可以帮你润色文字、优化表达，但你要保留审查记录。这是 human-in-the-loop 的证据。**"

### 练习 1：AI 润色审查

下面这段文字是某个 AI 工具生成的"模型结论"：

> "逻辑回归模型成功预测了客户流失，准确率达到 82.3%，证明模型性能优秀。SHAP 分析显示，使用时长和客服联系次数是最重要的预测因子，说明这两个因素直接影响流失。建议公司优先关注低活跃客户，通过主动联系来降低流失率。"

**审查清单**：
- [ ] "证明"这个词合适吗？统计结论可以说"证明"吗？
- [ ] "直接影响"——这是因果声明吗？有证据支持吗？
- [ ] "模型性能优秀"——有基准比较吗？82.3% 算好吗？
- [ ] 缺少了什么？不确定性？置信区间？模型的适用范围？
- [ ] "主动联系来降低流失率"——这个建议有实验支持吗？还是只是推测？

**你的修订版**（用你自己的话写，修正上述问题）：

```
例如："AI 的结论有几个问题。第一，'证明'这个词不合适——统计检验只能'支持'假设，不能'证明'。第二，'直接影响'是因果声明，但观察数据只能发现相关，不能证明因果。第三，'模型性能优秀'缺少基准比较——82.3% 比随机猜测好多少？第四，完全缺少不确定性表达——置信区间是多少？模型的不确定性如何？第五，建议'主动联系'没有实验支持，这需要 A/B 测试验证。修订版应该用'支持'代替'证明'，用'相关'代替'直接'，补充基准比较和置信区间，明确建议需要实验验证。"
```

---

### 练习 2：记录 AI 使用日志

如果你在本周作业中使用了 AI（如 ChatGPT、Claude 等），请记录：

**采纳的建议**：
- 原文：...
- AI 建议：...
- 理由：为什么采纳

**拒绝的建议**：
- AI 建议：...
- 理由：为什么拒绝

**自己的修改**：
- 你在 AI 输出基础上做了哪些修改？

**AI 使用日志模板**：

```markdown
# AI 使用日志

## 使用场景
- [ ] 报告文字润色
- [ ] 代码调试
- [ ] 图表优化
- [ ] 展示脚本编写
- [ ] 其他：...

## 采纳的建议

### 建议 1
- **原文**："模型证明了客服联系次数导致流失"
- **AI 建议**：改为"模型显示客服联系次数与流失相关"
- **理由**：避免因果声明，改为相关

### 建议 2
- **原文**：（没有）
- **AI 建议**：在"可复现信息"中添加 Python 版本号
- **理由**：有助于其他人复现环境

## 拒绝的建议

### 建议 1
- **AI 建议**：删除置信区间误差棒，"让图表更简洁"
- **理由**：不确定性量化是核心原则，不能删除

### 建议 2
- **AI 建议**：用"显著"代替" statistically significant"
- **理由**：这是统计学术语，应该保留

## 自己的修改
- 补充了模型假设检查的章节
- 添加了数据代表性的限制说明
- 调整了展示结构，让"边界"部分更突出
```

**提交物**：
- 审查清单（勾选哪些问题存在）
- 你的修订版（3-5 句话）
- （如果使用 AI）AI 使用日志

---

## StatLab 本周任务

老潘说："**16 周的 StatLab 超级线，本周要收敛成终稿报告**。"

**你的任务**：

1. **创建报告生成流水线**：
   - 创建 `generate_report.py` 入口脚本
   - 调用 16 周的分析函数
   - 生成 `report.md` 和 `report.html`

2. **完成审计清单**：
   - 创建 `audit_checklist.md`
   - 逐项自检
   - 记录问题并修复

3. **准备展示材料**：
   - 写展示脚本（5 分钟版本）
   - 列出关键图表和说明
   - （可选）创建幻灯片

4. **（可选）AI 辅助润色**：
   - 用 AI 润色报告文字
   - 记录采纳/拒绝的建议
   - 保留审查记录

**终稿报告结构**：

```markdown
# [你的分析主题] 报告

## 可复现信息
- 数据来源：...
- 数据日期：...
- 报告生成时间：...
- 随机种子：...
- 依赖版本：...

## 研究问题
...

## 数据概览
...

## 数据清洗
...

## 描述统计
...

## 统计检验
...

## 建模与评估
...

## 可解释性分析
...

## 因果推断（如适用）
...

## 高级分析（如适用）
...

## 结论与限制
...

## 参考文献
...
```

**提交物**：
- `report.md`（终稿报告）
- `report.html`（可选，HTML 版本）
- `generate_report.py`（报告生成脚本）
- `audit_checklist.md`（审计清单 + 自检结果）
- `ai_usage_log.md`（如果使用 AI）
- 展示脚本和关键图表清单
- 所有图表（`figures/` 目录）

---

## 提交检查清单

在提交作业前，请确认：

- [ ] 报告生成脚本可以从上到下线性执行
- [ ] 固定了所有随机种子
- [ ] 包含可复现信息章节
- [ ] 创建了审计清单并完成自检
- [ ] 准备了展示脚本（5 分钟版本）
- [ ] 如果使用 AI，保留了审查记录
- [ ] 代码可以运行（或注明哪些部分是伪代码）
- [ ] 分析部分用你自己的话写（不是复制粘贴）
- [ ] 图表路径引用正确
- [ ] 如果遇到困难，参考了 `starter_code/solution.py`，请说明参考了哪些部分

---

## 提示与帮助

如果你在完成作业时遇到困难：

1. 回顾 CHAPTER.md 中的示例代码
2. 参考本周的 StatLab 示例（`examples/16_report_pipeline.py`、`examples/16_audit_checklist.py`）
3. 查阅相关工具文档：
   - Pandoc: https://pandoc.org/
   - Quarto: https://quarto.org/
   - Python markdown 库: https://python-markdown.github.io/
4. 如果你对报告生成脚本的结构不熟悉，可以参考 `starter_code/solution.py`（但不要直接复制）

**记住**：本周的核心是**从"能跑"到"能交付"**。可复现、可审计、诚实——这三个原则会伴随你整个职业生涯。AI 可以加速，但你要负责。

---

祝你本周学习愉快！记住老潘的话：**"统计学教你怎么分析，工程教你怎么交付。AI 可以加速，但你要负责。交付的核心是'信任'——可复现、可审计、诚实，这三点建立信任。"**
