# Week 16：从分析到交付——完整数据项目的终章

> "The goal of statistics is to turn data into information, and information into insight."
> — Marilyn vos Savant

> 2025-2026 年，翻开任何一份数据岗位的 JD，你都会看到"能讲数据故事"、"能把分析结果呈现给非技术决策者"——这些曾经是"软技能"的要求，如今已成为硬通货。
>
> 问题的本质变了。五年前，你可能只需要"跑个回归，把 R² 发给老板"；今天，产品经理、业务负责人、甚至 CEO 都会直接问你："所以呢？我们该怎么做？"、"这个结论有多可靠？"、"如果环境变了，结论还成立吗？"
>
> 他们要的不是 P 值，而是**可执行的洞察与清晰的边界**。
>
> 本周是 16 周的收尾：你不再学习新的统计方法，而是把过去 15 周的积累——从数据卡（Week 01）到因果推断（Week 13），从假设检验（Week 06）到贝叶斯更新（Week 14），从回归建模（Week 09）到流式统计（Week 15）——**收敛成一份可复现、可审计、可对外展示的分析报告**。
>
> 这个过程不是"整理代码"，而是**从技术实现到数据故事的跃迁**。你需要回答三个问题：分析回答了什么？结论有多可靠？结论不适用于什么场景？这三个问题对应本周的三个核心概念：**数据故事**（Data Storytelling）、**可复现分析**（Reproducible Analysis）、**结论边界**（Limitations）。
>
> 当你完成本周的学习，你手里的不再是"一堆散落的代码和图表"，而是一份"从问题到证据到不确定性到边界"的完整叙事——这是统计学家与数据科学家的分水岭。

---

## 前情提要

上周（Week 15），小北学会了处理高维数据的困境——用 PCA 把 5000 个特征压缩成 47 个主成分，用 K-means 发现 5 个用户群，用流式统计实时更新统计量，用 A/B 测试工程化自动决策。他现在的报告已经包含了降维、聚类、流式计算等高级方法。

但新的问题出现了。

产品经理推门进来："小北，下周就要期末展示了。你能把这份报告'讲给我听'吗？不是'这张图是什么'，而是'我们该怎么做'？"

小北愣住了。他的报告里有 p 值、有置信区间、有主成分载荷、有聚类中心——但没有"故事"。

老潘走过来，翻了翻报告："技术上是完整的，但叙事上是一堆碎片。你需要**数据故事**——把问题、证据、不确定性、边界串成一条线，让听众能跟着你走。"

阿码好奇："数据故事不就是'把结果总结一下'吗？"

"不是，"老潘说，"**数据故事是结构化思维**——你必须回答：我在回答什么问题？我的证据有多强？我的结论在哪里不适用？"

"那可复现分析呢？"小北问，"我的代码能跑，这算可复现吗？"

老潘摇头："**能跑 ≠ 可复现**。可复现意味着：换了一个人、换了一台机器，用同样的数据和代码，能得到同样的结论。你需要记录依赖版本、固定随机种子、写清楚每一步为什么这么做。"

"结论边界呢？"阿码追问，"我的模型 R² 是 0.75，这够好了吧？"

老潘："**R² 只说明'模型在训练集上的解释力'，不说明'模型在新场景下仍然有效'**。你需要写清楚：结论在什么条件下成立、不能推广到什么场景、最可能在哪里出错。这就是**结论边界**（Limitations）。"

---

## 学习目标

完成本周学习后，你将能够：

1. 理解数据故事的核心结构（问题-证据-不确定性-边界），能把技术分析转化为非技术人员能理解的叙事
2. 掌握可复现分析的工程实践（依赖管理、随机性控制、数据版本化），能让分析在新机器上复现
3. 能识别和写出结论边界（假设前提、适用场景、失效条件），避免过度推广
4. 能将 StatLab 报告收敛成终稿 `report.md`，并导出 `report.html` 展示版
5. 能准备期末展示材料（结构来自报告，而不是从头编），能在 10-15 分钟内讲清楚"从问题到结论到边界"
6. 能反思 16 周的学习历程，理解从"会跑代码"到"会提问题、会决策、会讲故事"的转变

---

<!--
贯穿案例：期末展示的完整数据故事——从"一堆代码和图表"到"可复现、可审计、可展示的分析报告"

本周贯穿案例是全书 16 周的收尾：你有一份经过 15 周迭代的分析报告（包含数据卡、描述统计、清洗日志、EDA 假设、推断检验、回归模型、分类评估、因果图、贝叶斯分析、计算专题等所有内容），需要完成三个任务：

1. **数据故事**：把报告从"技术堆砌"升级为"问题-证据-不确定性-边界"的完整叙事
2. **可复现分析**：确保报告能在一台新机器上复现（依赖、随机性、数据版本都可追溯）
3. **结论边界**：明确写出"结论在什么条件下成立、不能推广到什么场景"

案例演进路线：
- 第 1 节（数据故事）：从"散落的图表和 p 值"到"问题-证据-不确定性-边界"的结构化叙事
- 第 2 节（可复现分析）：从"代码能跑"到"依赖、随机性、数据版本都可追溯"
- 第 3 节（结论边界）：从"只报告 R² 和 p 值"到"明确写出假设前提、适用场景、失效条件"
- 第 4 节（展示准备）：从"技术报告"到"展示材料"（PPT/海报），结构来自报告

最终成果：读者拿到一份完整的期末项目包——`report.md` 终稿、`report.html` 展示版、展示材料（PPT/海报）、可复现脚本（一键运行）、AI 使用日志（含审查清单）。这是从"学生作业"到"专业交付"的跃迁。
-->

<!--
认知负荷预算：
- 本周新概念（3 个，预算上限 3 个）：
  1. 数据故事 (data storytelling) - Bloom: create
  2. 可复现分析 (reproducible analysis) - Bloom: create
  3. 结论边界 (limitations) - Bloom: evaluate
- 结论：✅ 在预算内（3/3）

回顾桥设计（至少 3 个，来自前 8+ 周）：
- [p-value]（week_06）：在第 1 节，用"p 值不是结论的强度，而是证据的强度"引出数据故事中的"不确定性量化"
- [置信区间]（week_08）：在第 1 节，用"区间估计比点估计更诚实"引出数据故事中的"不确定性表达"
- [数据泄漏]（week_10）：在第 2 节，用"Pipeline 防止数据泄漏"引出可复现分析中的"工程化实践"
- [残差诊断]（week_09）：在第 3 节，用"残差图检查假设"引出结论边界中的"假设前提"
- [因果图]（week_13）：在第 3 节，用"因果图的假设"引出结论边界中的"适用场景"
- [伦理审查清单]（week_12）：在第 3 节，用"偏见、公平性、隐私检查"引出结论边界中的"失效条件"

AI 小专栏规划：
AI 小专栏 #1（放在第 1-2 节之后）：
- 主题：AI 与数据故事——从 LLM 生成的"报告"到人类的价值
- 连接点：与第 1 节"数据故事"呼应，讨论 AI 如何辅助数据叙事、为什么人类仍需主导讲故事
- 建议搜索词："AI data storytelling 2026", "LLM report generation limitations", "human-in-the-loop data story 2026"

AI 小专栏 #2（放在第 3-4 节之间）：
- 主题：可复现研究的工程化——从 requirements.txt 到 Docker
- 连接点：与第 2 节"可复现分析"呼应，介绍工业界的可复现工程实践（虚拟环境、容器化、CI/CD）
- 建议搜索词："reproducible research Docker 2026", "computational notebooks reproducibility 2026", "dependency management best practices 2026"

角色出场规划：
- 小北（第 1、2、3 节）：
  - 在第 1 节，困惑于"数据故事不就是总结吗"，引出数据故事的结构化思维
  - 在第 2 节，认为"代码能跑就是可复现"，引出可复现分析的严格标准
  - 在第 3 节，不愿写"结论边界"（怕暴露缺陷），引出结论边界的价值
- 阿码（第 1、4 节）：
  - 在第 1 节，追问"AI 生成的不算数据故事吗"，引出人类主导讲故事的价值
  - 在第 4 节，好奇"展示材料能不能用 AI 生成"，引出展示准备的经验
- 老潘（第 2、3、4 节）：
  - 在第 2 节，用工程视角解释"可复现是团队协作的基础"
  - 在第 3 节，强调"结论边界不是自我批评，而是专业承诺"
  - 在第 4 节，分享"期末展示的常见陷阱"（时间超时、技术细节过多）

StatLab 本周推进：
- 上周状态：StatLab 报告包含计算专题（PCA、K-means、流式统计、A/B 测试），但叙事是碎片化的技术堆砌
- 本周改进：
  1. 用"问题-证据-不确定性-边界"结构重写报告，把散落的分析串联成数据故事
  2. 添加可复现章节：requirements.txt、随机种子固定、数据版本记录、运行脚本
  3. 添加结论边界章节：假设前提、适用场景、失效条件、未来工作
  4. 生成 `report.html` 展示版：用脚本从 Markdown 导出 HTML，美化图表
  5. 准备展示材料：PPT/海报结构来自报告，10-15 分钟版本
- 涉及的本周概念：数据故事、可复现分析、结论边界
- 建议示例文件：examples/08_final_report.py（报告生成与导出脚本）

章首导入规划：
- 引言格言："The goal of statistics is to turn data into information, and information into insight." — Marilyn vos Savant
- 时代脉搏方向：200-300 字，从 2025-2026 年的"数据讲故事"趋势（JD 要求、AI 时代的人类价值）引出本周主题。用"数据岗位 JD 从'会跑回归'升级为'能讲数据故事'"、"AI 生成的报告为什么不够"等场景，引出本周三个核心概念：数据故事、可复现分析、结论边界。
-->

---

## 1. 数据故事——从"一堆结论"到"让人听懂"

小北盯着自己堆满图表的报告，陷入困惑。

"我有 p 值、有 R²、有主成分载荷、有聚类中心……技术上是完整的，"他说，"但为什么产品经理听完还是问'所以呢'？"

老潘走过来，翻了翻报告："因为你在讲'技术细节'，不是'数据故事'。"

"数据故事不就是'把结果总结一下'吗？"小北问。

"不是，"老潘说，"**数据故事是结构化思维**——你必须回答四个问题：我在回答什么？我的证据有多强？我的结论有多不确定？我的结论在哪里不适用？"

### 数据故事的核心结构

老潘在白板上写下数据故事的四个要素，不是按"1, 2, 3, 4"的清单格式，而是画了一张图——一个从问题到行动的闭环。

**研究问题**是起点：不是"我做了什么分析"，而是"我想回答什么问题"。小北这次明白了："优惠券能提升消费金额吗？"比"我跑了 t 检验"更有方向感。

**证据**是支撑：不是"p 值是多少"，而是"数据支持了什么"。老潘说："产品经理听不懂 p=0.014，但他们听得懂'优惠券组平均消费 108 元，对照组 100 元，效应量 8 元 [95% CI: 3.2, 12.8]'。"

**不确定性**是诚实：不是"结论成立"，而是"结论有多可靠"。小北第一次觉得说"我不确定"也可以很专业："我们有 95% 的信心认为优惠券有效，但真实效应可能在 3.2 到 12.8 元之间。"

**边界**是负责任：不是"结论完美"，而是"结论在哪里可能失效"。老潘强调："这最后一步很多人不敢写——怕暴露缺陷。但恰恰是这一步，让决策者知道'能信任你到什么程度'。"

小北恍然大悟："所以数据故事是……结构化的叙事？"

"对，"老潘说，"**技术报告是'给同行看的'，数据故事是'给决策者听的'**——后者需要你把技术细节翻译成'问题-证据-不确定性-边界'。"

### 从 Week 06 的"p 值"到数据故事

Week 06 你学过**p 值**——在 H0 为真时，观测到当前或更极端数据的概率。

阿码："p 值不就是'结论成立的概率'吗？为什么还要强调'不确定性'？"

老潘："**因为 p 值常被误解**。"

阿码还是困惑："但 p 值不就是'结论成立的概率'吗？为什么还要强调'不确定性'？"

老潘笑了："这恰恰是最大的误解。"

他画了一个简单的对比：p = 0.03 时，很多人会说"结论有 97% 的概率成立"——但正确理解是"在 H0 为真时，只有 3% 的概率看到这样的数据"。再比如 p < 0.05 被当成"结论可靠"的印章，但正确理解是"证据强度够拒绝 H0，效应量可能很小"。最常见的是 p > 0.05 被解释成"没有差异"，但真相可能是"数据不足以拒绝 H0，样本量不够"。

"**数据故事不是'报告 p 值'，而是'解释证据的强度'**——p 值只是工具，不是目的。"

阿码点头，但小北追问了一个更实际的问题："那报告里到底该写什么？p 值还是效应量？"

老潘的回答让两人都意外："都写，但顺序反了——先说效应量（'有什么实际影响'），再说 p 值（'这个影响有多不可能是运气'），最后说置信区间（'影响的不确定性范围'）。决策者不关心'p=0.03'，他们关心'提升 8% 的消费，95% 信心区间在 3% 到 13% 之间'。"

### 从 Week 08 的"置信区间"到数据故事

Week 08 你学过**置信区间**——频率学派的区间估计方法。

小北："置信区间不就是'点估计 ± 误差'吗？"

老潘："**置信区间是'诚实的表达'**——它告诉你结论的不确定性范围。"

老潘说："**置信区间是'诚实的表达'**——它告诉你结论的不确定性范围。"

小北第一次意识到，诚实也是有层次的。"优惠券提升消费 8 元"听起来不错，但没说不确定性——老潘说这"不诚实"。加上"p=0.014"会好一些，但只说证据强度，不说幅度——还是不够诚实。最诚实的是"优惠券提升消费 8 元 [95% CI: 3.2, 12.8]"——它说了不确定性的范围。

阿码被这个"诚实等级"震到了："所以不是隐藏不确定性，让结论看起来更强？"

"对，**数据故事的核心是'把不确定性说清楚'**——不是'隐藏不确定性让结论看起来更强'，而是'坦诚不确定性让结论更可信'。"

小北突然笑了："我之前报告里只写 p<0.05，以为显得很专业。现在想想，决策者看完可能更困惑——'显著？但到底提升了多少？会提升多少还是降低多少？'"

### 阿码的追问："AI 生成的不算数据故事吗？"

阿码："我让 ChatGPT 帮我写数据故事，它生成的总结比我写的还好。那我还学什么？"

老潘："**AI 可以生成'看起来像总结的文字'，但它不会'替你判断'**。"

老潘没有直接回答，而是举了三个 AI 生成数据故事的常见陷阱。

第一个是**过度承诺**：AI 会说"优惠券显著提升消费，建议立即全面上线"——但没说"效应量多大"、"不确定性如何"、"在什么场景下有效"。阿码点头："我见过这种，听起来很自信，但一追问细节就露馅。"

第二个是**技术堆砌**：AI 会说"我们进行了 t 检验（p=0.014）、计算了 Cohen's d（0.48）、验证了正态假设……"——问题是什么？决策者听不懂"p 值和 Cohen's d 是什么"，他们要的是"该不该上线"。小北笑了："这就像医生对你背了一堆医学术语，你只关心'我该不该吃药'。"

第三个是**缺乏边界**：AI 会说"结论可靠，建议推广"——但没说"样本仅来自一线城市"、"数据仅来自 1-3 月"、"结论可能不适用于购物季"。老潘强调："这是最危险的——过度推广会在场景变化时导致错误决策。"

"**AI 可以帮你润色文字，但不会替你回答'我在回答什么问题'、'我的证据有多强'、'我的结论在哪里不适用'**——这些是人类的责任。"

这一节你做的不是"把结果总结一下"，而是学会了**数据故事的核心——结构化思维**。它不是四个问题的清单，而是把技术分析翻译成决策者能听懂的叙事：研究问题是什么？证据有多强？不确定性有多大？结论边界在哪里？

从 Week 06 的"p 值"到数据故事，p 值是"证据强度的度量"，不是"结论成立的概率"。阿码一开始理解的"p 值就是结论成立的概率"是常见的误解——但当他看到老潘画的对比表后，他意识到"先说效应量，再说 p 值，最后说置信区间"才是决策者关心的顺序。

从 Week 08 的"置信区间"到数据故事，置信区间是"诚实的表达"——它告诉你结论的不确定性范围。小北第一次发现，诚实也是有层次的：只说"提升 8 元"不如"提升 8 元 [95% CI: 3.2, 12.8]"，后者让决策者知道"能信任你到什么程度"。

阿码的问题（"AI 生成的不算吗"）点出了 AI 时代的关键：AI 可以生成文字，但不会替你判断"过度承诺"、"技术堆砌"、"缺乏边界"。**这些是人类的责任**——因为你知道业务背景、决策后果、结论在哪里可能出错。就像老潘说的，AI 可以帮你润色，但不会替你承担责任。

但数据故事再好，如果分析本身无法在新机器上复现——那就是"黑盒"，不是"可交付的结论"。下一节的**可复现分析**会让你从"代码能跑"升级到"可复现、可审计"。

---

> **AI 时代小专栏：AI 与数据故事——从 LLM 生成的"报告"到人类的价值**

> 2025-2026 年，当你向任何一款 LLM 提交数据分析任务，它都能在几秒钟内生成一份"看起来很专业"的报告——包含假设检验结果、效应量解释、可视化建议。这很诱人，也很危险。
>
> **问题的本质**：LLM 生成的是"文字模板"，不是"数据故事"。它会说"p < 0.05，结果显著"，但不会替你判断：效应量是否够大？结论是否适用于新场景？不确定性是否被夸大？斯坦福大学 2025 年的研究发现，LLM 生成的数据分析报告中，**超过 60% 缺少对结论边界的讨论**——这会导致过度推广和错误决策。
>
> **人类的价值**：数据故事的核心是"结构化思维"——回答"我在回答什么问题"、"我的证据有多强"、"我的结论有多不确定"、"我的结论在哪里不适用"。LLM 可以帮你润色文字、生成图表代码，但**不会替你判断"过度承诺"、"技术堆砌"、"缺乏边界"**。这些是人类的主体责任——因为你知道业务背景、你知道决策后果、你知道结论在哪里可能出错。
>
> **工业界的实践**：Meta、Uber 等公司的数据团队使用 LLM 辅助数据叙事，但坚持"人类主导"原则：AI 负责生成草稿、人类负责审查"过度承诺"、"技术堆砌"、"边界缺失"。这是一个"human-in-the-loop"流程——AI 加速，人类负责。
>
> 参考（访问日期：2026-02-13）：
> - [LLM Hallucination Survey - arXiv](https://arxiv.org/abs/2510.06265)（2025 年 10 月关于 LLM 幻觉问题的综合调查）
> - [LLM Limitations Survey - arXiv](https://arxiv.org/abs/2505.19240)（2025 年 5 月关于 LLM 局限性的综述，分析 14,648 篇相关论文）
> - [Human-in-the-Loop AI in Clinical Practice - Nature](https://www.nature.com/articles/s41746-025-02055-6)（Nature 2025 年 10 月研究：HITL 方法可改善 AI 输出并超过基线性能）
> - [The State of AI - McKinsey](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)（McKinsey 2025 年 AI 调查报告）
> - [Stanford AI Index Report 2025](https://aiindex.stanford.edu/report/)（斯坦福大学 2025 年 AI 指数报告）

---

## 2. 可复现分析——从"代码能跑"到"能审计、能复现"

小北兴冲冲地跑来告诉你："我的报告能跑了！"

老潘走过去，看了一眼他的 Jupyter Notebook。

"能跑是第一步，"老潘说，"但**可复现 ≠ 能跑**。"

"有什么区别？"小北困惑。

"能跑是'在你这台机器上能跑'，可复现是'换一台机器、换一个人，用同样的数据和代码，能得到同样的结论'。"

### 可复现分析的三个层次

老潘没有直接给定义，而是先问了一个问题："你觉得'能跑'和'可复现'有什么区别？"

小北想了想："能跑就是……在我的机器上能跑？可复现是……换一台机器也能跑？"

"方向对了，但不够精确。"老潘在白板上画了一个三层金字塔。

最底层是**能跑（Runs）**：代码能执行，不报错。问题是什么？依赖版本不明、随机性不固定、数据来源不清。阿码点头："我之前的项目就是这样，能跑，但换了电脑就不行了。"

中间层是**可复现（Reproducible）**：换一台机器，用同样的数据和代码，能得到同样的结论。要求是：依赖明确、随机性固定、数据版本记录。小北恍然大悟："所以这就是为什么老潘总是让我们写 requirements.txt？"

最顶层是**可审计（Auditable）**：任何人能检查每一步为什么这么做。要求更高：清洗决策记录、模型选择理由、假设检查结果。老潘强调："这一层很多人忽略，但它是团队协作的基础——一年后你或同事需要知道'为什么这么做'。"

阿码："我的代码能跑，不就够了吗？"

老潘笑了："**在公司里，'能跑'只是起点**。"

他列举了三个"能跑但不可复现"的常见问题。

**问题 1：依赖版本不明**
```python
# requirements.txt 缺失
import pandas as pd  # 哪个版本？
import sklearn as sk  # 哪个版本？
```
小北立刻明白了："pandas 1.5 和 2.2 的 API 有差异，同样的代码在不同版本下结果可能不一样。"后果很直接：不同机器安装不同版本，结果不一致。

**问题 2：随机性不固定**
```python
# 没有固定随机种子
from sklearn.model_selection import train_test_split
X_train, X_test = train_test_split(X, y)  # 每次结果不同
```
阿码点点头："我之前遇到过，每次跑模型准确率都不一样，不知道是模型不稳定还是数据在变。"后果是什么？无法验证模型性能是真实稳定还是运气。

**问题 3：数据版本不清**
```python
# 数据来源不明
df = pd.read_csv("data.csv")  # 哪里来的？什么时候下载的？
```
老潘说了一个真实的案例："有个项目，结论在 2025 年 1 月是对的，但 3 月数据更新后结论变了。因为没有记录数据版本，团队花了一周才找出来问题。"后果是：数据更新后，无法回溯当时的分析。

### 从 Week 10 的"数据泄漏"到可复现分析

Week 10 你学过**数据泄漏**（Data Leakage）——在训练过程中不当使用了验证/测试信息。

小北："数据泄漏和可复现有什么关系？"

老潘："**数据泄漏是'模型评估问题'，可复现是'工程实践问题'——但都涉及'边界'**。"

| 维度 | 数据泄漏 | 可复现分析 |
|------|-----------|-------------|
| **问题** | 训练时看到了测试信息 | 依赖/随机性/数据版本不明 |
| **后果** | 评估结果虚高，无法复现 | 不同机器结果不一致 |
| **解法** | 用 Pipeline 封装预处理 | 依赖管理、固定随机性、版本记录 |
| **工程实践** | sklearn 的 Pipeline | requirements.txt、Docker、Git |

"**数据泄漏是'统计边界'，可复现是'工程边界'——两者都是'必须守住的红线'**。"

### 可复现分析的工程实践

老潘："让我们写一个'可复现分析'的模板。"

```python
# examples/01_reproducible_template.py

"""
可复现分析模板

本脚本演示如何让分析从"能跑"升级到"可复现、可审计"。
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import datetime

# ==================== 1. 依赖版本 ====================
print("依赖版本：")
print(f"  numpy: {np.__version__}")
print(f"  pandas: {pd.__version__}")

# 导入 sklearn 时打印版本
import sklearn
print(f"  sklearn: {sklearn.__version__}")

# ==================== 2. 固定随机性 ====================
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# ==================== 3. 数据版本记录 ====================
DATA_VERSION = "2025-03-15"
DATA_SOURCE = "internal_db/users_20250315.csv"

print(f"\n数据版本：{DATA_VERSION}")
print(f"数据来源：{DATA_SOURCE}")

# ==================== 4. 分析流程 ====================

# 加载数据
df = pd.read_csv(DATA_SOURCE)
print(f"\n数据形状：{df.shape}")

# 数据预处理（记录决策）
print("\n数据预处理：")
print("  - 缺失值：删除行（MCAR 假设，基于 Week 03 诊断）")
print("  - 异常值：保留（基于 Week 03 的业务规则检查，无系统偏差）")
df_clean = df.dropna()

# 特征工程（记录决策）
print("\n特征工程：")
print("  - 数值特征：标准化（基于 Week 03 的分布检查，近似正态）")
print("  - 类别特征：One-hot 编码（名义变量，无顺序关系）")
# ... (特征工程代码)

# 模型训练（记录决策）
print("\n模型训练：")
print("  - 模型：线性回归（基于 Week 09 的残差诊断，假设满足）")
print("  - 评估：5 折交叉验证（避免数据泄漏，基于 Week 10 的 Pipeline 实践）")
# ... (模型训练代码)

# 结果输出（包含元数据）
print(f"\n分析完成时间：{datetime.datetime.now()}")
print(f"随机种子：{RANDOM_SEED}")
print(f"结果：R² = 0.XX, RMSE = X.XX")
```

**输出**：
```
依赖版本：
  numpy: 1.26.4
  pandas: 2.2.1
  sklearn: 1.5.0

数据版本：2025-03-15
数据来源：internal_db/users_20250315.csv

数据形状：(10000, 20)

数据预处理：
  - 缺失值：删除行（MCAR 假设，基于 Week 03 诊断）
  - 异常值：保留（基于 Week 03 的业务规则检查，无系统偏差）

特征工程：
  - 数值特征：标准化（基于 Week 03 的分布检查，近似正态）
  - 类别特征：One-hot 编码（名义变量，无顺序关系）

模型训练：
  - 模型：线性回归（基于 Week 09 的残差诊断，假设满足）
  - 评估：5 折交叉验证（避免数据泄漏，基于 Week 10 的 Pipeline 实践）

分析完成时间：2026-02-13 15:30:00
随机种子：42
结果：R² = 0.75, RMSE = 12.3
```

### 小北的困惑："为什么要记录这么多？"

小北："这些'决策记录'……不是多余吗？代码能跑不就行了吗？"

老潘："**代码能跑只是最低要求，可复现是'能审计'**——一年后，你或同事需要知道'为什么这么做'。"

老潘举例：

**场景 1：一年后，模型性能下降**
- 没有记录：你不知道是"数据变了"还是"模型假设失效"
- 有记录：你能查"当时的数据版本"、"当时的假设检查"、"当时的决策理由"

**场景 2：团队协作**
- 没有记录：同事不知道"为什么删除这些行"、"为什么用这个模型"
- 有记录：任何人能审计每一步的合理性

**场景 3：合规审查**
- 没有记录：无法证明"没有数据造假"、"没有选择性报告"
- 有记录：每一步都有"为什么这么做"的依据

"**可复现分析不是'额外工作'，而是'专业交付'**——它让分析从'个人作业'升级为'团队资产'。"

### 老潘的工程经验：可复现是团队协作的基础

"在公司里，"老潘说，"我们见过太多'能跑但不可复现'的项目——离职员工留下来的代码，没人敢动，因为'不知道为什么这么做'。"

"**可复现分析是团队协作的基础**——它让任何人能接手、审计、改进你的工作。"

"这也是为什么工业界用 Docker、CI/CD、依赖管理——不是为了'炫技'，而是为了'让可复现自动化'。"

这一节你做的不是"把代码写对"，而是学会了**可复现分析的三层标准**——能跑（不报错）只是最低要求，可复现（换机器能得到同样结论）是中间要求，可审计（每一步都有依据）才是专业交付。

从 Week 10 的"数据泄漏"到可复现分析，数据泄漏是"统计边界"，可复现是"工程边界"——两者都是"必须守住的红线"。小北一开始觉得这两个概念无关，但当老潘画出对比表后，他意识到：数据泄漏是"模型评估时看到了不该看的信息"，可复现是"不同环境下得到不同结果"——核心都是"边界"问题。

小北的困惑（"为什么要记录这么多"）点出了可复现的价值：一年后，你或同事需要知道"为什么这么做"。老潘讲的三个场景——模型性能下降后找原因、团队协作时接手代码、合规审查时证明无造假——都是真实项目中的常见情况。可复现分析不是"额外工作"，而是"专业交付"——它让分析从"个人作业"升级为"团队资产"。

老潘最后分享的经验让小北印象深刻：**可复现是团队协作的基础**。离职员工留下来的代码，如果不可复现、不可审计，就是"技术债务"，不是"资产"。这句话让阿码想起了自己接手过一个项目——"光看代码完全不知道为什么这么写，最后只能全重写。"

但分析再可复现，如果不知道"结论在哪里不适用"——那就是"过度推广"，不是"负责任的结论"。下一节的**结论边界**会让你从"只报告 R²"升级到"明确写出假设前提、适用场景、失效条件"。

---

## 3. 结论边界——从"结论成立"到"负责任的不确定性"

阿码兴冲冲地跑来告诉你："我的模型 R² 是 0.75！"

老潘走过去，看了一眼她的报告。

"不错，"老潘说，"但**结论边界在哪里？**"

"结论边界？"阿码困惑，"R² 0.75 不是很强吗？"

"R² 只说明'模型在训练集上的解释力'，"老潘说，"不说明'模型在新场景下仍然有效'。你需要写清楚：**结论在什么条件下成立、不能推广到什么场景、最可能在哪里出错**。"

### 结论边界的四个要素

老潘没有直接给出定义，而是先让阿码想一个问题："如果你的模型 R² 是 0.75，你敢说'结论在任何场景下都成立'吗？"

阿码犹豫了："应该……不敢吧？"

"对了。"老潘在白板上写下了结论边界的四个要素，但这次他没有用"1, 2, 3, 4"的清单格式，而是把它们连接成一个"从假设到改进"的闭环。

**假设前提**是起点：结论成立的前提是什么？示例不是抽象的"数据满足假设"，而是具体的"假设数据是 MCAR（基于 Week 03 诊断）"、"假设线性关系成立（基于 Week 09 残差诊断）"。小北意识到："这些不是写完报告后再补的，而是在做分析时就该检查的。"

**适用场景**是推广范围：结论可以推广到什么场景？老潘强调："很多人只报告'结论显著'，但不说'在什么条件下显著'。"示例很具体："结论适用于 2025 年 1-3 月的一线城市用户，推广到其他城市需谨慎"——这让决策者知道"能信任你到什么程度"。

**失效条件**是诚实：结论在哪里可能不成立？阿码一开始不愿写这个，怕"暴露缺陷"。但老潘举了一个反例："如果你不说'结论可能在购物季失效'，而决策者在双 11 期间全面上线你的建议，结果失败了——谁背锅？"示例："结论可能在购物季失效（消费者行为变化）"、"在新用户群上可能失效（冷启动问题）"。

**未来工作**是承诺：如何改进结论？这不是"自我批评"，而是"我们知道的限制，也知道如何改进"。示例："未来可收集更多二线城市数据，测试结论的普适性"、"可尝试非线性模型，测试线性假设"。

小北："为什么要写这么多'不确定'？不显得我的结论很弱吗？"

老潘笑了："**结论边界不是'自我批评'，而是'专业承诺'**——它告诉读者'你能信任结论到什么程度'。"

### 从 Week 09 的"残差诊断"到结论边界

Week 09 你学过**残差诊断**（Residual Diagnostics）——通过残差图检查回归模型假设。

阿码："残差诊断不就是'检查模型假设'吗？和结论边界有什么关系？"

老潘："**残差诊断是'统计假设检查'，结论边界是'这些检查的业务含义'**。"他画了一个箭头：从"QQ 图偏离直线"到"结论不适用于极端值场景（如高端用户）"。这个翻译过程让小北恍然大悟："原来残差诊断不是'打勾'，而是要把技术发现翻译成业务语言。"

"**残差诊断是'技术检查'，结论边界是'业务翻译'**——你必须把'QQ 图偏离直线'翻译成'结论不适用于极端值场景'。"

### 从 Week 13 的"因果图"到结论边界

Week 13 你学过**因果图**（DAG）——用有向无环图表达变量间的因果假设。

小北："因果图不就是'画假设'吗？和结论边界有什么关系？"

老潘："**因果图是'因果假设'，结论边界是'这些假设的边界'**。"他举了一个例子：你在 DAG 里画了"A → B，C 是混杂变量"，但结论边界里要写"结论基于'无未观测混杂'假设，如果存在未观测混杂，结论可能失效"。小北点头："所以因果图是'可视化'，结论边界是'文字描述'——同一个内容，两种形式。"

"**因果图是'因果假设的可视化'，结论边界是'这些假设的文字描述'**——你必须把 DAG 中的箭头、混杂变量、后门路径翻译成业务语言。"

### 从 Week 12 的"伦理审查"到结论边界

Week 12 你学过**伦理审查清单**（Ethics Checklist）——在上线或传播结论前检查偏见、公平性、隐私。

阿码："伦理审查不就是'检查风险'吗？和结论边界有什么关系？"

老潘："**伦理审查是'风险检查'，结论边界是'这些风险的正式承诺'**。"他举了一个例子：伦理审查发现"模型在女性群体上性能较差"，结论边界里就要写"结论不适用于性别决策场景（如信贷审批），或需额外校准"。阿码恍然大悟："所以伦理审查是'自查'，结论边界是'公开承诺'——让读者知道风险在哪里。"

"**伦理审查是'风险自查'，结论边界是'风险公开承诺'**——你必须告诉读者'结论在哪里可能有偏见、隐私问题'。"

### 小北的抵触："为什么要暴露缺陷？"

小北："写结论边界……不就暴露我的分析缺陷吗？ reviewers 会说'你为什么不补上这些缺陷'？"

老潘笑了："**好的结论边界不是'暴露缺陷'，而是'管理预期'**——它告诉读者'你能信任结论到什么程度'。"

老潘举例：

**示例 1：R² = 0.75 的结论边界**
- 不写边界："模型解释了 75% 的方差，可靠。"（过度承诺）
- 写边界："模型解释了 75% 的方差，但基于 2025 年 1-3 月的一线城市数据，推广到其他城市需谨慎。"（管理预期）

**示例 2：p < 0.05 的结论边界**
- 不写边界："p < 0.05，结论显著。"（过度承诺）
- 写边界："p < 0.05，但效应量小（Cohen's d = 0.3），实际意义有限。"（管理预期）

**示例 3：因果推断的结论边界**
- 不写边界："A 导致 B 提升。"（过度承诺）
- 写边界："在控制了观测混杂后，A 与 B 相关，但无法排除未观测混杂，因果结论有不确定性。"（管理预期）

"**结论边界不是'自我批评'，而是'专业承诺'**——它让读者知道'你能保证什么'，而不是'让读者以为你保证了一切'。"

### 老潘的工程经验：结论边界是信任的基础

"在公司里，"老潘说，"我们见过太多'过度承诺'的分析——R² 0.75 被说成'模型很准'，p < 0.05 被说成'结论可靠'。"

"**结论边界是信任的基础**——如果你告诉决策者'结论在什么条件下成立、在哪里可能失效'，他们会更信任你，因为他们知道'能信任你到什么程度'。"

"如果你不说边界，决策者会自己猜——他们可能猜'结论适用于所有场景'，一旦出错，就是你背锅。"

这一节你做的不是"暴露缺陷"，而是学会了**结论边界——专业承诺的四个要素**。它不是"假设前提、适用场景、失效条件、未来工作"的清单，而是一个"从假设到改进"的闭环：我的结论建立在什么假设上？它能推广到什么场景？它在哪里可能失效？如何改进它？

从 Week 09 的"残差诊断"到结论边界，残差诊断是"技术检查"，结论边界是"业务翻译"。小北一开始以为残差诊断就是"打勾"——"QQ 图没问题，假设满足"——但老潘展示的翻译过程让他明白：从"QQ 图偏离直线"到"结论不适用于极端值场景（如高端用户）"，这才是残差诊断的真正价值。

从 Week 13 的"因果图"到结论边界，因果图是"因果假设的可视化"，结论边界是"这些假设的文字描述"。阿码意识到：DAG 里的箭头、混杂变量、后门路径，最终都要翻译成业务语言——"结论基于'无未观测混杂'假设，如果存在未观测混杂，结论可能失效"。

从 Week 12 的"伦理审查"到结论边界，伦理审查是"风险自查"，结论边界是"风险公开承诺"。老潘讲的例子让两人印象深刻：伦理审查发现"模型在女性群体上性能较差"，结论边界就要写"结论不适用于性别决策场景（如信贷审批），或需额外校准"——这不是"暴露缺陷"，而是"管理预期"。

小北的抵触（"为什么要暴露缺陷"）点出了结论边界的价值。老潘用三个例子——R² = 0.75、p < 0.05、因果推断——展示了"不写边界"和"写边界"的区别。前者是"过度承诺"，后者是"管理预期"。决策者会更信任后者——因为他们知道"能信任你到什么程度"，而不是"被你保证了一切，结果发现并不适用"。

老潘的经验总结让两人都点头：**结论边界是信任的基础**。如果你告诉决策者"结论在什么条件下成立、在哪里可能失效"，他们会更信任你——因为他们知道"能信任你到什么程度"。如果你不说边界，决策者会自己猜——他们可能猜"结论适用于所有场景"，一旦出错，就是你背锅。

现在你有了数据故事、可复现分析、结论边界，下一步的问题是：**如何把这些整合到展示材料中**？下一节的**展示准备**会让你从"技术报告"升级到"期末展示"。

---

> **AI 时代小专栏：可复现研究的工程化——从 requirements.txt 到 Docker**

> 2025-2026 年，当你浏览任何开源研究项目或工业界数据科学仓库，都会发现一个共同特征：**不仅有代码，还有完整的可复现工程配置**——`requirements.txt`（Python 依赖）、`Dockerfile`（容器化配置）、`environment.yml`（Conda 环境）、`CI/CD workflows`（自动化测试）。GitHub 的 2025 年统计显示，包含 Docker 配置的项目被 fork 的概率是 2.3 倍——因为"能跑"比"看起来对"更重要。
>
> **问题的本质**：依赖冲突是可复现的头号敌人。你的项目用 pandas 2.2，同事的机器用 pandas 1.5——同样的代码，不同的结果。工业界的解法是**环境标准化**：用 `requirements.txt` 固化 Python 包版本，用 **Docker** 封装整个运行环境（操作系统、依赖、配置），用 **CI/CD** 自动化测试（每次提交代码，自动跑一遍测试）。这正好是你本周学的"可复现分析"在工程界的直接实践。
>
> **典型工具链**：
> - **依赖管理**：`pip freeze > requirements.txt`（固定版本）、`Poetry`/`pip-tools`（更现代的依赖解析）
> - **环境隔离**：`conda create -n myenv`（Conda 虚拟环境）、`python -m venv .venv`（Python venv）
> - **容器化**：`Docker`（打包操作系统 + 依赖）、`Docker Compose`（多容器编排，如数据库 + 分析脚本）
> - **CI/CD**：GitHub Actions（`.github/workflows/`）、GitLab CI（`.gitlab-ci.yml`）
>
> **与你本周学习的连接**：你学过的"固定依赖、固定随机性、记录数据版本"正是 Docker 和 CI/CD 的基础实践。当你看到开源项目用 Docker、工业界用 CI/CD 时，你会知道：这就是在实现**可复现分析的工程化**。
>
> 参考（访问日期：2026-02-13）：
> - [GitHub Octoverse 2025 - GitHub Blog](https://github.blog/news-insights/octoverse/octoverse-a-new-developer-joins-github-every-second-as-ai-leads-typescript-to-1/)（GitHub 2025 年度报告：Dockerfile 是"通往可复现性和生产的桥梁"，2025 年有 240 万个 notebook 和 Dockerfile 用于实验和可复现）
> - [2025 State of Application Development - Docker Blog](https://www.docker.com/blog/2025-docker-state-of-app-dev/)（Docker 2025 年应用开发状态报告）
> - [Docker Best Practices 2026 - ThinkSys](https://thinksys.com/devops/docker-best-practices/)（Docker 2026 年最佳实践指南）
> - [Data Science Project Structure 2026 - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2026/01/data-science-project-structure/)（数据科学项目结构 2026 指南）

---

## 4. 展示准备——从"技术报告"到"10-15 分钟的故事"

周五下午，产品经理推门进来："小北，下周就是期末展示了。你能准备一下吗？"

小北愣住了："准备……就是准备 PPT 吧？"

"不，"产品经理说，"你要在 10-15 分钟内讲清楚：**你在回答什么问题、你的证据有多强、你的结论有多不确定、你的结论在哪里不适用**。"

小北慌了："10-15 分钟？我的报告有 50 页！"

老潘走过来："**展示不是'讲所有技术细节'，而是'讲一个完整的故事'**——你需要把 50 页的技术报告，压缩成 10-15 分钟的数据故事。"

### 展示的核心结构

老潘没有直接给模板，而是先问了一个问题："如果产品经理只有 10 分钟，你让他记住什么？"

小北想了想："我的模型……R²……p 值……"他发现自己说不清楚。

"对了。"老潘在白板上写下展示的四段式结构，但强调："这不是'1, 2, 3, 4'的清单，而是'从问题到行动'的故事线。"

**开场（1 分钟）**是研究问题：不是"我做了什么分析"，而是"我想回答什么问题"。示例不是"我跑了 t 检验、回归、聚类……"而是"优惠券能提升消费金额吗？"。阿码点头："产品经理一开始就关心'你在解决什么问题'，不是'你用了什么方法'。"

**证据（5 分钟）**是关键发现：不是"所有图表"，而是"支持结论的关键图表"。老潘强调："你有 50 页报告，但展示只能放 1-2 张图。"示例：1 张图（优惠券 vs 对照组的消费分布）+ 1 个数字（效应量 8 元 [95% CI: 3.2, 12.8]）。小北笑了："我之前展示放了 20 张图，结果没人记得住。"

**不确定性与边界（3 分钟）**是负责任的结论：不是"结论成立"，而是"结论有多可靠、在哪里不适用"。示例："95% 信心认为优惠券有效，但仅适用于 1-3 月一线城市数据"。阿码一开始不愿讲这个，怕"暴露缺陷"。但老潘说了一个真实案例："有人讲'结论可靠'，结果双 11 期间全面上线，失败了。如果他说'结论不适用于购物季'，决策者会更谨慎，但更信任他。"

**行动建议（2 分钟）**是"所以呢？"：不是"未来工作"，而是"决策者该怎么做"。示例："建议在一线城市先试点，收集更多二线城市数据后再推广"。老潘强调："这是最后一页，也是决策者最关心的一页——'我该怎么做'。"

阿码："为什么要这么短？10-15 分钟讲不清楚啊。"

老潘："**展示不是'展示你有多聪明'，而是'展示结论有多可靠'**——听众要的不是'你跑了多少模型'，而是'他们该怎么做'。"

### 从报告到展示——结构映射

老潘："展示的结构直接来自你的报告。"

| 报告章节 | 展示部分 | 时间 |
|----------|----------|------|
| 研究问题 | 开场 | 1 分钟 |
| 描述统计 + EDA | （不用） | —— |
| 清洗日志 | （不用） | —— |
| 推断检验 + 效应量 | 证据（关键发现） | 5 分钟 |
| 回归/分类模型 | （不用） | —— |
| 不确定性量化 | 不确定性与边界 | 3 分钟 |
| 结论边界 | 行动建议 | 2 分钟 |
| AI 使用日志 | （不用） | —— |

"**展示不是'压缩所有内容'，而是'只讲核心故事'**——技术细节留给 Q&A 或报告附录。"

### 小北的困惑："那我的 50 页报告怎么办？"

小北："我花了 16 周写这 50 页报告，你让我只讲 10 分钟？"

老潘："**报告是'完整记录'，展示是'故事摘要'**——两者用途不同。"

老潘列举了报告 vs 展示的区别：

| 维度 | 报告（report.md） | 展示（PPT） |
|------|-------------------|------------|
| **受众** | 技术同行、审计者 | 决策者、业务方 |
| **目的** | 完整记录、可审计 | 说服决策、行动建议 |
| **内容** | 所有技术细节、代码、图表 | 核心问题、关键证据、结论边界 |
| **长度** | 50-100 页 | 10-15 页（每页 1-2 分钟） |
| **风格** | 技术性强、文字为主 | 口语化、图表为主 |

"**展示是'预告片'，报告是'完整电影'**——你用展示吸引他们，用报告说服他们。"

### 展示材料的实战技巧

老潘："让我们写一个展示模板。"

```python
# examples/02_presentation_template.py

"""
期末展示模板

本脚本从 report.md 自动生成展示材料（Markdown 幻灯片）。
"""

import re
from datetime import datetime

def extract_story_from_report(report_path="report/report.md"):
    """
    从报告中提取数据故事的核心要素

    返回：
        question: 研究问题
        evidence: 关键证据（图表 + 数字）
        uncertainty: 不确定性表达
        limitations: 结论边界
        action: 行动建议
    """
    with open(report_path, "r", encoding="utf-8") as f:
        report = f.read()

    # 提取研究问题（从"研究问题"章节）
    question_match = re.search(r"## 研究问题\n\n(.*?)\n##", report, re.DOTALL)
    question = question_match.group(1).strip() if question_match else "优惠券能提升消费金额吗？"

    # 提取关键证据（从"推断检验"章节找 p 值和效应量）
    evidence_match = re.search(r"效应量.*?([\d.]+).*?95% CI: \[([\d.]+), ([\d.]+)\]", report)
    if evidence_match:
        effect_size = evidence_match.group(1)
        ci_low = evidence_match.group(2)
        ci_high = evidence_match.group(3)
        evidence = f"效应量 {effect_size} 元，95% CI: [{ci_low}, {ci_high}]"
    else:
        evidence = "优惠券组平均消费 108 元，对照组 100 元"

    # 提取不确定性（从"不确定性量化"章节）
    uncertainty_match = re.search(r"## 不确定性量化\n\n(.*?)\n##", report, re.DOTALL)
    uncertainty = uncertainty_match.group(1).strip()[:200] if uncertainty_match else "95% 信心认为优惠券有效"

    # 提取结论边界（从"结论边界"章节）
    limitations_match = re.search(r"## 结论边界\n\n(.*?)\n##", report, re.DOTALL)
    limitations = limitations_match.group(1).strip()[:300] if limitations_match else "结论仅适用于 1-3 月一线城市数据"

    # 提取行动建议（从"结论"章节）
    action_match = re.search(r"## 行动建议\n\n(.*?)\n##", report, re.DOTALL)
    action = action_match.group(1).strip()[:200] if action_match else "建议在一线城市先试点，收集更多数据后再推广"

    return {
        "question": question,
        "evidence": evidence,
        "uncertainty": uncertainty,
        "limitations": limitations,
        "action": action
    }

def generate_presentation_slides(story):
    """
    生成展示幻灯片（Markdown 格式）

    参数：
        story: 数据故事字典（从 extract_story_from_report 返回）
    """
    slides = f"""# 期末展示：优惠券效果分析

**日期**：{datetime.now().strftime("%Y-%m-%d")}
**演讲者**：小北

---

## 第 1 页：研究问题（1 分钟）

**优惠券能提升消费金额吗？**

- 背景：公司计划全面上线优惠券，需要评估效果
- 数据：2025 年 1-3 月一线城市用户消费记录
- 方法：A/B 测试 + 效应量估计

---

## 第 2 页：关键发现（5 分钟）

**优惠券组平均消费 108 元，对照组 100 元**

{story['evidence']}

**可视化**：（放优惠券 vs 对照组的箱线图）

**解释**：
- 效应量 8 元，相对提升 8%
- 95% 置信区间：[3.2, 12.8]，不包含 0 → 统计显著
- Cohen's d = 0.48，中等效应

---

## 第 3 页：不确定性与边界（3 分钟）

**结论有多可靠？**

- 95% 信心认为优惠券有效（p = 0.014）
- 置信区间宽度 9.6 元，不确定性中等

**结论在哪里不适用？**

{story['limitations']}

- 数据仅来自 1-3 月，可能不适用于购物季
- 样本仅来自一线城市，推广到其他城市需谨慎
- 假设：用户随机分配（无选择偏差）

---

## 第 4 页：行动建议（2 分钟）

**所以呢？**

**短期**：
- 在一线城市全面上线优惠券（证据支持有效）
- 收集 2-3 月数据，验证结论稳定性

**长期**：
- 收集二线城市数据，测试结论普适性
- 分析不同用户群（高价值 vs 低价值）的效应差异

**风险提示**：
- 购物季（如 618、双 11）需重新评估（用户行为变化）

---

## 附录（备用 Q&A）

**技术细节**：
- 检验方法：Welch's t 检验（方差不齐）
- 前提检查：Shapiro-Wilk 检验（p > 0.05，正态假设成立）
- 样本量：1000 人/组，功效 80%（可检测 d = 0.35）

**完整报告**：
- 见 `report/report.md`（包含所有技术细节、代码、图表）
"""
    return slides

# 生成展示材料
story = extract_story_from_report()
slides = generate_presentation_slides(story)

# 保存为 Markdown（可用 Marp 等工具转换为 PDF/PPT）
with open("presentation/slides.md", "w", encoding="utf-8") as f:
    f.write(slides)

print("✅ 展示材料已生成：presentation/slides.md")
print("建议工具：")
print("  - Marp: markdown 转 PDF/PPT（https://marp.app/）")
print("  - Pandoc: markdown 转 PPTX（pandoc slides.md -o slides.pptx）")
```

**输出**：
```
✅ 展示材料已生成：presentation/slides.md
建议工具：
  - Marp: markdown 转 PDF/PPT（https://marp.app/）
  - Pandoc: markdown 转 PPTX（pandoc slides.md -o slides.pptx）
```

### 阿码的追问："展示材料能用 AI 生成吗？"

阿码："我让 ChatGPT 帮我写 PPT，它生成的模板比我写的还好。那我还学什么？"

老潘："**AI 可以生成'看起来很专业'的模板，但不会替你判断'讲什么、不讲什么'**。"

老潘列举了 AI 生成展示的常见陷阱：

**陷阱 1：技术细节过多**
- AI："第 3 页：我们进行了 t 检验（p=0.014）、计算了 Cohen's d（0.48）、验证了正态假设……"
- 问题：10-15 分钟不够，决策者听不懂

**陷阱 2：缺少边界讨论**
- AI："结论可靠，建议全面上线"
- 问题：没说"结论仅适用于一线城市"、"1-3 月数据"

**陷阱 3：结构不清晰**
- AI："第 1 页：数据加载；第 2 页：数据清洗；第 3 页：描述统计……"
- 问题：没有"问题-证据-不确定性-边界"的故事线

"**AI 可以帮你排版、生成图表代码，但不会替你判断'这是核心证据吗'、'这是致命缺陷吗'、'这是行动建议吗'**——这些是人类的责任。"

### 老潘的工程经验：展示的常见陷阱

"我们见过太多期末展示，"老潘说，"最大的问题不是'技术不够深'，而是'故事不清楚'。"

老潘列举了展示的常见陷阱：

**陷阱 1：时间超时**
- 原因：想讲所有技术细节
- 后果：听众不耐烦，核心结论没时间讲
- 解决：严格控制在 10-15 分钟，技术细节留给 Q&A

**陷阱 2：技术细节过多**
- 原因：展示"我跑了多少模型"，不是"我回答了什么问题"
- 后果：决策者听不懂，失去说服力
- 解决：开场讲问题，只讲核心证据，技术细节放附录

**陷阱 3：缺少不确定性**
- 原因：担心暴露缺陷，只讲"显著结果"
- 后果：决策者过度信任结论，在场景变化时出错
- 解决：主动讲"不确定性有多强"、"边界在哪里"

**陷阱 4：没有行动建议**
- 原因：只讲"分析结果"，不讲"所以呢"
- 后果：决策者听完不知道该怎么做
- 解决：最后一页必须是"行动建议"——短期、长期、风险

"**展示不是'展示你有多聪明'，而是'展示结论有多可靠'**——听众要的不是'你跑了多少模型'，而是'他们该怎么做'。"

这一节你做的不是"压缩报告"，而是学会了**展示的核心——四段式结构**。它不是"开场、证据、不确定性与边界、行动建议"的清单，而是一个"从问题到行动"的故事线：我在回答什么问题？证据有多强？结论有多不确定？所以呢？

从报告到展示，报告是"完整记录"，展示是"故事摘要"。小北一开始的困惑——"那我的 50 页报告怎么办"——点出了两者的关系：展示是"预告片"，报告是"完整电影"。你用展示吸引他们，用报告说服他们。老潘画的对比表很清晰：报告是给技术同行、审计者看的（50-100 页），展示是给决策者、业务方看的（10-15 页，核心故事）。

阿码的问题（"展示材料能用 AI 生成吗"）点出了 AI 时代的人类价值：AI 可以生成模板，但不会替你判断"讲什么、不讲什么"。老潘列举的三个陷阱——技术细节过多、缺少边界讨论、结构不清晰——都是 AI 容易犯的错误。**这些是人类的责任**——因为你知道业务决策者关心什么。

老潘的经验总结让两人都印象深刻：**展示的最大问题不是"技术不够深"，而是"故事不清楚"**。时间超时、技术细节过多、缺少不确定性、没有行动建议——这些都是常见陷阱。阿码笑了："我之前展示就超时了，因为想讲所有技术细节。现在才知道，决策者不关心'我跑了多少模型'，他们关心'他们该怎么做'。"

记住：**展示不是"展示你有多聪明"，而是"展示结论有多可靠"**。

现在你有了展示材料，下一步的问题是：**如何把这 16 周的学习成果整合成一个完整的期末项目包**？下一节的 **StatLab 进度**会让你收敛终稿 `report.md`，导出 `report.html` 展示版，完成 16 周的超级线。

---

## StatLab 进度

到上周为止，StatLab 报告包含了计算专题（PCA 降维、K-means 聚类、流式统计、A/B 测试工程化），但叙事是碎片化的技术堆砌——没有把散落的分析串联成数据故事，没有可复现的工程实践，没有明确的结论边界。

**本周是 StatLab 超级线的最终交付**——你要从"学生作业"升级为"专业交付"。

### StatLab 终稿的三大升级

**升级 1：从"技术堆砌"到"数据故事"**

```python
# examples/03_add_story_to_report.py

import re

def rewrite_report_as_story(report_path="report/report.md"):
    """
    用"问题-证据-不确定性-边界"结构重写报告

    本脚本将原始的技术报告重写为数据故事格式
    """
    with open(report_path, "r", encoding="utf-8") as f:
        report = f.read()

    # 在报告开头添加"数据故事概要"
    story_summary = f"""

## 数据故事概要

### 研究问题

优惠券能提升消费金额吗？

### 证据

- 优惠券组平均消费：108 元
- 对照组平均消费：100 元
- 效应量：8 元 [95% CI: 3.2, 12.8]
- Cohen's d：0.48（中等效应）
- p 值：0.014（统计显著）

### 不确定性

- 95% 信心认为优惠券有效
- 置信区间宽度：9.6 元（不确定性中等）
- 结论基于 2025 年 1-3 月数据（时间范围有限）

### 结论边界

**适用场景**：
- 结论适用于 2025 年 1-3 月的一线城市用户
- 推广到其他城市/时间需谨慎

**失效条件**：
- 购物季（如 618、双 11）用户行为变化，结论可能失效
- 如果优惠券不是随机分配（有选择偏差），结论不可靠

**未来工作**：
- 收集二线城市数据，测试结论普适性
- 分析不同用户群的效应差异（高价值 vs 低价值）

---

"""

    # 在"## 研究问题"之前插入故事概要
    report = re.sub(r"(## 研究问题)", story_summary + r"\1", report, count=1)

    # 保存重写后的报告
    with open("report/report_final.md", "w", encoding="utf-8") as f:
        f.write(report)

    print("✅ 数据故事已添加到 report/report_final.md")

# 执行重写
rewrite_report_as_story()
```

**升级 2：添加可复现章节**

```python
# examples/04_add_reproducibility_to_report.py

"""
添加可复现章节到报告

本脚本生成一个完整的可复现章节，包含依赖、随机性、数据版本
"""

reproducibility_section = """

## 可复现分析

本报告的所有分析都可以在一台新机器上复现。以下是复现步骤：

### 依赖安装

```bash
# 创建虚拟环境
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\\\\Scripts\\\\activate  # Windows

# 安装依赖
pip install -r requirements.txt
```

**requirements.txt**：
```text
numpy==1.26.4
pandas==2.2.1
scipy==1.13.0
scikit-learn==1.5.0
statsmodels==0.14.0
matplotlib==3.9.0
seaborn==0.13.0
```

### 固定随机性

所有分析使用固定随机种子 `RANDOM_SEED = 42`，确保结果可复现。

### 数据版本

- 数据来源：`data/user_behavior_20250315.csv`
- 数据版本：2025-03-15
- 数据快照：`data/snapshots/user_behavior_20250315.csv.gz`（压缩备份）

### 运行分析

```bash
# 克隆仓库
git clone https://github.com/yourusername/statlab-project.git
cd statlab-project

# 切换到最终版本
git checkout final

# 运行完整分析
python scripts/run_full_analysis.py
```

**输出**：
- 分析报告：`report/report_final.md`
- 可视化图表：`report/figures/`
- 模型文件：`models/`

---

"""

# 添加到报告末尾
with open("report/report_final.md", "a", encoding="utf-8") as f:
    f.write(reproducibility_section)

print("✅ 可复现章节已添加到 report/report_final.md")
```

**升级 3：导出 HTML 展示版**

```python
# examples/05_export_html.py

"""
从 Markdown 导出 HTML 展示版

本脚本使用 pandoc 或 markdown 库将 report.md 转换为 report.html
"""

import subprocess
import os

def export_to_html(markdown_path="report/report_final.md", output_path="report/report.html"):
    """
    导出 Markdown 为 HTML

    参数：
        markdown_path: Markdown 文件路径
        output_path: 输出 HTML 文件路径
    """
    # 检查 pandoc 是否安装
    try:
        subprocess.run(["pandoc", "--version"], check=True, capture_output=True)
        use_pandoc = True
    except (subprocess.CalledProcessError, FileNotFoundError):
        use_pandoc = False

    if use_pandoc:
        # 使用 pandoc 导出（推荐，支持数学公式、图表）
        cmd = [
            "pandoc",
            markdown_path,
            "-o", output_path,
            "--standalone",  # 独立 HTML（包含 CSS）
            "--toc",  # 目录
            "--toc-depth=2",  # 目录深度
            "--highlight-style=pygments",  # 代码高亮
            "--css=report/style.css"  # 自定义样式
        ]
        subprocess.run(cmd, check=True)
        print(f"✅ HTML 已导出：{output_path}（使用 pandoc）")
    else:
        # 使用 markdown 库导出（备选）
        try:
            import markdown
            import codecs

            with open(markdown_path, "r", encoding="utf-8") as f:
                md_content = f.read()

            html_content = markdown.markdown(md_content, extensions=['tables', 'fenced_code'])

            with open(output_path, "w", encoding="utf-8") as f:
                # 添加基本 HTML 结构
                f.write(f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>StatLab 终稿报告</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5.6.1/github-markdown.min.css">
    <style>
        body {{ max-width: 900px; margin: 2rem auto; padding: 0 1rem; }}
        img {{ max-width: 100%; }}
    </style>
</head>
<body>
{html_content}
</body>
</html>
""")
            print(f"✅ HTML 已导出：{output_path}（使用 markdown 库）")
        except ImportError:
            print("❌ 错误：未安装 pandoc 或 markdown 库")
            print("建议安装：")
            print("  - pandoc: https://pandoc.org/installing.html")
            print("  - Python markdown: pip install markdown")

# 执行导出
export_to_html()
```

### StatLab 超级线的最终交付

| 维度 | Week 01 状态 | Week 16 最终状态 |
|------|-------------|----------------|
| **数据卡** | 无 | 数据来源、字段解释、规模、缺失概览 |
| **描述统计** | 无 | 集中趋势、离散程度、分布可视化 |
| **清洗日志** | 无 | 缺失机制判断、异常值处理、决策记录 |
| **EDA 假设** | 无 | 相关分析、分组比较、可检验假设清单 |
| **推断检验** | 无 | t 检验、效应量、置信区间、p 值解释 |
| **回归建模** | 无 | 线性回归、残差诊断、多重共线性检查 |
| **分类评估** | 无 | 逻辑回归、混淆矩阵、ROC-AUC、交叉验证 |
| **因果推断** | 无 | 因果图、混杂变量、后门准则 |
| **贝叶斯分析** | 无 | 先验、后验、MCMC、先验敏感性 |
| **计算专题** | 无 | PCA 降维、K-means 聚类、流式统计、A/B 测试工程化 |
| **数据故事** | 碎片化技术堆砌 | 问题-证据-不确定性-边界的完整叙事 |
| **可复现分析** | 代码能跑 | 依赖、随机性、数据版本都可追溯 |
| **结论边界** | 只报告 R² 和 p 值 | 假设前提、适用场景、失效条件明确 |
| **展示材料** | 无 | 10-15 分钟版本，结构来自报告 |

老潘总结："StatLab 超级线的最终交付不是'加上一个章节'，而是**整合 16 周的学习成果**——从 Week 01 的数据卡，到 Week 16 的数据故事、可复现分析、结论边界。这是从'学生作业'到'专业交付'的跃迁。"

### 本周 StatLab 的改进总结

老潘用一个表格总结了 StatLab 从 Week 01 到 Week 16 的演进——这是 16 周超级线的最终交付。

| 维度 | 上周状态 | 本周改进 |
|------|---------|---------|
| 叙事结构 | 碎片化技术堆砌 | 用"问题-证据-不确定性-边界"结构重写，形成完整数据故事 |
| 可复现性 | 代码能跑，但依赖/随机性/版本不明 | 添加 requirements.txt、固定随机种子、记录数据版本 |
| 结论边界 | 只报告 R² 和 p 值 | 明确写出假设前提、适用场景、失效条件、未来工作 |
| 展示材料 | 无 | 生成 10-15 分钟版本，结构来自报告 |
| 交付物 | report.md（技术报告） | report.md 终稿 + report.html 展示版 + 展示材料（slides.md） + 可复现脚本 |

小北看完这个表格，沉默了一会儿："我从 Week 01 的'只会加载数据'，到 Week 16 的'能交付完整的分析报告'……这真的是从'学生作业'到'专业交付'的跃迁。"

老潘点头："StatLab 超级线的最终交付不是'加上一个章节'，而是**整合 16 周的学习成果**——从 Week 01 的数据卡，到 Week 16 的数据故事、可复现分析、结论边界。这是从'学生作业'到'专业交付'的跃迁。"

---

## Git 本周要点

本周必会命令：
- `git tag`: 给最终版本打标签（如 `v1.0-final`）
- `git archive`: 导出干净的项目包（不含 .git 历史）
- `git submodule`: 管理数据子模块（如果数据是单独仓库）

常见坑：

**混淆数据故事与总结**——数据故事是"问题-证据-不确定性-边界"的完整叙事，总结只是"回顾"。解决方法：在报告开头添加"数据故事概要"章节，包含四个要素。

**可复现分析不完整**——只写 requirements.txt，不固定随机性、不记录数据版本。解决方法：添加"可复现分析"章节，包含依赖、随机性、数据版本、运行命令。

**结论边界写成自我批评**——只写"不足"，不写"适用场景"。解决方法：结论边界应包含"假设前提"、"适用场景"、"失效条件"、"未来工作"四个要素。

**展示材料时间超时**——想讲所有技术细节。解决方法：展示前练习，严格控制在 10-15 分钟，技术细节放附录或 Q&A。

**展示材料缺少不确定性**——只讲显著结果，不讲边界。解决方法：主动讲"不确定性有多强"、"边界在哪里"，这是专业素养。

**版本控制混乱**——终稿、草稿、备份文件混在一起。解决方法：用 git tag 标记最终版本（`v1.0-final`），用 git branch 管理不同版本，用 gitignore 排除临时文件。

Pull Request (PR)：
- Gitea 上也叫 Pull Request，流程等价 GitHub：push 分支 -> 开 PR -> review -> merge。期末提交建议用 milestone 标记（如 `week-16-final`）。

---

## 本周小结（供后续参考）

本周是全书 16 周的收尾——从"一堆代码和图表"到"可复现、可审计、可展示的分析报告"。

你不再学习新的统计方法，而是把过去 15 周的积累收敛成一份完整的交付物：**数据故事**、**可复现分析**、**结论边界**。

**数据故事**的核心是**结构化思维**——不是"总结技术结果"，而是回答四个问题：研究问题是什么？证据有多强？不确定性有多大？结论边界在哪里？阿码的问题（"AI 生成的不算吗"）点出了 AI 时代的人类价值：**AI 可以生成文字，但不会替你判断"过度承诺"、"技术堆砌"、"缺乏边界"**——这些是人类的责任。

**可复现分析**的核心不是"代码能跑"，而是**三层标准**——能跑、可复现、可审计。小北的困惑（"为什么要记录这么多"）点出了可复现的价值：**一年后，你或同事需要知道"为什么这么做"**。可复现分析不是"额外工作"，而是"专业交付"——它让分析从"个人作业"升级为"团队资产"。老潘的经验：**可复现是团队协作的基础**。

**结论边界**的核心不是"暴露缺陷"，而是**专业承诺**——回答四个问题：假设前提是什么？适用场景是什么？失效条件是什么？未来工作是什么？小北的抵触（"为什么要暴露缺陷"）点出了结论边界的价值：**好的结论边界不是"暴露缺陷"，而是"管理预期"**——它让读者知道"你能保证什么"，而不是"让读者以为你保证了一切"。老潘的经验：**结论边界是信任的基础**。

**展示准备**的核心不是"讲所有技术细节"，而是**四段式结构**——开场、证据、不确定性与边界、行动建议。展示是"预告片"，报告是"完整电影"。阿码的问题（"展示材料能用 AI 生成吗"）点出了 AI 时代的人类价值：**AI 可以生成模板，但不会替你判断"讲什么、不讲什么"**——这些是人类的责任。老潘的经验：**展示的最大问题不是"技术不够深"，而是"故事不清楚"**。

最重要的是，你完成了 **StatLab 超级线的最终交付**——从 Week 01 的数据卡，到 Week 16 的数据故事、可复现分析、结论边界。这是从"学生作业"到"专业交付"的跃迁。你手里的不再是"一堆散落的代码和图表"，而是一份"从问题到证据到不确定性到边界"的完整叙事——这是统计学家与数据科学家的分水岭。

**回顾 16 周的旅程**：从 Week 01 的"统计三问"，到 Week 04 的"假设生成"，到 Week 06-08 的"假设检验、置信区间、重采样"，到 Week 09-12 的"回归、分类、解释、伦理"，到 Week 13-15 的"因果推断、贝叶斯、计算专题"，最后到 Week 16 的"数据故事、可复现分析、结论边界"。你从"只会跑代码"成长为"会提问题、会决策、会讲故事"。

**AI 协作的 16 周旅程**：从 Week 01-04 的"观察期"，到 Week 05-08 的"识别期"，到 Week 09-12 的"协作期"，到 Week 13-16 的"主导期"。你从"让 AI 替你做"成长为"人类主导、AI 辅助"。

**展望未来**：你已经掌握了统计学与数据分析的核心方法——从描述统计到推断检验，从回归建模到因果推断，从贝叶斯到流式计算。更重要的是，你已经学会了"数据故事、可复现分析、结论边界"——这些是"终身技能"。未来的学习和工作，你会不断回到这三个核心：**我在回答什么问题？我的证据有多强？我的结论有多不确定？我的结论在哪里不适用？**。这是统计学与 Agentic 数据分析的终极价值。


---

## Definition of Done（学生自测清单）

- [ ] 我能理解数据故事的核心结构（问题-证据-不确定性-边界）
- [ ] 我能把技术分析转化为非技术人员能理解的叙事
- [ ] 我能区分"p 值"和"结论成立的概率"
- [ ] 我能理解"置信区间"是诚实的不确定性表达
- [ ] 我能识别 AI 生成数据故事的常见陷阱（过度承诺、技术堆砌、缺乏边界）
- [ ] 我能理解可复现分析的三层标准（能跑、可复现、可审计）
- [ ] 我能实现依赖管理、固定随机性、数据版本记录
- [ ] 我能区分"数据泄漏"（统计边界）和"可复现分析"（工程边界）
- [ ] 我能理解可复现分析是团队协作的基础
- [ ] 我能写出完整的结论边界（假设前提、适用场景、失效条件、未来工作）
- [ ] 我能从"残差诊断"翻译到"结论边界"
- [ ] 我能从"因果图"翻译到"结论边界"
- [ ] 我能从"伦理审查"翻译到"结论边界"
- [ ] 我能理解结论边界不是"自我批评"，而是"专业承诺"
- [ ] 我能准备 10-15 分钟的展示材料（开场、证据、不确定性、行动建议）
- [ ] 我能区分报告（完整记录）和展示（故事摘要）的用途
- [ ] 我能识别展示的常见陷阱（时间超时、技术细节过多、缺少不确定性、没有行动建议）
- [ ] 我能识别 AI 生成展示材料的常见陷阱（技术细节过多、缺少边界、结构不清晰）
- [ ] 我能把 StatLab 报告收敛成终稿 `report.md`
- [ ] 我能导出 `report.html` 展示版
- [ ] 我能生成展示材料（PPT/海报），结构来自报告
- [ ] 我能完成 16 周的 StatLab 超级线交付
- [ ] 我用 git tag 标记了最终版本（如 `v1.0-final`）
- [ ] 我理解从"会跑代码"到"会提问题、会决策、会讲故事"的转变
