"""
ç¤ºä¾‹ï¼šåˆ†ææŠ¥å‘Šå®¡è®¡æ¸…å•è„šæœ¬

æœ¬ä¾‹æ¼”ç¤ºå¦‚ä½•ç”¨è„šæœ¬è‡ªåŠ¨æ£€æŸ¥æŠ¥å‘Šçš„å¯å¤ç°æ€§å’Œå®Œæ•´æ€§ã€‚
è¿™æ˜¯äº¤ä»˜å‰çš„æœ€åä¸€é“é˜²çº¿ï¼Œç¡®ä¿æŠ¥å‘Šç»å¾—èµ·å®¡æŸ¥ã€‚

å®¡è®¡ç»´åº¦ï¼š
1. æ•°æ®ä¸å¯å¤ç°æ€§
2. ç»Ÿè®¡å‡è®¾ä¸æ–¹æ³•
3. è¯šå®æ€§ä¸é€æ˜åº¦
4. å™äº‹ä¸ç»“æ„

è¿è¡Œæ–¹å¼ï¼špython3 chapters/week_16/examples/16_audit_checklist.py

é¢„æœŸè¾“å‡ºï¼š
- æ‰“å°å®¡è®¡ç»“æœï¼ˆé€šè¿‡/ä¸é€šè¿‡çš„é¡¹ç›®ï¼‰
- ç”Ÿæˆå®¡è®¡æ¸…å• Markdown æ–‡ä»¶
- ç»™å‡ºæ”¹è¿›å»ºè®®
"""
from __future__ import annotations

import re
import json
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime


# ===== å®¡è®¡æ£€æŸ¥é¡¹å®šä¹‰ =====
AUDIT_CHECKS = {
    "data_reproducibility": {
        "name": "æ•°æ®ä¸å¯å¤ç°æ€§",
        "checks": [
            {
                "id": "data_source",
                "name": "æ•°æ®æ¥æºæ˜ç¡®",
                "description": "æŠ¥å‘Šå†™æ¸…æ¥šæ•°æ®ä»å“ªæ¥ï¼ˆURLã€é‡‡é›†æ—¶é—´ã€æ•°æ®é›†åç§°ï¼‰",
                "check_func": "check_data_source",
                "severity": "critical"
            },
            {
                "id": "random_seed",
                "name": "éšæœºç§å­å›ºå®š",
                "description": "æ‰€æœ‰éšæœºæ“ä½œéƒ½å›ºå®šäº†ç§å­",
                "check_func": "check_random_seed",
                "severity": "critical"
            },
            {
                "id": "dependency_version",
                "name": "ä¾èµ–ç‰ˆæœ¬è®°å½•",
                "description": "åˆ—å‡ºäº†æ‰€æœ‰åº“çš„ç‰ˆæœ¬å·",
                "check_func": "check_dependency_version",
                "severity": "high"
            },
            {
                "id": "code_runnable",
                "name": "ä»£ç å¯è¿è¡Œ",
                "description": "é™„å¸¦å¯è¿è¡Œçš„è„šæœ¬æˆ– README è¯´æ˜",
                "check_func": "check_code_runnable",
                "severity": "high"
            }
        ]
    },
    "statistical_assumptions": {
        "name": "ç»Ÿè®¡å‡è®¾ä¸æ–¹æ³•",
        "checks": [
            {
                "id": "assumption_checked",
                "name": "æ£€éªŒå‰æéªŒè¯",
                "description": "åœ¨ä½¿ç”¨ t æ£€éªŒ/ANOVA/å›å½’å‰æ£€æŸ¥äº†å‡è®¾ï¼ˆæ­£æ€æ€§ã€æ–¹å·®é½æ€§ã€çº¿æ€§ç­‰ï¼‰",
                "check_func": "check_assumption",
                "severity": "high"
            },
            {
                "id": "confidence_interval",
                "name": "ä¸ç¡®å®šæ€§é‡åŒ–",
                "description": "æŠ¥å‘Šäº†ç½®ä¿¡åŒºé—´æˆ–æ ‡å‡†è¯¯ï¼Œä¸åªæ˜¯ç‚¹ä¼°è®¡",
                "check_func": "check_confidence_interval",
                "severity": "high"
            },
            {
                "id": "multiple_comparison",
                "name": "å¤šé‡æ¯”è¾ƒæ ¡æ­£",
                "description": "ä¸€æ¬¡æ€§æ£€éªŒå¤šä¸ªæŒ‡æ ‡æ—¶è¯´æ˜æ˜¯å¦åšäº†æ ¡æ­£",
                "check_func": "check_multiple_comparison",
                "severity": "medium"
            },
            {
                "id": "model_diagnostics",
                "name": "æ¨¡å‹è¯Šæ–­",
                "description": "å›å½’æ¨¡å‹åŒ…å«æ®‹å·®è¯Šæ–­ã€å¼‚å¸¸ç‚¹å½±å“åˆ†æ",
                "check_func": "check_model_diagnostics",
                "severity": "high"
            }
        ]
    },
    "honesty_transparency": {
        "name": "è¯šå®æ€§ä¸é€æ˜åº¦",
        "checks": [
            {
                "id": "chart_honesty",
                "name": "å›¾è¡¨è¯šå®æ€§",
                "description": "Y è½´æœªæˆªæ–­ï¼ˆæŸ±çŠ¶å›¾ä» 0 å¼€å§‹ï¼‰ï¼Œæ ‡æ³¨äº†æ ·æœ¬é‡",
                "check_func": "check_chart_honesty",
                "severity": "high"
            },
            {
                "id": "missing_disclosed",
                "name": "ç¼ºå¤±å¤„ç†è¯´æ˜",
                "description": "å†™æ¸…æ¥šäº†ç¼ºå¤±å€¼æœºåˆ¶å’Œå¤„ç†ç­–ç•¥",
                "check_func": "check_missing_disclosed",
                "severity": "high"
            },
            {
                "id": "causation_boundary",
                "name": "å› æœå£°æ˜è¾¹ç•Œ",
                "description": "åŒºåˆ†'ç›¸å…³'ä¸'å› æœ'ï¼Œä¸è¯´'è¯æ˜'äº†å› æœå…³ç³»",
                "check_func": "check_causation_boundary",
                "severity": "critical"
            },
            {
                "id": "limitations_stated",
                "name": "æ¨¡å‹é™åˆ¶è¯´æ˜",
                "description": "æ˜ç¡®æ¨¡å‹é€‚ç”¨èŒƒå›´å’Œå¤±æ•ˆåœºæ™¯",
                "check_func": "check_limitations",
                "severity": "medium"
            }
        ]
    },
    "narrative_structure": {
        "name": "å™äº‹ä¸ç»“æ„",
        "checks": [
            {
                "id": "question_clear",
                "name": "ç ”ç©¶é—®é¢˜æ¸…æ™°",
                "description": "æŠ¥å‘Šå¼€å¤´æ˜ç¡®è¦å›ç­”çš„é—®é¢˜",
                "check_func": "check_question_clear",
                "severity": "high"
            },
            {
                "id": "method_traceable",
                "name": "æ–¹æ³•å¯è¿½æº¯",
                "description": "æ¯ä¸ªç»“è®ºå¯¹åº”çš„åˆ†ææ–¹æ³•å†™æ¸…æ¥š",
                "check_func": "check_method_traceable",
                "severity": "medium"
            },
            {
                "id": "results_discussion_separated",
                "name": "ç»“æœä¸è®¨è®ºåˆ†ç¦»",
                "description": "ç»“æœæ˜¯'å‘ç°äº†ä»€ä¹ˆ'ï¼Œè®¨è®ºæ˜¯'æ„å‘³ç€ä»€ä¹ˆ'",
                "check_func": "check_results_discussion_separated",
                "severity": "low"
            },
            {
                "id": "conclusion_no_exaggeration",
                "name": "ç»“è®ºä¸å¤¸å¤§",
                "description": "ä¸è¯´'è¯æ˜äº†'ï¼Œè¯´'æ”¯æŒäº†/æš—ç¤ºäº†'",
                "check_func": "check_conclusion_no_exaggeration",
                "severity": "high"
            }
        ]
    }
}


# ===== æ£€æŸ¥å‡½æ•°å®ç° =====

class ReportAuditor:
    """æŠ¥å‘Šå®¡è®¡å™¨"""

    def __init__(self, report_path: str, code_dir: str = None):
        """
        åˆå§‹åŒ–å®¡è®¡å™¨

        å‚æ•°:
            report_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆ.md æˆ– .htmlï¼‰
            code_dir: ä»£ç ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.report_path = Path(report_path)
        self.code_dir = Path(code_dir) if code_dir else None
        self.report_content = ""
        self.audit_results = {}

        # è¯»å–æŠ¥å‘Šå†…å®¹
        if self.report_path.exists():
            with open(self.report_path, 'r', encoding='utf-8') as f:
                self.report_content = f.read()
        else:
            raise FileNotFoundError(f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_path}")

    def check_data_source(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ•°æ®æ¥æºæ˜¯å¦æ˜ç¡®"""
        keywords = ['æ•°æ®æ¥æº', 'æ•°æ®é›†', 'dataset', 'source', 'Kaggle', 'UCI']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°æ•°æ®æ¥æºæè¿°" if found else "æœªæ‰¾åˆ°æ•°æ®æ¥æºæè¿°"

    def check_random_seed(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦å›ºå®šäº†éšæœºç§å­"""
        keywords = ['random_seed', 'éšæœºç§å­', 'seed', 'np.random.seed']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°éšæœºç§å­è®¾ç½®" if found else "æœªæ‰¾åˆ°éšæœºç§å­è®¾ç½®"

    def check_dependency_version(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦è®°å½•äº†ä¾èµ–ç‰ˆæœ¬"""
        keywords = ['ä¾èµ–ç‰ˆæœ¬', 'requirements', 'version', 'ç‰ˆæœ¬']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°ä¾èµ–ç‰ˆæœ¬è®°å½•" if found else "æœªæ‰¾åˆ°ä¾èµ–ç‰ˆæœ¬è®°å½•"

    def check_code_runnable(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯è¿è¡Œçš„ä»£ç """
        if self.code_dir is None:
            return False, "æœªæŒ‡å®šä»£ç ç›®å½•ï¼Œæ— æ³•æ£€æŸ¥"

        # æ£€æŸ¥æ˜¯å¦æœ‰ Python è„šæœ¬æˆ– README
        py_files = list(self.code_dir.glob("*.py"))
        readme = self.code_dir / "README.md"

        has_script = len(py_files) > 0
        has_readme = readme.exists()

        if has_script or has_readme:
            return True, f"æ‰¾åˆ° {len(py_files)} ä¸ª Python è„šæœ¬" if has_script else "æ‰¾åˆ° README"
        return False, "æœªæ‰¾åˆ°å¯è¿è¡Œçš„ä»£ç è¯´æ˜"

    def check_assumption(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦éªŒè¯äº†ç»Ÿè®¡å‡è®¾"""
        keywords = ['å‡è®¾', 'æ­£æ€æ€§', 'æ–¹å·®é½æ€§', 'æ®‹å·®', 'assumption',
                    'Shapiro', 'Levene', 'æ®‹å·®å›¾']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°å‡è®¾æ£€éªŒç›¸å…³å†…å®¹" if found else "æœªæ‰¾åˆ°å‡è®¾æ£€éªŒ"

    def check_confidence_interval(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦æŠ¥å‘Šäº†ç½®ä¿¡åŒºé—´"""
        keywords = ['ç½®ä¿¡åŒºé—´', '95% CI', 'confidence interval', 'CI:', 'æ ‡å‡†è¯¯']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°ç½®ä¿¡åŒºé—´æŠ¥å‘Š" if found else "æœªæ‰¾åˆ°ç½®ä¿¡åŒºé—´"

    def check_multiple_comparison(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦å¤„ç†äº†å¤šé‡æ¯”è¾ƒ"""
        keywords = ['å¤šé‡æ¯”è¾ƒ', 'Bonferroni', 'FDR', 'æ ¡æ­£', 'correction']
        found = any(kw in self.report_content for kw in keywords)
        # å¦‚æœæ²¡æœ‰å¤šä¸ªæ£€éªŒï¼Œè¿™ä¸ªæ£€æŸ¥é€šè¿‡
        test_count = self.report_content.count('p å€¼') + self.report_content.count('p-value')
        if test_count <= 1:
            return True, "åªæœ‰ä¸€ä¸ªæ£€éªŒï¼Œæ— éœ€å¤šé‡æ¯”è¾ƒæ ¡æ­£"
        return found, "æ‰¾åˆ°å¤šé‡æ¯”è¾ƒå¤„ç†" if found else "æœ‰å¤šä¸ªæ£€éªŒä½†æœªè¯´æ˜æ˜¯å¦æ ¡æ­£"

    def check_model_diagnostics(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹è¯Šæ–­"""
        keywords = ['æ®‹å·®', 'è¯Šæ–­', 'diagnostic', 'residual', 'QQ', 'æ®‹å·®å›¾']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°æ¨¡å‹è¯Šæ–­å†…å®¹" if found else "æœªæ‰¾åˆ°æ¨¡å‹è¯Šæ–­"

    def check_chart_honesty(self) -> Tuple[bool, str]:
        """æ£€æŸ¥å›¾è¡¨è¯šå®æ€§ï¼ˆç®€åŒ–ç‰ˆï¼šåªæ£€æŸ¥æ˜¯å¦æåˆ°äº†æ ·æœ¬é‡ï¼‰"""
        keywords = ['æ ·æœ¬é‡', 'n=', 'N=', 'sample size']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°æ ·æœ¬é‡æ ‡æ³¨" if found else "æœªæ‰¾åˆ°æ ·æœ¬é‡æ ‡æ³¨"

    def check_missing_disclosed(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦è¯´æ˜äº†ç¼ºå¤±å€¼å¤„ç†"""
        keywords = ['ç¼ºå¤±', 'missing', 'æ’è¡¥', 'åˆ é™¤', 'impute']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°ç¼ºå¤±å€¼å¤„ç†è¯´æ˜" if found else "æœªæ‰¾åˆ°ç¼ºå¤±å€¼è¯´æ˜"

    def check_causation_boundary(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦åŒºåˆ†äº†ç›¸å…³å’Œå› æœ"""
        # æ£€æŸ¥æ˜¯å¦é¿å…äº†"è¯æ˜"å› æœçš„è¯´æ³•
        bad_patterns = [
            r'è¯æ˜.*å¯¼è‡´', r'è¯å®.*å› æœ', r'proves.*caus',
            r'å¿…ç„¶å¯¼è‡´', r'è‚¯å®š.*å› æœ'
        ]
        has_bad_claim = any(re.search(p, self.report_content) for p in bad_patterns)

        # æ£€æŸ¥æ˜¯å¦æœ‰æ°å½“çš„è¾¹ç•Œå£°æ˜
        good_patterns = [
            r'ç›¸å…³.*å› æœ', r'ä¸.*å› æœ', r'ä¸èƒ½.*å› æœ',
            r'correlation.*causation', r'ä¸ç­‰äº'
        ]
        has_boundary = any(re.search(p, self.report_content) for p in good_patterns)

        if has_bad_claim:
            return False, "æŠ¥å‘Šä¸­æœ‰ç»å¯¹çš„å› æœå£°æ˜ï¼Œå»ºè®®ä¿®æ”¹"
        if has_boundary:
            return True, "æ‰¾åˆ°äº†ç›¸å…³/å› æœè¾¹ç•Œå£°æ˜"
        return False, "æœªæ˜ç¡®åŒºåˆ†ç›¸å…³å’Œå› æœ"

    def check_limitations(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦è¯´æ˜äº†ç ”ç©¶å±€é™"""
        keywords = ['å±€é™', 'é™åˆ¶', 'limitation', 'è¾¹ç•Œ', 'å‡è®¾æ¡ä»¶']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°ç ”ç©¶å±€é™è¯´æ˜" if found else "æœªæ‰¾åˆ°å±€é™è¯´æ˜"

    def check_question_clear(self) -> Tuple[bool, str]:
        """æ£€æŸ¥ç ”ç©¶é—®é¢˜æ˜¯å¦æ¸…æ™°"""
        # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜ç›¸å…³çš„ç« èŠ‚æ ‡é¢˜æˆ–å†…å®¹
        has_question_section = re.search(r'##.*é—®é¢˜|##.*ç›®æ ‡', self.report_content)
        return (has_question_section is not None,
                "æ‰¾åˆ°é—®é¢˜/ç›®æ ‡ç« èŠ‚" if has_question_section else "æœªæ‰¾åˆ°æ˜ç¡®çš„é—®é¢˜é™ˆè¿°")

    def check_method_traceable(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ–¹æ³•æ˜¯å¦å¯è¿½æº¯"""
        keywords = ['æ–¹æ³•', 'method', 't æ£€éªŒ', 'ANOVA', 'å›å½’', 'é€»è¾‘å›å½’']
        found = any(kw in self.report_content for kw in keywords)
        return found, "æ‰¾åˆ°æ–¹æ³•æè¿°" if found else "æœªæ‰¾åˆ°æ–¹æ³•æè¿°"

    def check_results_discussion_separated(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦åˆ†ç¦»äº†ç»“æœå’Œè®¨è®º"""
        has_results = re.search(r'##.*ç»“æœ', self.report_content)
        has_discussion = re.search(r'##.*è®¨è®º|##.*ç»“è®º', self.report_content)
        separated = has_results and has_discussion
        return separated, "ç»“æœå’Œè®¨è®ºåˆ†ç¦»" if separated else "ç»“æœå’Œè®¨è®ºæœªæ˜ç¡®åˆ†ç¦»"

    def check_conclusion_no_exaggeration(self) -> Tuple[bool, str]:
        """æ£€æŸ¥ç»“è®ºæ˜¯å¦å¤¸å¤§"""
        # æ£€æŸ¥æ˜¯å¦é¿å…ä½¿ç”¨"è¯æ˜"ç­‰ç»å¯¹è¯æ±‡
        bad_words = ['è¯æ˜äº†', 'è¯å®äº†', 'ç¡®å‡¿']
        has_bad = any(bw in self.report_content for bw in bad_words)
        return (not has_bad,
                "ç»“è®ºç”¨è¯è°¨æ…" if not has_bad else "ç»“è®ºä¸­æœ‰å¤¸å¤§ç”¨è¯ï¼ˆ'è¯æ˜'ç­‰ï¼‰")

    def run_all_checks(self) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰å®¡è®¡æ£€æŸ¥

        è¿”å›:
            å®¡è®¡ç»“æœå­—å…¸
        """
        print("\nå¼€å§‹å®¡è®¡...")
        print("=" * 60)

        results = {
            "timestamp": datetime.now().isoformat(),
            "report_path": str(self.report_path),
            "categories": {}
        }

        # è·å–æ‰€æœ‰æ£€æŸ¥æ–¹æ³•
        check_methods = {
            name: getattr(self, name)
            for name in dir(self)
            if name.startswith('check_') and callable(getattr(self, name))
        }

        # éå†æ¯ä¸ªç±»åˆ«
        for cat_id, category in AUDIT_CHECKS.items():
            cat_results = {
                "name": category["name"],
                "checks": []
            }

            print(f"\nç±»åˆ«: {category['name']}")
            print("-" * 40)

            for check in category["checks"]:
                check_id = check["id"]
                check_func_name = check["check_func"]

                if check_func_name in check_methods:
                    passed, message = check_methods[check_func_name]()
                else:
                    passed, message = False, f"æ£€æŸ¥å‡½æ•°æœªå®ç°: {check_func_name}"

                check_result = {
                    "id": check_id,
                    "name": check["name"],
                    "description": check["description"],
                    "severity": check["severity"],
                    "passed": passed,
                    "message": message
                }

                cat_results["checks"].append(check_result)

                # æ‰“å°ç»“æœ
                status = "âœ“" if passed else "âœ—"
                print(f"  {status} {check['name']}: {message}")

            results["categories"][cat_id] = cat_results

        return results

    def generate_markdown_report(self, audit_results: Dict,
                                  output_path: str = 'output/audit_checklist.md') -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„å®¡è®¡æ¸…å•

        å‚æ•°:
            audit_results: å®¡è®¡ç»“æœå­—å…¸
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        è¿”å›:
            Markdown å­—ç¬¦ä¸²
        """
        lines = []

        lines.append("# åˆ†ææŠ¥å‘Šå®¡è®¡æ¸…å•\n")
        lines.append(f"> **å®¡è®¡æ—¶é—´**ï¼š{audit_results['timestamp']}\n")
        lines.append(f"> **æŠ¥å‘Šè·¯å¾„**ï¼š{audit_results['report_path']}\n")
        lines.append("---\n")

        # ç»Ÿè®¡
        total_checks = 0
        passed_checks = 0
        critical_failed = 0

        for cat_data in audit_results["categories"].values():
            for check in cat_data["checks"]:
                total_checks += 1
                if check["passed"]:
                    passed_checks += 1
                elif check["severity"] == "critical":
                    critical_failed += 1

        lines.append("## å®¡è®¡æ‘˜è¦\n\n")
        lines.append(f"- **æ€»æ£€æŸ¥é¡¹**ï¼š{total_checks}\n")
        lines.append(f"- **é€šè¿‡é¡¹**ï¼š{passed_checks}\n")
        lines.append(f"- **ä¸é€šè¿‡é¡¹**ï¼š{total_checks - passed_checks}\n")
        lines.append(f"- **å…³é”®å¤±è´¥**ï¼š{critical_failed}\n")

        if critical_failed > 0:
            lines.append(f"\nâš ï¸ **è­¦å‘Š**ï¼šæœ‰ {critical_failed} ä¸ªå…³é”®æ£€æŸ¥é¡¹æœªé€šè¿‡ï¼Œå»ºè®®ä¿®å¤åå†äº¤ä»˜ã€‚\n")
        elif passed_checks == total_checks:
            lines.append("\nâœ“ **æ‰€æœ‰æ£€æŸ¥é¡¹é€šè¿‡**ï¼ŒæŠ¥å‘Šå¯ä»¥äº¤ä»˜ã€‚\n")
        else:
            lines.append("\n**æ³¨æ„**ï¼šæœ‰éå…³é”®æ£€æŸ¥é¡¹æœªé€šè¿‡ï¼Œå»ºè®®å®¡æŸ¥åå†³å®šæ˜¯å¦äº¤ä»˜ã€‚\n")

        lines.append("\n---\n")

        # è¯¦ç»†æ¸…å•
        for cat_id, cat_data in audit_results["categories"].items():
            lines.append(f"## {cat_data['name']}\n\n")

            for check in cat_data["checks"]:
                status_icon = "âœ…" if check["passed"] else "âŒ"
                severity_badge = f"`{check['severity'].upper()}`" if not check["passed"] else ""

                lines.append(f"### {status_icon} {check['name']} {severity_badge}\n\n")
                lines.append(f"{check['description']}\n\n")
                lines.append(f"**å®¡è®¡ç»“æœ**ï¼š{check['message']}\n\n")

                if not check["passed"] and check["severity"] in ["high", "critical"]:
                    lines.append(f"ğŸ’¡ **å»ºè®®**ï¼šä¼˜å…ˆå¤„ç†æ­¤é¡¹ã€‚\n\n")

            lines.append("\n")

        # æ·»åŠ æ”¹è¿›å»ºè®®
        lines.append("---\n")
        lines.append("## æ”¹è¿›å»ºè®®\n\n")

        has_suggestions = False
        for cat_data in audit_results["categories"].values():
            for check in cat_data["checks"]:
                if not check["passed"] and check["severity"] in ["high", "critical"]:
                    has_suggestions = True
                    lines.append(f"- **{check['name']}**ï¼š{check['description']}\n")

        if not has_suggestions:
            lines.append("æ‰€æœ‰å…³é”®æ£€æŸ¥é¡¹å·²é€šè¿‡ï¼Œæ— éœ€æ”¹è¿›å»ºè®®ã€‚\n")

        markdown = "".join(lines)

        # å†™å…¥æ–‡ä»¶
        output_file = Path(output_path)
        output_file.parent.mkdir(exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown)

        print(f"\nå®¡è®¡æ¸…å•å·²ä¿å­˜åˆ°: {output_path}")

        return markdown


# ===== æ¼”ç¤ºå®¡è®¡æµç¨‹ =====
def demo_audit():
    """æ¼”ç¤ºå®¡è®¡æµç¨‹"""
    print("=" * 60)
    print("åˆ†ææŠ¥å‘Šå®¡è®¡æ¸…å•")
    print("=" * 60)

    # é¦–å…ˆåˆ›å»ºä¸€ä¸ªç¤ºä¾‹æŠ¥å‘Š
    sample_report = """# å®¢æˆ·æµå¤±åˆ†ææŠ¥å‘Š

> **æŠ¥å‘Šç”Ÿæˆæ—¶é—´**ï¼š2026-02-21
> **éšæœºç§å­**ï¼š42

## å¯å¤ç°ä¿¡æ¯

- **æ•°æ®æ¥æº**ï¼šKaggle ç”µå•†å®¢æˆ·æ•°æ®é›†ï¼ˆ2025å¹´é‡‡é›†ï¼‰
- **æ ·æœ¬æ•°é‡**ï¼š1000 ä¸ªå®¢æˆ·
- **ä¾èµ–ç‰ˆæœ¬**ï¼š
  - numpy: 1.24.0
  - pandas: 2.0.0
  - scikit-learn: 1.3.0

## ç ”ç©¶é—®é¢˜

æœ¬åˆ†ææ—¨åœ¨å›ç­”ï¼šå“ªäº›å®¢æˆ·ç‰¹å¾ä¸æµå¤±è¡Œä¸ºç›¸å…³ï¼Ÿ

## æè¿°ç»Ÿè®¡

| æŒ‡æ ‡ | å‡å€¼ | æ ‡å‡†å·® |
|------|------|--------|
| ä½¿ç”¨æ—¶é•¿ | 24.5 | 15.3 |
| æœˆæ¶ˆè´¹ | 85.2 | 45.6 |

## ç»Ÿè®¡æ£€éªŒ

æˆ‘ä»¬ä½¿ç”¨ Mann-Whitney U æ£€éªŒï¼ˆå› ä¸ºæ•°æ®ä¸å®Œå…¨æ»¡è¶³æ­£æ€å‡è®¾ï¼‰ï¼š

- **ä½¿ç”¨æ—¶é•¿å·®å¼‚**ï¼šp < 0.001ï¼ˆæ˜¾è‘—ï¼‰
- **æ¶ˆè´¹é‡‘é¢å·®å¼‚**ï¼šp = 0.003 [95% CI: 2.1, 4.8]ï¼ˆæ˜¾è‘—ï¼‰

## å»ºæ¨¡ç»“æœ

é€»è¾‘å›å½’æ¨¡å‹çš„ AUC ä¸º 0.78ï¼Œ95% ç½®ä¿¡åŒºé—´ [0.72, 0.84]ã€‚

**æ®‹å·®è¯Šæ–­**ï¼šæ®‹å·®å¤§è‡´æ­£æ€åˆ†å¸ƒï¼Œæ— æ˜æ˜¾å¼‚æ–¹å·®æ€§ã€‚

## ç»“è®º

åˆ†æ**æ”¯æŒ**ä»¥ä¸‹ç»“è®ºï¼š
1. ä½¿ç”¨æ—¶é•¿ä¸æµå¤±ç›¸å…³ï¼ˆä½†ä¸èƒ½ç¡®å®šå› æœå…³ç³»ï¼‰
2. æ¶ˆè´¹è¡Œä¸ºæ˜¯é¢„æµ‹æµå¤±çš„é‡è¦å› å­

### ç ”ç©¶å±€é™

1. æ•°æ®ä¸ºæ¨ªæˆªé¢æ•°æ®ï¼Œæ— æ³•ç¡®å®šå› æœæ–¹å‘
2. æ ·æœ¬æ¥è‡ªå•ä¸€å¹³å°ï¼Œå¤–æ¨æ€§æœ‰é™
3. æœªè€ƒè™‘å­£èŠ‚æ€§å› ç´ 

---

*æœ¬æŠ¥å‘Šç”±å¯å¤ç°åˆ†ææµæ°´çº¿è‡ªåŠ¨ç”Ÿæˆ*
"""

    # å†™å…¥ç¤ºä¾‹æŠ¥å‘Š
    report_path = Path('output/sample_report_for_audit.md')
    report_path.parent.mkdir(exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(sample_report)
    print(f"ç¤ºä¾‹æŠ¥å‘Šå·²åˆ›å»º: {report_path}")

    # åˆ›å»ºå®¡è®¡å™¨å¹¶è¿è¡Œæ£€æŸ¥
    auditor = ReportAuditor(str(report_path))
    results = auditor.run_all_checks()

    # ç”Ÿæˆå®¡è®¡æ¸…å•
    auditor.generate_markdown_report(results, 'output/audit_checklist.md')

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("å®¡è®¡æ‘˜è¦")
    print("=" * 60)

    total = sum(len(c["checks"]) for c in results["categories"].values())
    passed = sum(
        sum(1 for check in cat["checks"] if check["passed"])
        for cat in results["categories"].values()
    )

    print(f"\næ€»æ£€æŸ¥é¡¹: {total}")
    print(f"é€šè¿‡: {passed}")
    print(f"ä¸é€šè¿‡: {total - passed}")
    print(f"é€šè¿‡ç‡: {passed/total*100:.1f}%")

    if passed == total:
        print("\nâœ“ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ŒæŠ¥å‘Šå¯ä»¥äº¤ä»˜ï¼")
    else:
        print(f"\nâš  æœ‰ {total - passed} ä¸ªæ£€æŸ¥é¡¹æœªé€šè¿‡ï¼Œè¯·æŸ¥çœ‹å®¡è®¡æ¸…å•è¯¦æƒ…ã€‚")


# ===== ä¸»å‡½æ•° =====
def main() -> None:
    """è¿è¡Œå®¡è®¡æ¼”ç¤º"""
    demo_audit()

    print("\n" + "=" * 60)
    print("ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("""
åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨å®¡è®¡å™¨ï¼š

1. å¯¹ä½ çš„ report.md è¿è¡Œå®¡è®¡ï¼š
   ```python
   auditor = ReportAuditor('path/to/report.md', code_dir='path/to/scripts')
   results = auditor.run_all_checks()
   auditor.generate_markdown_report(results, 'audit_checklist.md')
   ```

2. åŒè¡Œè¯„å®¡æ—¶äº¤æ¢å®¡è®¡æ¸…å•ï¼Œé€é¡¹æ£€æŸ¥

3. ä¿®å¤å…³é”®é—®é¢˜åé‡æ–°å®¡è®¡

4. å°†å®¡è®¡æ¸…å•éšæŠ¥å‘Šä¸€èµ·äº¤ä»˜

é˜¿ç é—®ï¼š'èƒ½ä¸èƒ½ç”¨ AI æ¥åšå®¡è®¡ï¼Ÿ'

è€æ½˜è¯´ï¼š'AI å¯ä»¥å¸®ä½ æ£€æŸ¥æ ¼å¼å’Œå…³é”®è¯ï¼Œä½†å®ƒä¸çŸ¥é“
ä½ çš„æ•°æ®æ¥æºæ˜¯å¦çœŸå®ã€å‡è®¾æ˜¯å¦åˆç†ã€‚å®¡è®¡æ¸…å•æ˜¯
å·¥å…·ï¼Œåˆ¤æ–­è¿˜å¾—é äººã€‚'
    """)


if __name__ == "__main__":
    main()
