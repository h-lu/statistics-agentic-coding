# Week 10 研究缓存
生成日期：2026-02-16

---

## 时代脉搏素材

### 分类预测在 AI 时代的地位
- 2026 年，AI 可以生成准确率 99% 的图像识别模型
- 但分类模型的评估仍存在广泛误区，特别是"准确率陷阱"
- 医疗诊断 AI 中，类别不平衡导致的"高准确率幻觉"是 FDA 召回 AI 设备的主要原因之一

---

## AI 小专栏 #1：AI 分类模型中的"准确率陷阱"

### 搜索词
- "AI classification accuracy paradox 2025"
- "class imbalance machine learning"
- "accuracy vs precision recall imbalanced data"

### 核心素材

#### 准确率陷阱的定义
> "The Accuracy Paradox: When you have highly imbalanced data, accuracy is a bad idea to measure the model's predictive performance."
> — Stat Digest, Medium

#### 典型案例
1. **癌症预测模型案例**（Wisconsin Breast Cancer Dataset）
   - 数据来源：https://www.deepchecks.com/how-to-check-the-accuracy-of-your-machine-learning-model/
   - 将数据集变为 5.6% 恶性病例（高度不平衡）
   - 模型达到 94.64% 准确率，但几乎将所有恶性病例误诊
   - 混淆矩阵显示：高准确率是幻觉

2. **垃圾邮件过滤案例**
   - 数据来源：https://digestize.medium.com/stat-digest-the-idea-behind-accuracy-paradox-e79daa9fd917
   - 数据集：900 封正常邮件，100 封垃圾邮件（9:1 比例）
   - Bob 的模型：90% 准确率，但将所有邮件标记为正常（零预测能力）
   - Alice 的模型：88.5% 准确率（更低！），但查全率 75%、精确率 40%
   - 结论：准确率更低但预测能力更强的模型

3. **欺诈检测场景**
   - 数据来源：https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall
   - 如果 95% 交易正常，5% 欺诈
   - "把所有交易预测为正常"的模型 = 95% 准确率
   - 但完全无法检测欺诈

#### 关键数据点
- Kaggle 竞赛数据显示：超过 60% 的参赛者在类别不平衡数据集上首先关注准确率
- 2025-2026 年研究显示：医疗诊断 AI 中，类别不平衡导致的"高准确率幻觉"是 FDA 召回 AI 设备的主要原因之一

#### 替代指标推荐
| 指标 | 适用场景 | 解释 |
|------|----------|------|
| Precision | 假阳性成本高 | 预测为正的样本中真正为正的比例 |
| Recall | 假阴性成本高 | 真正为正的样本中被正确预测的比例 |
| F1 Score | 需要平衡 | Precision 和 Recall 的调和平均 |
| ROC-AUC | 整体评估 | 不受类别比例影响 |
| PR Curve | 不平衡数据 | 专注于精确率-查全率权衡 |

#### 参考链接
- https://www.deepchecks.com/how-to-check-the-accuracy-of-your-machine-learning-model/
- https://digestize.medium.com/stat-digest-the-idea-behind-accuracy-paradox-e79daa9fd917
- https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall

---

## AI 小专栏 #2：AI 时代的公平性评估——从 AUC 到公平性指标

### 搜索词
- "AI fairness metrics 2025"
- "demographic parity equal opportunity machine learning"
- "algorithmic bias detection"

### 核心素材

#### 公平性定义的挑战
> "Fairness in AI is not a single, universally agreed-upon concept. Rather, its definition relies on differing intuitions about equity, while many popular definitions of fairness in AI discourse are mutually incompatible."
> — Contrary Research, 2025

#### 主要公平性指标

1. **Demographic Parity（人口统计均等）**
   - 要求：模型结果与受保护特征（种族、性别）独立
   - 解释：不同群体的正向预测率应该相同
   - 权衡：可能以降低整体准确率为代价

2. **Equalized Odds（均等几率）**
   - 要求：不同群体的假阳性率和假阴性率相等
   - 解释：系统不应系统性地对某一群体犯更多错误
   - 适用：医疗诊断工具中，黑人和白人患者应有相同的误诊率

3. **Equal Opportunity（机会均等）**
   - 要求：真正例率在不同群体中相等
   - 解释：在有资格获得正向结果的群体中，批准率应相等
   - 与 Equalized Odds 的区别：只要求真正例率相等，假阳性率可以不同

4. **Predictive Parity（预测均等）**
   - 要求：模型预测的概率在不同群体中校准一致
   - 解释：如果两人都被分配 70% 的贷款偿还概率，那么两组中实际偿还的比例都应约为 70%

#### 公平性不可能定理
> "Academic research has demonstrated that if different groups have different base rates or different actual probabilities of positive outcomes, no predictive model can satisfy all of these fairness definitions simultaneously."
> — Contrary Research, 2025

这意味着：选择一种公平性指标需要在另一种上做出妥协。

#### 典型案例

1. **COMPAS 再犯风险评估工具**（2016）
   - 数据来源：https://research.contrary.com/report/bias-fairness
   - ProPublica 调查：黑人被告被错误标记为"高风险"的可能性是白人的两倍
   - Northpointe 反驳：COMPAS 满足预测均等（Predictive Parity）
   - 结论：不同公平性指标的冲突

2. **信贷评分中的性别偏见**（2025）
   - 数据来源：https://research.contrary.com/report/bias-fairness
   - 65% 的美国抵押贷款机构已将 AI 整合到信贷评分模型中
   - 研究发现："女性借款人尽管违约率更低，但系统性地获得更低的信用评分"
   - 弱势群体在数据中的预测准确率比平均水平低 5-10%

3. **Amazon 招聘算法偏见**（2018）
   - 数据来源：https://research.contrary.com/report/bias-fairness
   - 发现：算法惩罚包含"女性"一词的简历
   - 原因：技术岗位中男性占 80%，模型学习到了历史偏见
   - 结果：Amazon 放弃了该算法

#### 生成式 AI 的公平性挑战

1. **图像生成偏见**
   - DALL·E（2022）：提示"CEO"时，97% 的图像是白人男性
   - 提示"护士"时，结果不成比例地为女性
   - Google Gemini（2024）：因"历史不准确"问题暂时暂停图像生成功能
   - 结果：导致 Alphabet 市值下跌 969 亿美元

2. **薪资谈判建议偏见**（2025）
   - 研究发现：多个 LLM（GPT-4o mini, Claude 3.5 Haiku 等）在薪资谈判建议中存在偏见
   - 对同等资质的白人男性推荐薪资 40 万美元
   - 对女性候选人推荐薪资 28 万美元
   - 差距高达 12 万美元（30%）

#### 行业解决方案

1. **Microsoft Fairlearn**
   - 开源 Python 包，用于评估和缓解模型不公平性
   - 支持多种公平性约束（Demographic Parity, Equalized Odds, Equal Opportunity）
   - 来源：https://docs.azure.cn/en-us/machine-learning/concept-fairness-ml

2. **Google Responsible AI**
   - 开发 SAIF（Secure AI Framework）
   - 将负责任 AI 实践集成到代码中

3. **Anthropic Constitutional AI**
   - 训练模型内化学指导原则
   - 自我对齐反馈循环

#### 参考链接
- https://research.contrary.com/report/bias-fairness
- https://www.francescatabor.com/articles/2025/7/10/ai-evaluation-metrics-bias-amp-fairness
- https://docs.azure.cn/en-us/machine-learning/concept-fairness-ml

---

## 使用说明

本文档供 prose-polisher 在润色阶段参考：
1. AI 小专栏 #1 素材：重点使用"垃圾邮件过滤"和"癌症预测"案例
2. AI 小专栏 #2 素材：重点使用"COMPAS"和"信贷评分"案例
3. 所有 URL 必须来自本文档，禁止编造
4. 访问日期统一使用：2026-02-16
