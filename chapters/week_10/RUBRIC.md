# Week 10：分类模型与逻辑回归——评分细则 (Rubric)

> **评分原则**：评分项必须可验证（通过 tests 或代码审查支撑）。
> 
> **核心理念**：分类分析不是只看准确率，而是要全面评估模型在每一类上的表现。没有混淆矩阵和 ROC 曲线的报告是不完整的。

---

## 评分维度与权重

| 评分维度 | 权重 | 说明 |
|---------|------|------|
| **代码正确性** | 30% | 代码能正确运行，无报错，逻辑清晰 |
| **指标计算准确性** | 25% | 混淆矩阵、精确率、查全率、F1、AUC 计算正确 |
| **模型评估完整性** | 25% | 包含完整的评估流程：混淆矩阵可视化、ROC 曲线、诊断结论 |
| **报告清晰度** | 20% | 报告结构清晰，指标解读到位，有业务洞察 |

---

## 等级划分

| 等级 | 分数范围 | 描述 |
|------|---------|------|
| **Excellent** | 90-100 | 全部完成，有深度分析，代码规范，报告专业 |
| **Good** | 75-89 | 基础+进阶完成，分析到位，代码可运行，报告完整 |
| **Pass** | 60-74 | 基础层完成，核心指标计算正确，有基本诊断 |
| **Needs Revision** | <60 | 基础层有缺失，需补充完善后重新提交 |

---

## 基础层评分细则（必做，满分 100）

### 任务 1.1：逻辑回归建模（30分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| 数据准备正确 | 8分 | 正确划分训练/测试集（分层抽样），特征标准化 | 代码审查 |
| 模型拟合正确 | 10分 | 使用 LogisticRegression 成功拟合，无报错 | pytest 通过 |
| 预测结果正确 | 7分 | 能输出预测概率和预测类别 | 代码审查 |
| 系数解读到位 | 5分 | 解释系数的正负含义，指出最重要的特征 | 报告审查 |

**关键检查点**：
- ✅ 使用 `stratify=y` 进行分层抽样
- ✅ 使用 `StandardScaler` 进行特征标准化
- ✅ 特征系数已输出并解读

---

### 任务 1.2：混淆矩阵与分类指标（35分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| 混淆矩阵计算正确 | 10分 | TP、FP、TN、FN 计算正确 | pytest 通过 |
| 混淆矩阵可视化 | 7分 | 使用热力图展示混淆矩阵 | 代码审查 |
| 指标计算正确 | 12分 | 准确率、精确率、查全率、F1 计算正确 | pytest 通过 |
| 指标解读到位 | 6分 | 对比准确率与 F1，分析精确率/查全率差异原因 | 报告审查 |

**关键检查点**：
- ✅ 混淆矩阵四个值正确提取（注意 sklearn 输出顺序：`[[TN, FP], [FN, TP]]`）
- ✅ 精确率 = TP / (TP + FP)
- ✅ 查全率 = TP / (TP + FN)
- ✅ F1 = 2 * (Precision * Recall) / (Precision + Recall)
- ⚠️ 类别不平衡时，F1 应明显低于准确率

---

### 任务 1.3：ROC 曲线与 AUC（35分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| ROC 曲线计算正确 | 10分 | 正确计算 FPR、TPR，使用预测概率而非类别 | pytest 通过 |
| ROC 曲线绘制 | 10分 | 正确绘制 ROC 曲线，标注对角线（随机分类器） | 代码审查 |
| AUC 计算正确 | 8分 | AUC 值计算正确 | pytest 通过 |
| AUC 解读与建议 | 7分 | 根据 AUC 范围给出模型表现评级和改进建议 | 报告审查 |

**关键检查点**：
- ✅ 使用 `predict_proba()` 获取预测概率，不是 `predict()`
- ✅ ROC 曲线经过 (0,0) 和 (1,1)，在对角线上方
- ✅ AUC 在 0.5（随机）到 1.0（完美）之间
- ✅ AUC 解读参考：
  - 0.5：随机猜测
  - 0.5-0.7：较差
  - 0.7-0.85：尚可
  - 0.85-0.95：良好
  - 0.95-1.0：优秀（需检查过拟合）

---

## 进阶层评分细则（选做，加分项）

### 任务 2.1：类别不平衡处理（+20分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| 不平衡识别 | 5分 | 正确计算正负样本比例，识别不平衡问题 | 代码审查 |
| 处理策略实现 | 8分 | 使用 class_weight='balanced' 或 SMOTE 处理 | 代码审查 |
| 对比分析 | 7分 | 对比处理前后的指标（特别是 F1 和查全率） | 报告审查 |

**关键检查点**：
- ✅ 仅在训练集上应用 SMOTE，测试集保持原始分布
- ✅ 类别不平衡时，F1 比准确率更能反映模型表现
- ✅ 处理后的查全率应有所提升（可能牺牲精确率）

---

### 任务 2.2：阈值优化 F1（+15分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| 多阈值计算 | 6分 | 遍历多个阈值，计算对应指标 | 代码审查 |
| 最优阈值找到 | 5分 | 找到 F1 最大的阈值 | 代码审查 |
| 业务场景讨论 | 4分 | 讨论不同业务目标下的阈值选择策略 | 报告审查 |

**关键检查点**：
- ✅ 阈值优化基于预测概率，不是重新训练模型
- ✅ 高查全率场景：选择较低阈值
- ✅ 高精确率场景：选择较高阈值

---

### 任务 2.3：对比不同模型的 AUC（+15分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| 多模型训练 | 6分 | 成功训练随机森林和决策树模型 | 代码审查 |
| ROC 对比图 | 5分 | 在同一张图上绘制多模型 ROC 曲线 | 代码审查 |
| 模型选择讨论 | 4分 | 讨论 AUC 对比结果和模型可解释性权衡 | 报告审查 |

**关键检查点**：
- ✅ 三个模型的 ROC 曲线颜色/线型区分清晰
- ✅ 标注每个模型的 AUC 值
- ✅ 讨论逻辑回归的可解释性优势（系数有明确含义）

---

## 挑战层评分细则（选做，加分项）

### 任务 3.1：AI 协作练习——审查 AI 生成分类报告（+20分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| 缺失指标识别 | 6分 | 列出至少 3 个缺失的关键指标 | 报告审查 |
| 逻辑问题分析 | 6分 | 分析"高准确率"在类别不平衡时的误导性 | 报告审查 |
| 报告重写 | 8分 | 重写报告，补充缺失指标和诊断结论 | 报告审查 |

**关键检查点**：
- ✅ 指出的缺失指标包括：混淆矩阵、精确率、查全率、F1、AUC 中的至少 3 个
- ✅ 解释准确率陷阱：类别不平衡时，全猜多数类也能获得高准确率
- ✅ 重写后的报告包含完整的分类评估要素

---

### 任务 3.2：公平性考量（+25分）

| 评分项 | 分值 | 评分标准 | 验证方式 |
|--------|------|---------|---------|
| 分组分析 | 8分 | 按性别/其他变量分组，分别计算指标 | 代码审查 |
| 公平性评估 | 7分 | 对比不同组的指标差异，识别潜在偏见 | 报告审查 |
| 公平性指标研究 | 5分 | 查阅资料，了解至少一种公平性指标 | 报告审查 |
| 改进建议 | 5分 | 给出实际项目中监控公平性的建议 | 报告审查 |

**关键检查点**：
- ✅ 分组后每组的样本量足够（避免样本过少导致指标不稳定）
- ✅ 公平性指标引用准确（Demographic Parity、Equal Opportunity、Equalized Odds 等）
- ✅ 意识到公平性与准确性之间的权衡关系

---

## 等级评定标准

### Excellent (90-100分)

- **基础层**：全部完成，指标计算准确，代码规范
- **进阶层**：至少完成 2 个进阶任务
- **挑战层**：至少完成 1 个挑战任务
- **报告**：结构清晰，有业务洞察，有对准确率陷阱的深刻理解
- **加分亮点**：
  - 使用 Bootstrap 估计 AUC 置信区间
  - 绘制 PR 曲线（Precision-Recall Curve）作为 ROC 的补充
  - 有对精确率/查全率权衡的深入讨论

### Good (75-89分)

- **基础层**：全部完成，指标计算正确
- **进阶层**：至少完成 1 个进阶任务
- **报告**：结构完整，有基本的指标解读
- **代码**：可运行，无明显错误

### Pass (60-74分)

- **基础层**：基本完成
- 混淆矩阵和分类指标计算正确
- ROC 曲线已绘制，AUC 已报告
- 报告包含基本要素

### Needs Revision (<60分)

以下情况需返工：
- ❌ 混淆矩阵计算错误（TP/FP/TN/FN 混淆）
- ❌ 指标公式使用错误（如用准确率代替 F1）
- ❌ ROC 曲线绘制错误（如使用预测类别而非概率）
- ❌ 报告缺少关键部分（无混淆矩阵或无 ROC 曲线）
- ❌ 代码无法运行或 pytest 不通过

---

## 常见扣分点

| 问题 | 扣分 | 说明 |
|------|------|------|
| 混淆矩阵理解错误 | -10分 | 将 TP/FP/TN/FN 位置搞混 |
| 未使用分层抽样 | -5分 | 未使用 `stratify=y`，可能导致类别分布不一致 |
| ROC 曲线使用预测类别 | -10分 | ROC 必须用预测概率，不能用类别 |
| 未进行特征标准化 | -5分 | 逻辑回归对特征尺度敏感，应标准化 |
| 类别不平衡未处理 | -5分 | 当比例 > 1:5 时，应使用 class_weight 或采样策略 |
| 报告缺少关键图表 | -10分 | 必须包含混淆矩阵热力图和 ROC 曲线 |
| 指标解读流于表面 | -5分 | 未分析精确率/查全率差异原因 |

---

## 快速评分检查清单

### 代码审查清单

- [ ] 逻辑回归模型正确拟合
- [ ] 使用 `train_test_split` 且 `stratify=y`
- [ ] 使用 `StandardScaler` 标准化
- [ ] 混淆矩阵计算正确（`confusion_matrix(y_test, y_pred)`）
- [ ] 四个指标计算正确（accuracy, precision, recall, f1）
- [ ] ROC 曲线使用 `predict_proba()` 的概率
- [ ] AUC 计算正确
- [ ] `pytest` 测试通过

### 报告审查清单

- [ ] 包含混淆矩阵表格（2×2）
- [ ] 包含分类指标表格（准确率、精确率、查全率、F1）
- [ ] 包含 AUC 值和解读
- [ ] 包含混淆矩阵热力图
- [ ] 包含 ROC 曲线图
- [ ] 有对模型表现的诊断结论
- [ ] （进阶）有类别不平衡处理对比
- [ ] （进阶）有阈值优化分析

---

## 参考：典型优秀作业特征

### 代码层面

```python
# 好的实践：完整的分类评估流程
def classification_report(y_true, y_pred, y_prob):
    """输出完整分类评估报告"""
    # 1. 混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    
    # 2. 指标
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred)
    }
    
    # 3. ROC
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc_score = auc(fpr, tpr)
    
    return cm, metrics, auc_score
```

### 报告层面

优秀报告的结构：
1. **数据概况**：样本量、特征数、类别分布
2. **模型配置**：算法、超参数、预处理步骤
3. **混淆矩阵**：表格 + 热力图可视化
4. **分类指标**：准确率、精确率、查全率、F1
5. **ROC 分析**：ROC 曲线图 + AUC 值 + 解读
6. **诊断结论**：模型优点、潜在问题、改进建议
7. **业务建议**：根据精确率/查全率权衡给出操作建议

---

## 备注

> **老潘的话**："评分不是目的，目的是让你养成完整的分类评估习惯。在公司里，提交一份只有准确率的分类报告，可能会被直接打回。混淆矩阵和 ROC 曲线是底线，这周的作业就是在训练这个习惯。"
