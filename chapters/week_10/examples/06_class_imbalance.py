"""
ç¤ºä¾‹ï¼šç±»åˆ«ä¸å¹³è¡¡â€”â€”è¯†åˆ«ã€åº”å¯¹ä¸è¯„ä¼°ã€‚

æœ¬ä¾‹æ¼”ç¤ºï¼š
1. ç±»åˆ«ä¸å¹³è¡¡çš„è¯†åˆ«ä¸å¯è§†åŒ–
2. class_weight='balanced' çš„ç”¨æ³•
3. è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰å’Œæ¬ é‡‡æ ·çš„åº”ç”¨
4. ä¸åŒç­–ç•¥çš„å¯¹æ¯”è¯„ä¼°
5. é€‚åˆç±»åˆ«ä¸å¹³è¡¡çš„è¯„ä¼°æŒ‡æ ‡

è¿è¡Œæ–¹å¼ï¼špython3 chapters/week_10/examples/06_class_imbalance.py
é¢„æœŸè¾“å‡ºï¼š
  - stdout è¾“å‡ºç±»åˆ«ä¸å¹³è¡¡æ£€æµ‹ç»“æœ
  - å¯¹æ¯”ä¸åŒå¤„ç†ç­–ç•¥çš„æ•ˆæœ
  - ä¿å­˜å›¾è¡¨åˆ° images/06_class_imbalance.png

æ ¸å¿ƒæ¦‚å¿µï¼š
  - ç±»åˆ«ä¸å¹³è¡¡æ¯”ä¾‹ > 1:10 æ—¶éœ€è¦ç‰¹æ®Šå¤„ç†
  - class_weight='balanced': è‡ªåŠ¨è°ƒæ•´ç±»åˆ«æƒé‡
  - SMOTE: åˆæˆå°‘æ•°ç±»æ ·æœ¬
  - è¯„ä¼°æŒ‡æ ‡: ä¸è¦ç”¨å‡†ç¡®ç‡ï¼Œç”¨ F1ã€AUC-PR ç­‰
"""
from __future__ import annotations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score,
    balanced_accuracy_score, average_precision_score, confusion_matrix,
    classification_report
)
from pathlib import Path


def setup_chinese_font() -> str:
    """é…ç½®ä¸­æ–‡å­—ä½“ï¼Œè¿”å›ä½¿ç”¨çš„å­—ä½“åç§°"""
    chinese_fonts = ['SimHei', 'Noto Sans CJK SC', 'Arial Unicode MS',
                     'PingFang SC', 'Microsoft YaHei']
    available = [f.name for f in fm.fontManager.ttflist]
    for font in chinese_fonts:
        if font in available:
            plt.rcParams['font.sans-serif'] = [font]
            plt.rcParams['axes.unicode_minus'] = False
            return font
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    return 'DejaVu Sans'


def generate_imbalanced_data(n: int = 2000, random_state: int = 42) -> pd.DataFrame:
    """
    ç”Ÿæˆä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®ï¼ˆæ­£ç±»çº¦å  5%ï¼‰ã€‚
    
    å‚æ•°:
        n: æ ·æœ¬é‡
        random_state: éšæœºç§å­
        
    è¿”å›:
        DataFrame åŒ…å«å®¢æˆ·ç‰¹å¾å’Œé«˜ä»·å€¼æ ‡ç­¾ï¼ˆå°‘æ•°ç±»ï¼‰
    """
    np.random.seed(random_state)
    
    data = pd.DataFrame({
        'æ³¨å†Œæœˆæ•°': np.random.randint(1, 60, n),
        'æœˆå‡æµè§ˆæ¬¡æ•°': np.random.poisson(25, n),
        'æœˆå‡æ¶ˆè´¹æ¬¡æ•°': np.random.poisson(2, n),
        'æœ€è¿‘ç™»å½•è·ä»Šå¤©æ•°': np.random.randint(1, 90, n)
    })
    
    # ç”Ÿæˆé«˜ä»·å€¼æ ‡ç­¾ï¼ˆå°‘æ•°ç±»ï¼Œçº¦ 5%ï¼‰
    score = (
        0.1 * data['æ³¨å†Œæœˆæ•°'] +
        0.05 * data['æœˆå‡æµè§ˆæ¬¡æ•°'] +
        0.5 * data['æœˆå‡æ¶ˆè´¹æ¬¡æ•°'] -
        0.03 * data['æœ€è¿‘ç™»å½•è·ä»Šå¤©æ•°'] -
        6 +
        np.random.normal(0, 1, n)
    )
    data['æ˜¯å¦é«˜ä»·å€¼'] = (score > np.percentile(score, 95)).astype(int)
    
    return data


def detect_imbalance(df: pd.DataFrame) -> dict:
    """
    æ£€æµ‹ç±»åˆ«ä¸å¹³è¡¡ã€‚
    
    è¿”å›:
        dict åŒ…å«ç±»åˆ«åˆ†å¸ƒä¿¡æ¯
    """
    print("=" * 70)
    print("ç±»åˆ«ä¸å¹³è¡¡æ£€æµ‹")
    print("=" * 70)
    
    y = df['æ˜¯å¦é«˜ä»·å€¼']
    class_counts = y.value_counts().sort_index()
    imbalance_ratio = class_counts.max() / class_counts.min()
    
    print(f"\nç±»åˆ«åˆ†å¸ƒï¼š")
    print(f"  ä½ä»·å€¼å®¢æˆ· (0): {class_counts[0]} ({class_counts[0]/len(y)*100:.2f}%)")
    print(f"  é«˜ä»·å€¼å®¢æˆ· (1): {class_counts[1]} ({class_counts[1]/len(y)*100:.2f}%)")
    print(f"\nä¸å¹³è¡¡æ¯”ä¾‹: 1:{imbalance_ratio:.1f}")
    
    print("\nä¸å¹³è¡¡ç¨‹åº¦åˆ¤æ–­ï¼š")
    if imbalance_ratio < 2:
        print("  âœ… ç±»åˆ«åŸºæœ¬å¹³è¡¡ï¼ˆ1:2 ä»¥å†…ï¼‰")
        severity = "balanced"
    elif imbalance_ratio < 5:
        print("  âš ï¸  è½»åº¦ä¸å¹³è¡¡ï¼ˆ1:2 åˆ° 1:5ï¼‰")
        severity = "mild"
    elif imbalance_ratio < 10:
        print("  âš ï¸  ä¸­åº¦ä¸å¹³è¡¡ï¼ˆ1:5 åˆ° 1:10ï¼‰")
        severity = "moderate"
    else:
        print("  ğŸš¨ ä¸¥é‡ä¸å¹³è¡¡ï¼ˆè¶…è¿‡ 1:10ï¼‰")
        severity = "severe"
    
    print("\nå‡†ç¡®ç‡é™·é˜±æ¼”ç¤ºï¼š")
    baseline_accuracy = class_counts[0] / len(y)
    print(f"  å¦‚æœå…¨éƒ¨é¢„æµ‹ä¸ºä½ä»·å€¼ï¼ˆå¤šæ•°ç±»ï¼‰ï¼š")
    print(f"  å‡†ç¡®ç‡ = {baseline_accuracy:.2%}")
    print(f"  ä½†æŸ¥å…¨ç‡ = 0%ï¼ˆæ‰€æœ‰é«˜ä»·å€¼å®¢æˆ·éƒ½è¢«æ¼æ‰ï¼ï¼‰")
    
    return {
        'class_counts': class_counts,
        'imbalance_ratio': imbalance_ratio,
        'severity': severity
    }


def baseline_model(X_train: np.ndarray, X_test: np.ndarray, 
                   y_train: pd.Series, y_test: pd.Series) -> dict:
    """
    åŸºå‡†æ¨¡å‹ï¼šä¸ä½¿ç”¨ä»»ä½•ä¸å¹³è¡¡å¤„ç†ã€‚
    """
    print("\n" + "=" * 70)
    print("ç­–ç•¥ 1ï¼šåŸºå‡†æ¨¡å‹ï¼ˆæ— å¤„ç†ï¼‰")
    print("=" * 70)
    
    model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    return evaluate_model(y_test, y_pred, y_prob, "åŸºå‡†æ¨¡å‹")


def balanced_class_weight(X_train: np.ndarray, X_test: np.ndarray,
                          y_train: pd.Series, y_test: pd.Series) -> dict:
    """
    ç­–ç•¥ 2ï¼šä½¿ç”¨ class_weight='balanced'ã€‚
    """
    print("\n" + "=" * 70)
    print("ç­–ç•¥ 2ï¼šclass_weight='balanced'")
    print("=" * 70)
    
    # è®¡ç®—æƒé‡
    from sklearn.utils.class_weight import compute_class_weight
    classes = np.unique(y_train)
    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
    
    print(f"\nè‡ªåŠ¨è®¡ç®—çš„ç±»åˆ«æƒé‡ï¼š")
    print(f"  ç±»åˆ« 0 (ä½ä»·å€¼): {weights[0]:.4f}")
    print(f"  ç±»åˆ« 1 (é«˜ä»·å€¼): {weights[1]:.4f}")
    print(f"\næƒé‡è®¡ç®—å…¬å¼ï¼šweight = n_samples / (n_classes * n_samples_in_class)")
    
    model = LogisticRegression(
        solver='lbfgs', 
        class_weight='balanced',
        max_iter=1000, 
        random_state=42
    )
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    return evaluate_model(y_test, y_pred, y_prob, "class_weight='balanced'")


def smote_oversampling(X_train: np.ndarray, X_test: np.ndarray,
                       y_train: pd.Series, y_test: pd.Series) -> dict:
    """
    ç­–ç•¥ 3ï¼šä½¿ç”¨ SMOTE è¿‡é‡‡æ ·ã€‚
    """
    print("\n" + "=" * 70)
    print("ç­–ç•¥ 3ï¼šSMOTE è¿‡é‡‡æ ·")
    print("=" * 70)
    
    try:
        from imblearn.over_sampling import SMOTE
        from collections import Counter
        
        print(f"\nåŸå§‹è®­ç»ƒé›†åˆ†å¸ƒ: {Counter(y_train)}")
        
        smote = SMOTE(random_state=42, k_neighbors=5)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
        
        print(f"SMOTE ååˆ†å¸ƒ: {Counter(y_train_resampled)}")
        
        model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)
        model.fit(X_train_resampled, y_train_resampled)
        
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        
        return evaluate_model(y_test, y_pred, y_prob, "SMOTE è¿‡é‡‡æ ·")
        
    except ImportError:
        print("\nâš ï¸  imbalanced-learn æœªå®‰è£…ï¼Œè·³è¿‡ SMOTE")
        print("   å®‰è£…å‘½ä»¤: pip install imbalanced-learn")
        return None


def undersampling(X_train: np.ndarray, X_test: np.ndarray,
                  y_train: pd.Series, y_test: pd.Series) -> dict:
    """
    ç­–ç•¥ 4ï¼šä½¿ç”¨éšæœºæ¬ é‡‡æ ·ã€‚
    """
    print("\n" + "=" * 70)
    print("ç­–ç•¥ 4ï¼šéšæœºæ¬ é‡‡æ ·")
    print("=" * 70)
    
    try:
        from imblearn.under_sampling import RandomUnderSampler
        from collections import Counter
        
        print(f"\nåŸå§‹è®­ç»ƒé›†åˆ†å¸ƒ: {Counter(y_train)}")
        
        undersampler = RandomUnderSampler(random_state=42)
        X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)
        
        print(f"æ¬ é‡‡æ ·ååˆ†å¸ƒ: {Counter(y_train_resampled)}")
        print(f"âš ï¸  æ³¨æ„ï¼šä¸¢å¤±äº† {len(y_train) - len(y_train_resampled)} ä¸ªå¤šæ•°ç±»æ ·æœ¬")
        
        model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)
        model.fit(X_train_resampled, y_train_resampled)
        
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        
        return evaluate_model(y_test, y_pred, y_prob, "éšæœºæ¬ é‡‡æ ·")
        
    except ImportError:
        print("\nâš ï¸  imbalanced-learn æœªå®‰è£…ï¼Œè·³è¿‡æ¬ é‡‡æ ·")
        return None


def threshold_tuning(X_train: np.ndarray, X_test: np.ndarray,
                     y_train: pd.Series, y_test: pd.Series) -> dict:
    """
    ç­–ç•¥ 5ï¼šé˜ˆå€¼è°ƒæ•´ï¼ˆä¸é‡æ–°è®­ç»ƒæ¨¡å‹ï¼‰ã€‚
    """
    print("\n" + "=" * 70)
    print("ç­–ç•¥ 5ï¼šé˜ˆå€¼è°ƒæ•´")
    print("=" * 70)
    
    model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)
    model.fit(X_train, y_train)
    
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ F1
    from sklearn.metrics import precision_recall_curve
    precision, recall, thresholds = precision_recall_curve(y_test, y_prob)
    
    # æ‰¾åˆ° F1 æœ€å¤§çš„é˜ˆå€¼
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
    optimal_idx = np.argmax(f1_scores[:-1])  # æ’é™¤æœ€åä¸€ä¸ª NaN
    optimal_threshold = thresholds[optimal_idx]
    
    print(f"\næœ€ä¼˜é˜ˆå€¼ (F1 æœ€å¤§): {optimal_threshold:.4f}")
    print(f"  è¯¥é˜ˆå€¼ä¸‹: Precision={precision[optimal_idx]:.2%}, Recall={recall[optimal_idx]:.2%}")
    
    # ä½¿ç”¨æ–°é˜ˆå€¼åšé¢„æµ‹
    y_pred_new = (y_prob >= optimal_threshold).astype(int)
    
    return evaluate_model(y_test, y_pred_new, y_prob, f"é˜ˆå€¼è°ƒæ•´ (t={optimal_threshold:.2f})")


def evaluate_model(y_test: pd.Series, y_pred: np.ndarray, y_prob: np.ndarray, 
                   method_name: str) -> dict:
    """
    è¯„ä¼°æ¨¡å‹è¡¨ç°ã€‚
    
    è¿”å›:
        dict åŒ…å«å„é¡¹æŒ‡æ ‡
    """
    accuracy = accuracy_score(y_test, y_pred)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc_pr = average_precision_score(y_test, y_prob)
    
    print(f"\n{method_name} è¯„ä¼°ç»“æœï¼š")
    print(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.2%}")
    print(f"  å¹³è¡¡å‡†ç¡®ç‡ (Balanced Accuracy): {balanced_acc:.2%}")
    print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.2%}")
    print(f"  æŸ¥å…¨ç‡ (Recall): {recall:.2%}")
    print(f"  F1 åˆ†æ•°: {f1:.2%}")
    print(f"  AUC-PR: {auc_pr:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    print(f"\n  æ··æ·†çŸ©é˜µ: TP={tp}, FP={fp}, TN={tn}, FN={fn}")
    
    return {
        'method': method_name,
        'accuracy': accuracy,
        'balanced_accuracy': balanced_acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc_pr': auc_pr,
        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn
    }


def compare_strategies(results: list) -> None:
    """
    å¯¹æ¯”ä¸åŒç­–ç•¥çš„æ•ˆæœã€‚
    """
    print("\n" + "=" * 70)
    print("ç­–ç•¥å¯¹æ¯”æ€»ç»“")
    print("=" * 70)
    
    # è¿‡æ»¤æ‰ None çš„ç»“æœ
    results = [r for r in results if r is not None]
    
    print(f"\n{'ç­–ç•¥':<25} {'å‡†ç¡®ç‡':<10} {'F1':<10} {'æŸ¥å…¨ç‡':<10} {'AUC-PR':<10}")
    print("-" * 75)
    
    for r in results:
        print(f"{r['method']:<25} {r['accuracy']:<10.2%} {r['f1']:<10.2%} {r['recall']:<10.2%} {r['auc_pr']:<10.4f}")
    
    # æ‰¾å‡º F1 æœ€é«˜çš„ç­–ç•¥
    best_f1 = max(results, key=lambda x: x['f1'])
    print(f"\nâœ… F1 æœ€é«˜çš„ç­–ç•¥: {best_f1['method']} (F1 = {best_f1['f1']:.2%})")
    
    # æ‰¾å‡ºæŸ¥å…¨ç‡æœ€é«˜çš„ç­–ç•¥
    best_recall = max(results, key=lambda x: x['recall'])
    print(f"âœ… æŸ¥å…¨ç‡æœ€é«˜çš„ç­–ç•¥: {best_recall['method']} (Recall = {best_recall['recall']:.2%})")


def plot_comparison(results: list) -> None:
    """ç»˜åˆ¶ç­–ç•¥å¯¹æ¯”å›¾"""
    setup_chinese_font()
    
    # è¿‡æ»¤æ‰ None çš„ç»“æœ
    results = [r for r in results if r is not None]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # å·¦å›¾ï¼šæŒ‡æ ‡å¯¹æ¯”
    ax1 = axes[0]
    methods = [r['method'] for r in results]
    x = np.arange(len(methods))
    width = 0.2
    
    metrics = ['precision', 'recall', 'f1']
    colors = ['#2E86AB', '#F18F01', '#C73E1D']
    labels = ['ç²¾ç¡®ç‡', 'æŸ¥å…¨ç‡', 'F1']
    
    for i, (metric, color, label) in enumerate(zip(metrics, colors, labels)):
        values = [r[metric] for r in results]
        ax1.bar(x + i * width, values, width, label=label, color=color, edgecolor='black')
    
    ax1.set_xlabel('ç­–ç•¥', fontsize=12)
    ax1.set_ylabel('åˆ†æ•°', fontsize=12)
    ax1.set_title('ä¸åŒç­–ç•¥çš„æŒ‡æ ‡å¯¹æ¯”', fontsize=13, fontweight='bold')
    ax1.set_xticks(x + width)
    ax1.set_xticklabels(methods, rotation=15, ha='right')
    ax1.legend(fontsize=10)
    ax1.set_ylim(0, 1)
    ax1.grid(True, alpha=0.3, axis='y')
    
    # å³å›¾ï¼šæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ï¼ˆä»¥æœ€ä½³ F1 ç­–ç•¥ä¸ºä¾‹ï¼‰
    ax2 = axes[1]
    best_result = max(results, key=lambda x: x['f1'])
    
    cm = np.array([[best_result['tn'], best_result['fp']],
                   [best_result['fn'], best_result['tp']]])
    
    im = ax2.imshow(cm, interpolation='nearest', cmap='Blues')
    ax2.set_title(f'æœ€ä½³ç­–ç•¥æ··æ·†çŸ©é˜µ\n{best_result["method"]}', fontsize=13, fontweight='bold')
    
    tick_marks = [0, 1]
    ax2.set_xticks(tick_marks)
    ax2.set_yticks(tick_marks)
    ax2.set_xticklabels(['ä½ä»·å€¼', 'é«˜ä»·å€¼'])
    ax2.set_yticklabels(['ä½ä»·å€¼', 'é«˜ä»·å€¼'])
    ax2.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    ax2.set_ylabel('å®é™…ç±»åˆ«', fontsize=12)
    
    thresh = cm.max() / 2.
    for i in range(2):
        for j in range(2):
            ax2.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black",
                    fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    output_dir = Path(__file__).parent.parent / 'images'
    output_dir.mkdir(exist_ok=True)
    plt.savefig(output_dir / '06_class_imbalance.png',
                dpi=150, bbox_inches='tight',
                facecolor='white', edgecolor='none')
    plt.close()
    
    print(f"\nå›¾è¡¨å·²ä¿å­˜åˆ°: images/06_class_imbalance.png")


def main() -> None:
    """ä¸»å‡½æ•°"""
    print("ç±»åˆ«ä¸å¹³è¡¡ï¼šè¯†åˆ«ã€åº”å¯¹ä¸è¯„ä¼°\n")
    
    # ç”Ÿæˆä¸å¹³è¡¡æ•°æ®
    df = generate_imbalanced_data(n=2000, random_state=42)
    
    # æ£€æµ‹ä¸å¹³è¡¡
    imbalance_info = detect_imbalance(df)
    
    # å‡†å¤‡æ•°æ®
    X = df[['æ³¨å†Œæœˆæ•°', 'æœˆå‡æµè§ˆæ¬¡æ•°', 'æœˆå‡æ¶ˆè´¹æ¬¡æ•°', 'æœ€è¿‘ç™»å½•è·ä»Šå¤©æ•°']]
    y = df['æ˜¯å¦é«˜ä»·å€¼']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ç‰¹å¾ç¼©æ”¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # æµ‹è¯•ä¸åŒç­–ç•¥
    results = []
    
    # ç­–ç•¥ 1ï¼šåŸºå‡†
    results.append(baseline_model(X_train_scaled, X_test_scaled, y_train, y_test))
    
    # ç­–ç•¥ 2ï¼šclass_weight='balanced'
    results.append(balanced_class_weight(X_train_scaled, X_test_scaled, y_train, y_test))
    
    # ç­–ç•¥ 3ï¼šSMOTE
    smote_result = smote_oversampling(X_train_scaled, X_test_scaled, y_train, y_test)
    if smote_result:
        results.append(smote_result)
    
    # ç­–ç•¥ 4ï¼šæ¬ é‡‡æ ·
    under_result = undersampling(X_train_scaled, X_test_scaled, y_train, y_test)
    if under_result:
        results.append(under_result)
    
    # ç­–ç•¥ 5ï¼šé˜ˆå€¼è°ƒæ•´
    results.append(threshold_tuning(X_train_scaled, X_test_scaled, y_train, y_test))
    
    # å¯¹æ¯”ç­–ç•¥
    compare_strategies(results)
    
    # ç»˜å›¾
    plot_comparison(results)
    
    print("\n" + "=" * 70)
    print("æ€»ç»“")
    print("=" * 70)
    print("\nç±»åˆ«ä¸å¹³è¡¡çš„è¯†åˆ«ï¼š")
    print("  - ä¸å¹³è¡¡æ¯”ä¾‹ > 1:10 æ—¶éœ€è¦ç‰¹æ®Šå¤„ç†")
    print("  - å‡†ç¡®ç‡æ˜¯è¯¯å¯¼æ€§æŒ‡æ ‡ï¼ˆå…¨çŒœå¤šæ•°ç±»ä¹Ÿèƒ½æœ‰é«˜å‡†ç¡®ç‡ï¼‰")
    print("\nåº”å¯¹ç­–ç•¥ï¼š")
    print("  1. class_weight='balanced': æœ€ç®€å•ï¼Œé€šå¸¸æ•ˆæœä¹Ÿä¸é”™")
    print("  2. SMOTE: ç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œå¢åŠ å°‘æ•°ç±»")
    print("  3. æ¬ é‡‡æ ·: å‡å°‘å¤šæ•°ç±»ï¼Œå¯èƒ½ä¸¢å¤±ä¿¡æ¯")
    print("  4. é˜ˆå€¼è°ƒæ•´: ä¸æ”¹å˜æ¨¡å‹ï¼Œåªè°ƒæ•´å†³ç­–é˜ˆå€¼")
    print("\nè¯„ä¼°æŒ‡æ ‡é€‰æ‹©ï¼š")
    print("  - ä¸è¦ç”¨å‡†ç¡®ç‡ï¼")
    print("  - ä¼˜å…ˆä½¿ç”¨ F1ã€AUC-PRã€å¹³è¡¡å‡†ç¡®ç‡")
    print("  - å…³æ³¨å°‘æ•°ç±»çš„æŸ¥å…¨ç‡")
    print("\nâš ï¸  é‡è¦ï¼šSMOTE/æ¬ é‡‡æ ·åªèƒ½åœ¨è®­ç»ƒé›†ä¸Šä½¿ç”¨ï¼Œæµ‹è¯•é›†å¿…é¡»ä¿æŒåŸå§‹åˆ†å¸ƒï¼")
    print()


if __name__ == "__main__":
    main()
