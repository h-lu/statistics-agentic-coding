"""
StatLab åˆ†ç±»è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨

æœ¬è„šæœ¬æ˜¯ StatLab è¶…çº§çº¿çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºåœ¨å¯å¤ç°åˆ†ææŠ¥å‘Šä¸­æ·»åŠ 
"åˆ†ç±»è¯„ä¼°"ç« èŠ‚ã€‚å®ƒæ‰§è¡Œå®Œæ•´çš„åˆ†ç±»åˆ†ææµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- é€»è¾‘å›å½’å»ºæ¨¡ï¼ˆç³»æ•°è§£é‡Š + ä¼˜åŠ¿æ¯”ï¼‰
- æ··æ·†çŸ©é˜µä¸åˆ†ç±»æŒ‡æ ‡ï¼ˆç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ï¼‰
- ROC-AUC åˆ†æï¼ˆé˜ˆå€¼æ— å…³è¯„ä¼°ï¼‰
- K-fold åˆ†å±‚äº¤å‰éªŒè¯
- åŸºçº¿å¯¹æ¯”ï¼ˆå¤šæ•°ç±»åˆ†ç±»å™¨ï¼‰
- è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šç‰‡æ®µå’Œå›¾è¡¨

è¿è¡Œæ–¹å¼ï¼špython3 chapters/week_10/examples/99_statlab.py
é¢„æœŸè¾“å‡ºï¼š
- æŠ¥å‘Šç‰‡æ®µï¼ˆè¿½åŠ åˆ° report.mdï¼‰
- ROC æ›²çº¿å›¾ï¼ˆä¿å­˜åˆ° report/images/ï¼‰

ä¾èµ–: éœ€è¦é¢„å…ˆæ¸…æ´—å¥½çš„æ•°æ®ï¼ˆå‡è®¾è·¯å¾„ä¸º data/clean_data.csvï¼‰

è¯´æ˜ï¼šæœ¬è„šæœ¬åœ¨ä¸Šå‘¨ï¼ˆå›å½’åˆ†æï¼‰åŸºç¡€ä¸Šå¢é‡ä¿®æ”¹ï¼Œå°†åˆ†æç›®æ ‡ä»
"è¿ç»­ç›®æ ‡é¢„æµ‹"æ‰©å±•åˆ°"äºŒåˆ†ç±»ç›®æ ‡é¢„æµ‹"ã€‚
"""
from __future__ import annotations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import (
    train_test_split, StratifiedKFold,
    cross_validate, cross_val_score
)
from sklearn.metrics import (
    confusion_matrix, classification_report,
    roc_auc_score, roc_curve, precision_recall_curve,
    accuracy_score, precision_score, recall_score, f1_score
)
from sklearn.dummy import DummyClassifier

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def classification_evaluation_to_report(
    df: pd.DataFrame,
    target: str,
    numeric_features: List[str],
    categorical_features: List[str],
    output_dir: str = "report"
) -> str:
    """
    å¯¹æ•°æ®é›†è¿›è¡Œå®Œæ•´çš„åˆ†ç±»è¯„ä¼°ï¼Œç”ŸæˆæŠ¥å‘Šç‰‡æ®µ

    å‚æ•°:
        df: æ¸…æ´—åçš„æ•°æ®
        target: ç›®æ ‡å˜é‡åï¼ˆå¦‚ 'purchase', 'churn'ï¼‰
        numeric_features: æ•°å€¼ç‰¹å¾åˆ—è¡¨
        categorical_features: ç±»åˆ«ç‰¹å¾åˆ—è¡¨
        output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•

    è¿”å›:
        Markdown æ ¼å¼çš„æŠ¥å‘Šç‰‡æ®µ
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    images_path = output_path / "images"
    images_path.mkdir(parents=True, exist_ok=True)

    # å‡†å¤‡æ•°æ®
    X = df[numeric_features + categorical_features]
    y = df[target]

    print("=" * 70)
    print("StatLab åˆ†ç±»è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨")
    print("=" * 70)
    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(y)}")
    print(f"  ç›®æ ‡å˜é‡: {target}")
    print(f"  {target}=1 çš„æ¯”ä¾‹: {y.mean():.2%}")
    print(f"  æ•°å€¼ç‰¹å¾: {', '.join(numeric_features)}")
    print(f"  ç±»åˆ«ç‰¹å¾: {', '.join(categorical_features)}")

    # ========== 1. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† ==========
    print(f"\nâœ… æ­¥éª¤ 1: åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

    # ========== 2. æ„å»º Pipeline ==========
    print(f"\nâœ… æ­¥éª¤ 2: æ„å»º Pipeline...")

    # æ•°å€¼ç‰¹å¾é¢„å¤„ç†
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    # ç±»åˆ«ç‰¹å¾é¢„å¤„ç†
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    # ColumnTransformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'
    )

    # å®Œæ•´ Pipeline
    full_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])

    print(f"  Pipeline: ColumnTransformer -> LogisticRegression")

    # ========== 3. æ‹Ÿåˆæ¨¡å‹ ==========
    print(f"\nâœ… æ­¥éª¤ 3: æ‹Ÿåˆé€»è¾‘å›å½’æ¨¡å‹...")
    full_pipeline.fit(X_train, y_train)

    # é¢„æµ‹
    y_pred = full_pipeline.predict(X_test)
    y_proba = full_pipeline.predict_proba(X_test)[:, 1]

    print(f"  âœ… æ¨¡å‹æ‹Ÿåˆå®Œæˆ")

    # ========== 4. æ··æ·†çŸ©é˜µä¸è¯„ä¼°æŒ‡æ ‡ ==========
    print(f"\nâœ… æ­¥éª¤ 4: è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")

    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_proba)

    print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
    print(f"  ç²¾ç¡®ç‡: {precision:.3f}")
    print(f"  å¬å›ç‡: {recall:.3f}")
    print(f"  F1 åˆ†æ•°: {f1:.3f}")
    print(f"  AUC: {auc:.3f}")

    # ========== 5. ROC æ›²çº¿ ==========
    print(f"\nâœ… æ­¥éª¤ 5: ç»˜åˆ¶ ROC æ›²çº¿...")

    fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba)

    plt.figure(figsize=(10, 6))
    plt.plot(fpr, tpr, linewidth=2, label=f'ROC æ›²çº¿ (AUC = {auc:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='éšæœºçŒœæµ‹ (AUC = 0.5)')
    plt.xlabel('å‡é˜³æ€§ç‡ (FPR)', fontsize=12)
    plt.ylabel('çœŸé˜³æ€§ç‡ (TPR / Recall)', fontsize=12)
    plt.title(f'ROC æ›²çº¿ - {target} é¢„æµ‹', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    roc_fig_path = images_path / "roc_curve.png"
    plt.savefig(roc_fig_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ… ROC æ›²çº¿å·²ä¿å­˜: {roc_fig_path}")

    # ========== 6. K-fold äº¤å‰éªŒè¯ ==========
    print(f"\nâœ… æ­¥éª¤ 6: è¿è¡Œ 5-fold äº¤å‰éªŒè¯...")

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    cv_results = cross_validate(
        full_pipeline, X, y,
        cv=skf,
        scoring={
            'accuracy': 'accuracy',
            'f1': 'f1',
            'roc_auc': 'roc_auc',
            'recall': 'recall'
        },
        return_train_score=False
    )

    cv_accuracy = cv_results['test_accuracy']
    cv_f1 = cv_results['test_f1']
    cv_auc = cv_results['test_roc_auc']
    cv_recall = cv_results['test_recall']

    print(f"  å‡†ç¡®ç‡: {cv_accuracy.mean():.3f} Â± {cv_accuracy.std():.3f}")
    print(f"  F1 åˆ†æ•°: {cv_f1.mean():.3f} Â± {cv_f1.std():.3f}")
    print(f"  AUC: {cv_auc.mean():.3f} Â± {cv_auc.std():.3f}")
    print(f"  å¬å›ç‡: {cv_recall.mean():.3f} Â± {cv_recall.std():.3f}")

    # ========== 7. åŸºçº¿å¯¹æ¯” ==========
    print(f"\nâœ… æ­¥éª¤ 7: ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”...")

    dummy = DummyClassifier(strategy='most_frequent', random_state=42)
    dummy.fit(X_train, y_train)
    y_pred_dummy = dummy.predict(X_test)
    y_proba_dummy = dummy.predict_proba(X_test)[:, 1]

    dummy_acc = accuracy_score(y_test, y_pred_dummy)
    dummy_recall = recall_score(y_test, y_pred_dummy, zero_division=0)
    dummy_auc = roc_auc_score(y_test, y_proba_dummy)

    print(f"  åŸºçº¿å‡†ç¡®ç‡: {dummy_acc:.3f}")
    print(f"  åŸºçº¿å¬å›ç‡: {dummy_recall:.3f}")
    print(f"  åŸºçº¿ AUC: {dummy_auc:.3f}")

    # ========== 8. æå–ç³»æ•°è¡¨ ==========
    print(f"\nâœ… æ­¥éª¤ 8: æå–æ¨¡å‹ç³»æ•°...")

    # è·å–ç‰¹å¾åï¼ˆOne-Hot ç¼–ç åï¼‰
    feature_names = numeric_features + list(
        full_pipeline.named_steps['preprocessor']
        .named_transformers_['cat']
        .named_steps['onehot']
        .get_feature_names_out(categorical_features)
    )

    # è·å–ç³»æ•°
    coefs = full_pipeline.named_steps['classifier'].coef_[0]

    # è®¡ç®—ä¼˜åŠ¿æ¯”
    odds_ratios = np.exp(coefs)

    # åˆ›å»ºç³»æ•°è¡¨
    coef_df = pd.DataFrame({
        'ç‰¹å¾': feature_names,
        'ç³»æ•°': coefs,
        'ä¼˜åŠ¿æ¯” (OR)': odds_ratios,
        '|ç³»æ•°|': np.abs(coefs)
    }).sort_values('|ç³»æ•°|', ascending=False)

    print(f"  âœ… æå–äº† {len(coef_df)} ä¸ªç‰¹å¾çš„ç³»æ•°")

    # ========== 9. ç”ŸæˆæŠ¥å‘Šç‰‡æ®µ ==========
    print(f"\nâœ… æ­¥éª¤ 9: ç”ŸæˆæŠ¥å‘Šç‰‡æ®µ...")

    report = generate_report_markdown(
        target=target,
        numeric_features=numeric_features,
        categorical_features=categorical_features,
        coef_df=coef_df,
        cm=cm,
        tn=tn, fp=fp, fn=fn, tp=tp,
        accuracy=accuracy, precision=precision, recall=recall, f1=f1, auc=auc,
        cv_accuracy=cv_accuracy, cv_f1=cv_f1, cv_auc=cv_auc, cv_recall=cv_recall,
        dummy_acc=dummy_acc, dummy_recall=dummy_recall,
        n_total=len(y),
        pos_ratio=y.mean(),
        roc_fig_path=roc_fig_path
    )

    print(f"  âœ… æŠ¥å‘Šç‰‡æ®µç”Ÿæˆå®Œæˆ")

    print("\n" + "=" * 70)
    print("âœ… åˆ†ç±»è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)

    return report


def generate_report_markdown(
    target: str,
    numeric_features: List[str],
    categorical_features: List[str],
    coef_df: pd.DataFrame,
    cm: np.ndarray,
    tn: int, fp: int, fn: int, tp: int,
    accuracy: float, precision: float, recall: float, f1: float, auc: float,
    cv_accuracy: np.ndarray, cv_f1: np.ndarray, cv_auc: np.ndarray, cv_recall: np.ndarray,
    dummy_acc: float, dummy_recall: float,
    n_total: int,
    pos_ratio: float,
    roc_fig_path: Path
) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Šç‰‡æ®µ"""

    # å‰ 10 ä¸ªé‡è¦ç‰¹å¾
    top_features = coef_df.head(10)

    # ç³»æ•°è¡¨ Markdown
    coef_table = ""
    for _, row in top_features.iterrows():
        coef_table += f"- **{row['ç‰¹å¾']}**: ç³»æ•° = {row['ç³»æ•°']:.3f}, ä¼˜åŠ¿æ¯” (OR) = {row['ä¼˜åŠ¿æ¯” (OR)']:.3f}\n"

    # AUC åˆ¤æ–­
    if auc > 0.8:
        auc_strength = "å¼º"
    elif auc > 0.7:
        auc_strength = "ä¸­ç­‰"
    elif auc > 0.6:
        auc_strength = "å¼±"
    else:
        auc_strength = "å¾ˆå·®"

    report = f"""
## åˆ†ç±»è¯„ä¼°

### ç ”ç©¶é—®é¢˜

å“ªäº›å› ç´ å½±å“ **{target}**ï¼ˆäºŒåˆ†ç±»ç›®æ ‡ï¼‰ï¼Ÿ

æœ¬èŠ‚ä½¿ç”¨é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰å»ºæ¨¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ {target}=1ï¼ˆå¦‚"è´­ä¹°"/"æµå¤±"ï¼‰çš„æ¦‚ç‡ï¼Œå¹¶è¯„ä¼°æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚

### æ¨¡å‹è®¾ç½®

**ç®—æ³•**: é€»è¾‘å›å½’ (Logistic Regression)

**ç‰¹å¾**:
- æ•°å€¼ç‰¹å¾: {', '.join(numeric_features)}
- ç±»åˆ«ç‰¹å¾: {', '.join(categorical_features)}

**é¢„å¤„ç†**:
- æ•°å€¼ç‰¹å¾: ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼ + æ ‡å‡†åŒ– (StandardScaler)
- ç±»åˆ«ç‰¹å¾: ä¼—æ•°å¡«å……ç¼ºå¤±å€¼ + One-Hot ç¼–ç  (OneHotEncoder)

**è¯„ä¼°æ–¹æ³•**: 5-fold åˆ†å±‚äº¤å‰éªŒè¯ (StratifiedKFold)

### é€»è¾‘å›å½’ç³»æ•°ä¸ä¼˜åŠ¿æ¯”

é€»è¾‘å›å½’çš„ç³»æ•°è¡¨ç¤º"å¯¹æ•°ä¼˜åŠ¿æ¯”"ï¼ˆlog-odds ratioï¼‰çš„å˜åŒ–ã€‚ä¸ºäº†æ›´ç›´è§‚åœ°è§£é‡Šï¼Œæˆ‘ä»¬è®¡ç®—**ä¼˜åŠ¿æ¯” (Odds Ratio, OR)**ï¼š

**ä¼˜åŠ¿æ¯” (OR) = exp(ç³»æ•°)**

- OR > 1: è¯¥ç‰¹å¾å¢åŠ ä¼šæé«˜ {target}=1 çš„ä¼˜åŠ¿
- OR < 1: è¯¥ç‰¹å¾å¢åŠ ä¼šé™ä½ {target}=1 çš„ä¼˜åŠ¿
- OR = 1: è¯¥ç‰¹å¾å¯¹ {target} æ— å½±å“

**å‰ 10 ä¸ªé‡è¦ç‰¹å¾**:

{coef_table}

**è§£é‡Šç¤ºä¾‹**:
- å¦‚æœæŸç‰¹å¾çš„ä¼˜åŠ¿æ¯” OR = 1.5ï¼Œè¯´æ˜è¯¥ç‰¹å¾æ¯å¢åŠ  1 å•ä½ï¼Œ{target}=1 çš„ä¼˜åŠ¿å¢åŠ åˆ°åŸæ¥çš„ 1.5 å€ï¼ˆå¢åŠ  50%ï¼‰
- å¦‚æœæŸç‰¹å¾çš„ä¼˜åŠ¿æ¯” OR = 0.8ï¼Œè¯´æ˜è¯¥ç‰¹å¾æ¯å¢åŠ  1 å•ä½ï¼Œ{target}=1 çš„ä¼˜åŠ¿é™ä½åˆ°åŸæ¥çš„ 0.8 å€ï¼ˆé™ä½ 20%ï¼‰

### æ··æ·†çŸ©é˜µä¸è¯„ä¼°æŒ‡æ ‡

**æ··æ·†çŸ©é˜µ** (Threshold = 0.5):

| | é¢„æµ‹ {target}=0 | é¢„æµ‹ {target}=1 |
|---|---|---|
| **å®é™… {target}=0** | {tn} (çœŸé˜´æ€§ TN) | {fp} (å‡é˜³æ€§ FP) |
| **å®é™… {target}=1** | {fn} (å‡é˜´æ€§ FN) | {tp} (çœŸé˜³æ€§ TP) |

**è¯„ä¼°æŒ‡æ ‡**:

| æŒ‡æ ‡ | å…¬å¼ | å€¼ | å«ä¹‰ |
|------|------|-----|------|
| **å‡†ç¡®ç‡ (Accuracy)** | (TP + TN) / æ€»æ ·æœ¬ | {accuracy:.2%} | æ‰€æœ‰é¢„æµ‹ä¸­ï¼Œé¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ |
| **ç²¾ç¡®ç‡ (Precision)** | TP / (TP + FP) | {precision:.2%} | é¢„æµ‹ä¸º {target}=1 çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£ä¸º 1 çš„æ¯”ä¾‹ |
| **å¬å›ç‡ (Recall)** | TP / (TP + FN) | {recall:.2%} | çœŸå®ä¸º {target}=1 çš„æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹ |
| **F1 åˆ†æ•°** | 2 Ã— (Prec Ã— Rec) / (Prec + Rec) | {f1:.3f} | ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•° |

**ä¸šåŠ¡è§£é‡Š**:

- **å‡é˜³æ€§æˆæœ¬ï¼ˆè¯¯æŠ¥ï¼‰**: {fp} ä¸ªæ ·æœ¬è¢«é”™è¯¯é¢„æµ‹ä¸º {target}=1ï¼Œå¯èƒ½æµªè´¹è¥é”€/è¿è¥èµ„æº
- **å‡é˜´æ€§æˆæœ¬ï¼ˆæ¼æŠ¥ï¼‰**: {fn} ä¸ªçœŸå® {target}=1 çš„æ ·æœ¬è¢«é—æ¼ï¼Œå¯èƒ½é€ æˆä¸šåŠ¡æŸå¤±ï¼ˆå¦‚æµå¤±å®¢æˆ·ã€æœªæˆäº¤è®¢å•ï¼‰
- **æ¨¡å‹ä»·å€¼**: æœ¬æ¨¡å‹çš„å¬å›ç‡ä¸º {recall:.1%}ï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹ï¼ˆå¬å›ç‡ {dummy_recall:.1%}ï¼‰æœ‰æ˜¾è‘—æå‡

### ROC-AUC åˆ†æ

**AUCï¼ˆROC æ›²çº¿ä¸‹é¢ç§¯ï¼‰**: {auc:.3f}

AUC è¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿæ ·æœ¬çš„èƒ½åŠ›ï¼Œä¸ä¾èµ–åˆ†ç±»é˜ˆå€¼ï¼š

- **AUC = 1.0**: å®Œç¾åˆ†ç±»å™¨
- **AUC = 0.5**: éšæœºçŒœæµ‹ï¼ˆåƒæŠ›ç¡¬å¸ï¼‰
- **æœ¬æ¨¡å‹ AUC = {auc:.3f}**: {auc_strength}åŒºåˆ†èƒ½åŠ›

**ç›´è§‚è§£é‡Š**:
AUC = {auc:.3f} çš„å«ä¹‰æ˜¯ï¼šå¦‚æœä½ éšæœºé€‰ä¸€ä¸ª {target}=1 çš„æ ·æœ¬å’Œä¸€ä¸ª {target}=0 çš„æ ·æœ¬ï¼Œæ¨¡å‹ç»™ {target}=1 çš„æ ·æœ¬æ›´é«˜æ¦‚ç‡çš„æ¦‚ç‡æ˜¯ {auc:.1%}ã€‚

![ROC æ›²çº¿](images/roc_curve.png)

### äº¤å‰éªŒè¯ç»“æœ

5-fold åˆ†å±‚äº¤å‰éªŒè¯ï¼ˆStratifiedKFoldï¼‰ç»“æœ:

| æŒ‡æ ‡ | å‡å€¼ Â± æ ‡å‡†å·® | è¯´æ˜ |
|------|---------------|------|
| **å‡†ç¡®ç‡** | {cv_accuracy.mean():.3f} Â± {cv_accuracy.std():.3f} | æ•´ä½“æ­£ç¡®ç‡ |
| **F1 åˆ†æ•°** | {cv_f1.mean():.3f} Â± {cv_f1.std():.3f} | ç²¾ç¡®ç‡ä¸å¬å›ç‡çš„å¹³è¡¡ |
| **AUC** | {cv_auc.mean():.3f} Â± {cv_auc.std():.3f} | åŒºåˆ†èƒ½åŠ› |
| **å¬å›ç‡** | {cv_recall.mean():.3f} Â± {cv_recall.std():.3f} | æ•è· {target}=1 çš„èƒ½åŠ› |

**ç¨³å®šæ€§è¯„ä¼°**:
- æ ‡å‡†å·®è¾ƒå°ï¼ˆ< 0.05ï¼‰ï¼Œè¯´æ˜æ¨¡å‹å¯¹ä¸åŒæ•°æ®åˆ’åˆ†ç¨³å¥
- å¦‚æœæ ‡å‡†å·®å¾ˆå¤§ï¼ˆ> 0.10ï¼‰ï¼Œè¯´æ˜æ¨¡å‹å¯¹æ•°æ®åˆ’åˆ†æ•æ„Ÿï¼Œéœ€è¦æ›´å¤šæ•°æ®æˆ–æ›´ç®€å•çš„æ¨¡å‹

### åŸºçº¿å¯¹æ¯”

ä¸**å¤šæ•°ç±»åŸºçº¿**ï¼ˆDummyClassifierï¼šæ€»æ˜¯é¢„æµ‹å‡ºç°æœ€å¤šçš„ç±»åˆ«ï¼‰å¯¹æ¯”:

| æ¨¡å‹ | å‡†ç¡®ç‡ | å¬å›ç‡ | AUC |
|------|---------|--------|-----|
| **å¤šæ•°ç±»åŸºçº¿** | {dummy_acc:.2%} | {dummy_recall:.2%} | 0.500 |
| **é€»è¾‘å›å½’** | {accuracy:.2%} | {recall:.2%} | {auc:.3f} |
| **æ”¹è¿›** | {(accuracy - dummy_acc):.1%} | {(recall - dummy_recall):.1%} | {(auc - 0.5):.3f} |

**ç»“è®º**:

- æœ¬æ¨¡å‹çš„å‡†ç¡®ç‡ä¸åŸºçº¿{"ç›¸å½“" if accuracy < dummy_acc * 1.05 else "ç•¥é«˜"}
- ä½†å¬å›ç‡ä»åŸºçº¿çš„ {dummy_recall:.1%} æå‡åˆ° {recall:.1%}ï¼Œ{"æœ‰æ˜¾è‘—æ”¹è¿›" if recall > dummy_recall * 1.5 else "æœ‰æ‰€æ”¹è¿›"}
- AUC = {auc:.3f} ({"å¼º" if auc > 0.8 else "ä¸­ç­‰" if auc > 0.7 else "å¼±"})åŒºåˆ†èƒ½åŠ›ï¼Œæ¨¡å‹æœ‰æ•ˆ

### å·¥ç¨‹å®è·µï¼šé˜²æ­¢æ•°æ®æ³„æ¼

æœ¬åˆ†æä½¿ç”¨ **Pipeline + ColumnTransformer** æ¨¡å¼ï¼š

```python
Pipeline(steps=[
    ('preprocessor', ColumnTransformer(
        transformers=[
            ('num', Pipeline([...]), numeric_features),
            ('cat', Pipeline([...]), categorical_features)
        ]
    )),
    ('classifier', LogisticRegression(...))
])
```

**å…³é”®å®è·µ**:

1. **æ‰€æœ‰é¢„å¤„ç†åœ¨ Pipeline å†…å®Œæˆ**:
   - æ ‡å‡†åŒ–ã€One-Hot ç¼–ç ã€ç¼ºå¤±å€¼å¡«å……éƒ½åœ¨ Pipeline å†…éƒ¨
   - äº¤å‰éªŒè¯æ—¶ï¼Œæ¯ä¸ªæŠ˜ç‹¬ç«‹æ‹Ÿåˆé¢„å¤„ç†å‚æ•°ï¼ˆå¦‚å‡å€¼ã€æ–¹å·®ï¼‰

2. **ç¡®ä¿æµ‹è¯•é›†ä¿¡æ¯ä¸ä¼šæ³„æ¼**:
   - æµ‹è¯•é›†åªç”¨äº `transform`ï¼Œä¸ç”¨äº `fit`
   - æ¯ä¸ªæŠ˜çš„è®­ç»ƒé›†ä¸ä¼š"çœ‹åˆ°"å…¶ä»–æŠ˜çš„ç»Ÿè®¡é‡

3. **å¯å¤ç°æ€§**:
   - å›ºå®šéšæœºç§å­ (`random_state=42`)
   - æ‰€æœ‰æ­¥éª¤å°è£…åœ¨ Pipeline å¯¹è±¡ä¸­ï¼Œå¯ç›´æ¥ç”¨äºæ–°æ•°æ®

è¿™æ˜¯åˆ†ç±»è¯„ä¼°ä¸­çš„æœ€ä½³å®è·µï¼Œé¿å…"è™šé«˜"çš„æ€§èƒ½ä¼°è®¡ã€‚

### å±€é™æ€§ä¸å› æœè­¦å‘Š

âš ï¸ **æœ¬åˆ†æä»…æè¿° {target} ä¸é¢„æµ‹ç‰¹å¾çš„å…³è”å…³ç³»ï¼Œä¸èƒ½ç›´æ¥æ¨æ–­å› æœ**ã€‚

**å±€é™æ€§**:

1. **ç±»åˆ«ä¸å¹³è¡¡**:
   - {target}=1 çš„æ ·æœ¬æ¯”ä¾‹ä¸º {pos_ratio:.1%}{"ï¼ˆè¾ƒä½ï¼‰" if pos_ratio < 0.2 else "ï¼ˆä¸­ç­‰ï¼‰"}
   - æ¨¡å‹å¯èƒ½åœ¨å°‘æ•°ç±»ä¸Šè¡¨ç°ä¸ä½³ï¼ˆå¬å›ç‡ä½ï¼‰
   - å¦‚éœ€ä¼˜åŒ–å°‘æ•°ç±»ï¼Œå¯è°ƒæ•´åˆ†ç±»é˜ˆå€¼æˆ–ä½¿ç”¨è¿‡é‡‡æ ·/æ¬ é‡‡æ ·æŠ€æœ¯

2. **è§‚å¯Ÿæ•°æ®**:
   - æœ¬åˆ†æåŸºäºè§‚æµ‹æ•°æ®ï¼Œæœªè¿›è¡Œéšæœºå®éªŒ
   - å¯èƒ½å­˜åœ¨æ··æ‚å˜é‡ï¼ˆconfoundersï¼‰å’Œåå‘å› æœ
   - ä¾‹å¦‚ï¼š{target} å¯èƒ½å½±å“æŸäº›ç‰¹å¾ï¼Œè€Œéå•å‘å› æœå…³ç³»

3. **é˜ˆå€¼é€‰æ‹©**:
   - é»˜è®¤é˜ˆå€¼ 0.5 å¯èƒ½ä¸æ˜¯ä¸šåŠ¡æœ€ä¼˜è§£
   - åº”æ ¹æ®å‡é˜³æ€§/å‡é˜´æ€§æˆæœ¬è°ƒæ•´ï¼ˆè§ ROC æ›²çº¿ï¼‰
   - å¦‚æ›´çœ‹é‡å¬å›ç‡ï¼ˆå‡å°‘æ¼æŠ¥ï¼‰ï¼Œå¯é™ä½é˜ˆå€¼

4. **æ•°æ®æ¼‚ç§»**:
   - å¦‚æœæœªæ¥æ•°æ®åˆ†å¸ƒä¸è®­ç»ƒæ•°æ®ä¸åŒï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸‹é™
   - å»ºè®®å®šæœŸç›‘æ§æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½ï¼Œå¹¶å®šæœŸé‡æ–°è®­ç»ƒ

**å› æœæ¨æ–­**:

Week 13 ä¼šå­¦ä¹ çš„**å› æœå›¾ (DAG)** å’Œè¯†åˆ«ç­–ç•¥ï¼ˆå¦‚ RCTã€å·¥å…·å˜é‡ã€åŒé‡å·®åˆ†ï¼‰å¯ç”¨äºå›ç­”"æ”¹å˜ X æ˜¯å¦ä¼šå¯¼è‡´ Y å˜åŒ–"çš„é—®é¢˜ã€‚

- æœ¬åˆ†æä»…é™äº"**é¢„æµ‹**"ï¼ˆPredictionï¼‰
- ä¸æ¶‰åŠ"**å› æœ**"ï¼ˆCausationï¼‰

### æ•°æ®æ¥æº

- **æ ·æœ¬é‡**: n = {n_total}
- **{target}=1 çš„æ¯”ä¾‹**: {pos_ratio:.2%}
- **åˆ†ææ—¥æœŸ**: 2026-02-12
- **éšæœºç§å­**: 42ï¼ˆä¿è¯å¯å¤ç°ï¼‰

---

"""
    return report


# ============================================================================
# ç¤ºä¾‹ä½¿ç”¨ï¼ˆç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºï¼‰
# ============================================================================

def demo_with_mock_data():
    """ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºå®Œæ•´æµç¨‹"""
    print("\n" + "=" * 70)
    print("StatLab åˆ†ç±»è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨ - æ¼”ç¤ºæ¨¡å¼")
    print("=" * 70)

    # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”µå•†è´­ä¹°åœºæ™¯ï¼‰
    np.random.seed(42)
    n = 1000

    df = pd.DataFrame({
        # æ•°å€¼ç‰¹å¾
        'age': np.random.randint(18, 70, n),
        'income': np.random.lognormal(10, 0.5, n),
        'days_since_last_purchase': np.random.randint(1, 365, n),

        # ç±»åˆ«ç‰¹å¾
        'gender': np.random.choice(['ç”·', 'å¥³'], n),
        'city_tier': np.random.choice(['ä¸€çº¿åŸå¸‚', 'äºŒçº¿åŸå¸‚', 'ä¸‰çº¿åŠä»¥ä¸‹'], n, p=[0.3, 0.4, 0.3]),
        'membership_level': np.random.choice(['æ™®é€š', 'é“¶å¡', 'é‡‘å¡'], n, p=[0.6, 0.3, 0.1]),
    })

    # ç›®æ ‡å˜é‡ï¼šè´­ä¹°ï¼ˆäºŒåˆ†ç±»ï¼‰
    # è´­ä¹°æ¦‚ç‡ä¸æ”¶å…¥ã€ä¼šå‘˜ç­‰çº§ã€è·ç¦»ä¸Šæ¬¡è´­ä¹°æ—¶é—´ç›¸å…³
    purchase_prob = (
        0.1 +
        0.2 * (df['income'] > df['income'].median()).astype(int) +
        0.3 * (df['membership_level'] == 'é‡‘å¡').astype(int) +
        0.15 * (df['membership_level'] == 'é“¶å¡').astype(int) +
        0.1 * (df['days_since_last_purchase'] < 30).astype(int)
    )
    df['purchase'] = np.random.binomial(1, np.clip(purchase_prob, 0, 1))

    print(f"\nğŸ“Š æ¨¡æ‹Ÿæ•°æ®æ¦‚è§ˆ:")
    print(df.head(10))
    print(f"\nè´­ä¹°ç‡: {df['purchase'].mean():.1%}")

    # 2. è¿è¡Œåˆ†ç±»è¯„ä¼°
    target = "purchase"
    numeric_features = ["age", "income", "days_since_last_purchase"]
    categorical_features = ["gender", "city_tier", "membership_level"]

    report = classification_evaluation_to_report(
        df=df,
        target=target,
        numeric_features=numeric_features,
        categorical_features=categorical_features,
        output_dir="report"
    )

    # 3. æ‰“å°æŠ¥å‘Š
    print("\n" + "=" * 70)
    print("ç”Ÿæˆçš„æŠ¥å‘Šç‰‡æ®µ:")
    print("=" * 70)
    print(report)

    # 4. ä¿å­˜åˆ°æ–‡ä»¶
    output_path = Path("report")
    output_path.mkdir(exist_ok=True)

    report_file = output_path / "classification_evaluation.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(f"âœ… ROC æ›²çº¿å·²ä¿å­˜åˆ°: {output_path}/images/roc_curve.png")

    return report


def main():
    """ä¸»å‡½æ•°"""
    # æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    report = demo_with_mock_data()

    print("\n" + "=" * 70)
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("=" * 70)
    print("""
    1. åœ¨ä½ çš„ StatLab é¡¹ç›®ä¸­ï¼Œæ›¿æ¢æ•°æ®æº:
       df = pd.read_csv("data/clean_data.csv")

    2. æŒ‡å®šä½ çš„ç›®æ ‡å˜é‡å’Œç‰¹å¾:
       target = "your_target_variable"  # å¦‚ 'purchase', 'churn'
       numeric_features = ["num_var1", "num_var2", ...]
       categorical_features = ["cat_var1", "cat_var2", ...]

    3. è¿è¡Œå‡½æ•°ç”ŸæˆæŠ¥å‘Š:
       report = classification_evaluation_to_report(
           df, target, numeric_features, categorical_features, "report"
       )

    4. å°†ç”Ÿæˆçš„æŠ¥å‘Šç‰‡æ®µè¿½åŠ åˆ° report.md

    æœ¬è„šæœ¬æ˜¯ StatLab è¶…çº§çº¿çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨ Week 09 å›å½’åˆ†æåŸºç¡€ä¸Š
    å¢åŠ äº†åˆ†ç±»è¯„ä¼°èƒ½åŠ›ã€‚å®Œæ•´æŠ¥å‘Šåº”åŒ…å«ï¼š
    - Week 01-04: æ•°æ®å¡ã€æè¿°ç»Ÿè®¡ã€æ¸…æ´—ã€EDA
    - Week 05-08: å‡è®¾æ£€éªŒã€ä¸ç¡®å®šæ€§é‡åŒ–
    - Week 09: å›å½’åˆ†æ
    - Week 10: åˆ†ç±»è¯„ä¼°ï¼ˆæœ¬è„šæœ¬ï¼‰
    """)


if __name__ == "__main__":
    main()
