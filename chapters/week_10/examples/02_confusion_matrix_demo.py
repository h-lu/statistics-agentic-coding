"""
ç¤ºä¾‹ï¼šä»å‡†ç¡®ç‡åˆ°æ··æ·†çŸ©é˜µâ€”â€”ç±»åˆ«ä¸å¹³è¡¡çš„é™·é˜±

æœ¬ä¾‹æ¼”ç¤ºï¼š
1. å‡†ç¡®ç‡åœ¨ç±»åˆ«ä¸å¹³è¡¡åœºæ™¯ä¸‹çš„è¯¯å¯¼æ€§
2. æ··æ·†çŸ©é˜µçš„å››ä¸ªç»„æˆï¼šTP, TN, FP, FN
3. ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 çš„è®¡ç®—ä¸ä¸šåŠ¡å«ä¹‰

è¿è¡Œæ–¹å¼ï¼špython3 chapters/week_10/examples/02_confusion_matrix_demo.py
é¢„æœŸè¾“å‡ºï¼š
- æ··æ·†çŸ©é˜µå¯è§†åŒ–ï¼ˆä¿å­˜ä¸º confusion_matrix.pngï¼‰
- æ§åˆ¶å°è¾“å‡ºå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 ç­‰æŒ‡æ ‡
- ä¸åŸºçº¿æ¨¡å‹çš„å¯¹æ¯”
"""
from __future__ import annotations

from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, classification_report,
    precision_score, recall_score, f1_score, accuracy_score
)
from sklearn.dummy import DummyClassifier

OUTPUT_DIR = Path(__file__).parent.parent.parent / "output" / "week_10"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def generate_imbalanced_data(n_samples: int = 1000, imbalance_ratio: float = 0.15) -> pd.DataFrame:
    """
    ç”Ÿæˆç±»åˆ«ä¸å¹³è¡¡çš„äºŒåˆ†ç±»æ•°æ®

    å‚æ•°:
        n_samples: æ€»æ ·æœ¬æ•°
        imbalance_ratio: å°‘æ•°ç±»çš„æ¯”ä¾‹

    è¿”å›:
        åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„ DataFrame
    """
    # ç”Ÿæˆç‰¹å¾
    n_minority = int(n_samples * imbalance_ratio)
    n_majority = n_samples - n_minority

    # å¤šæ•°ç±»ï¼ˆä¸æµå¤±ï¼‰
    X_majority = np.random.randn(n_majority, 2) + np.array([2, 2])
    y_majority = np.zeros(n_majority)

    # å°‘æ•°ç±»ï¼ˆæµå¤±ï¼‰
    X_minority = np.random.randn(n_minority, 2) + np.array([-1, -1])
    y_minority = np.ones(n_minority)

    # åˆå¹¶
    X = np.vstack([X_majority, X_minority])
    y = np.hstack([y_majority, y_minority])

    # æ‰“ä¹±
    indices = np.random.permutation(len(y))
    X = X[indices]
    y = y[indices]

    return pd.DataFrame({
        'feature_1': X[:, 0],
        'feature_2': X[:, 1],
        'churn': y
    })


def plot_confusion_matrix(cm: np.ndarray, class_names: list, title: str = "æ··æ·†çŸ©é˜µ") -> None:
    """ç”»æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
    plt.figure(figsize=(8, 6))

    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    # åˆ›å»ºæ ‡ç­¾
    labels = np.array([
        [f"{cm[i, j]}\n({cm_percent[i, j]:.1f}%)"
         for j in range(cm.shape[1])]
        for i in range(cm.shape[0])
    ])

    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})

    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.title(title, fontsize=14)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')
    print(f"âœ… {title}å·²ä¿å­˜ä¸º confusion_matrix.png")
    plt.close()


def explain_confusion_matrix_metrics(tn: int, fp: int, fn: int, tp: int) -> None:
    """
    è§£é‡Šæ··æ·†çŸ©é˜µæŒ‡æ ‡çš„ä¸šåŠ¡å«ä¹‰

    å‚æ•°:
        tn, fp, fn, tp: æ··æ·†çŸ©é˜µçš„å››ä¸ªå…ƒç´ 
    """
    print("\n" + "=" * 60)
    print("æ··æ·†çŸ©é˜µæŒ‡æ ‡è¯¦è§£")
    print("=" * 60)

    # åŸºæœ¬ç»Ÿè®¡
    total = tn + fp + fn + tp
    actual_positive = tp + fn
    actual_negative = tn + fp
    predicted_positive = tp + fp
    predicted_negative = tn + fn

    print(f"\næ··æ·†çŸ©é˜µï¼š")
    print(f"{'':>12} {'é¢„æµ‹ä¸æµå¤±':>12} {'é¢„æµ‹æµå¤±':>12}")
    print(f"{'å®é™…ä¸æµå¤±':>12} {tn:>12} {fp:>12}")
    print(f"{'å®é™…æµå¤±':>12} {fn:>12} {tp:>12}")

    print(f"\nåŸºæœ¬ç»Ÿè®¡ï¼š")
    print(f"  æ€»æ ·æœ¬æ•°: {total}")
    print(f"  å®é™…æµå¤±: {actual_positive} ({actual_positive/total*100:.1f}%)")
    print(f"  å®é™…ä¸æµå¤±: {actual_negative} ({actual_negative/total*100:.1f}%)")

    # å‡†ç¡®ç‡
    accuracy = (tp + tn) / total
    print(f"\n{'='*60}")
    print("1. å‡†ç¡®ç‡ (Accuracy)")
    print("=" * 60)
    print(f"  å…¬å¼: (TP + TN) / (TP + TN + FP + FN)")
    print(f"  è®¡ç®—: ({tp} + {tn}) / {total} = {accuracy:.2%}")
    print(f"  å«ä¹‰: æ‰€æœ‰é¢„æµ‹ä¸­ï¼Œé¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹")
    print(f"  âš ï¸  åœ¨ç±»åˆ«ä¸å¹³è¡¡æ—¶ä¼šè¯¯å¯¼ï¼")

    # ç²¾ç¡®ç‡
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    print(f"\n{'='*60}")
    print("2. ç²¾ç¡®ç‡ (Precision) - æŸ¥å‡†ç‡")
    print("=" * 60)
    print(f"  å…¬å¼: TP / (TP + FP)")
    print(f"  è®¡ç®—: {tp} / ({tp} + {fp}) = {precision:.2%}")
    print(f"  å«ä¹‰: åœ¨æ‰€æœ‰é¢„æµ‹ä¸º'æµå¤±'çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£æµå¤±çš„æ¯”ä¾‹")
    print(f"  ä¸šåŠ¡ä»·å€¼: é¿å…è¯¯æŠ¥ï¼Œå‡å°‘è¥é”€æˆæœ¬æµªè´¹")
    print(f"  åœºæ™¯: ç»™'å¯èƒ½æµå¤±'å®¢æˆ·å‘ä¼˜æƒ åˆ¸ï¼Œç²¾ç¡®ç‡ä½æ„å‘³ç€")
    print(f"        å¾ˆå¤šä¼˜æƒ åˆ¸å‘ç»™äº†æœ¬æ¥ä¸ä¼šæµå¤±çš„å®¢æˆ·")

    # å¬å›ç‡
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    print(f"\n{'='*60}")
    print("3. å¬å›ç‡ (Recall) - æŸ¥å…¨ç‡ / çµæ•åº¦ / TPR")
    print("=" * 60)
    print(f"  å…¬å¼: TP / (TP + FN)")
    print(f"  è®¡ç®—: {tp} / ({tp} + {fn}) = {recall:.2%}")
    print(f"  å«ä¹‰: åœ¨æ‰€æœ‰çœŸå®'æµå¤±'çš„æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹")
    print(f"  ä¸šåŠ¡ä»·å€¼: å‡å°‘æ¼æŠ¥ï¼ŒæŠ“ä½æ›´å¤šæµå¤±å®¢æˆ·")
    print(f"  åœºæ™¯: å®¢æˆ·æµå¤±é¢„è­¦ï¼Œå¬å›ç‡ä½æ„å‘³ç€")
    print(f"        å¤§é‡çœŸå®æµå¤±å®¢æˆ·è¢«é—æ¼ï¼ŒæŸå¤±å®¢æˆ·ç»ˆèº«ä»·å€¼")

    # F1 åˆ†æ•°
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    print(f"\n{'='*60}")
    print("4. F1 åˆ†æ•° (F1-Score)")
    print("=" * 60)
    print(f"  å…¬å¼: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)")
    print(f"  è®¡ç®—: 2 Ã— {precision:.3f} Ã— {recall:.3f} / ({precision:.3f} + {recall:.3f}) = {f1:.3f}")
    print(f"  å«ä¹‰: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°")
    print(f"  ä¸ºä»€ä¹ˆç”¨è°ƒå’Œå¹³å‡ï¼Ÿæƒ©ç½šæç«¯æƒ…å†µ")
    print(f"    ä¾‹å¦‚: ç²¾ç¡®ç‡=1.0, å¬å›ç‡=0.01")
    print(f"         ç®—æœ¯å¹³å‡=0.505 (çœ‹èµ·æ¥è¿˜è¡Œ)")
    print(f"         è°ƒå’Œå¹³å‡â‰ˆ0.02 (æ­ç¤ºæ¨¡å‹å‡ ä¹æ— ç”¨)")

    # ç‰¹å¼‚æ€§å’Œå‡é˜³æ€§ç‡
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0
    print(f"\n{'='*60}")
    print("5. å…¶ä»–æŒ‡æ ‡")
    print("=" * 60)
    print(f"  å‡é˜³æ€§ç‡ (FPR): FP / (FP + TN) = {fp} / ({fp} + {tn}) = {fpr:.2%}")
    print(f"  çœŸé˜´æ€§ç‡ (TNR/ç‰¹å¼‚æ€§): TN / (TN + FP) = {tn} / ({tn} + {fp}) = {tnr:.2%}")

    # ä¸šåŠ¡æˆæœ¬
    print(f"\n{'='*60}")
    print("6. ä¸šåŠ¡æˆæœ¬è§†è§’")
    print("=" * 60)
    cost_fp = 100  # è¯¯æŠ¥æˆæœ¬ï¼ˆå…ƒï¼‰
    cost_fn = 500  # æ¼æŠ¥æˆæœ¬ï¼ˆå…ƒï¼‰

    total_cost = fp * cost_fp + fn * cost_fn
    print(f"  å‡é˜³æ€§æˆæœ¬ï¼ˆè¯¯æŠ¥ï¼‰: {fp} ä¸ª Ã— Â¥{cost_fp} = Â¥{fp * cost_fp:,}")
    print(f"  å‡é˜´æ€§æˆæœ¬ï¼ˆæ¼æŠ¥ï¼‰: {fn} ä¸ª Ã— Â¥{cost_fn} = Â¥{fn * cost_fn:,}")
    print(f"  æ€»æˆæœ¬: Â¥{total_cost:,}")

    print(f"\n  ğŸ’¡ å¦‚æœä¼˜åŒ–ç²¾ç¡®ç‡ï¼ˆå‡å°‘ FPï¼‰ï¼Œå¯ä»¥èŠ‚çœ Â¥{fp * cost_fp:,}")
    print(f"  ğŸ’¡ å¦‚æœä¼˜åŒ–å¬å›ç‡ï¼ˆå‡å°‘ FNï¼‰ï¼Œå¯ä»¥èŠ‚çœ Â¥{fn * cost_fn:,}")


def demonstrate_accuracy_paradox() -> None:
    """æ¼”ç¤ºå‡†ç¡®ç‡æ‚–è®º"""
    print("\n" + "=" * 60)
    print("å‡†ç¡®ç‡æ‚–è®ºæ¼”ç¤º")
    print("=" * 60)

    # åœºæ™¯1ï¼šå¹³è¡¡æ•°æ®
    print("\nã€åœºæ™¯1ï¼šå¹³è¡¡æ•°æ®ã€‘")
    tn1, fp1, fn1, tp1 = 80, 10, 10, 80
    acc1 = (tp1 + tn1) / (tp1 + tn1 + fp1 + fn1)
    recall1 = tp1 / (tp1 + fn1)

    print(f"  æ··æ·†çŸ©é˜µ: TN={tn1}, FP={fp1}, FN={fn1}, TP={tp1}")
    print(f"  å‡†ç¡®ç‡: {acc1:.2%}")
    print(f"  å¬å›ç‡: {recall1:.2%}")
    print(f"  è¯„ä¼°: æ¨¡å‹è¡¨ç°è‰¯å¥½")

    # åœºæ™¯2ï¼šä¸å¹³è¡¡æ•°æ®ï¼ˆå‚»ç“œæ¨¡å‹ï¼‰
    print("\nã€åœºæ™¯2ï¼šä¸å¹³è¡¡æ•°æ® - æ€»æ˜¯é¢„æµ‹å¤šæ•°ç±»ã€‘")
    tn2, fp2, fn2, tp2 = 150, 0, 30, 0
    acc2 = (tp2 + tn2) / (tp2 + tn2 + fp2 + fn2)
    recall2 = tp2 / (tp2 + fn2) if (tp2 + fn2) > 0 else 0

    print(f"  æ··æ·†çŸ©é˜µ: TN={tn2}, FP={fp2}, FN={fn2}, TP={tp2}")
    print(f"  å‡†ç¡®ç‡: {acc2:.2%}")
    print(f"  å¬å›ç‡: {recall2:.2%}")
    print(f"  è¯„ä¼°: æ¨¡å‹æ¯«æ— ä»·å€¼ï¼å‡†ç¡®ç‡é«˜ä½†å¬å›ç‡ä¸º0")

    # åœºæ™¯3ï¼šä¸å¹³è¡¡æ•°æ®ï¼ˆçœŸå®æ¨¡å‹ï¼‰
    print("\nã€åœºæ™¯3ï¼šä¸å¹³è¡¡æ•°æ® - çœŸå®æ¨¡å‹ã€‘")
    tn3, fp3, fn3, tp3 = 140, 10, 15, 15
    acc3 = (tp3 + tn3) / (tp3 + tn3 + fp3 + fn3)
    recall3 = tp3 / (tp3 + fn3) if (tp3 + fn3) > 0 else 0

    print(f"  æ··æ·†çŸ©é˜µ: TN={tn3}, FP={fp3}, FN={fn3}, TP={tp3}")
    print(f"  å‡†ç¡®ç‡: {acc3:.2%}")
    print(f"  å¬å›ç‡: {recall3:.2%}")
    print(f"  è¯„ä¼°: æ¨¡å‹æœ‰ä»·å€¼ï¼å‡†ç¡®ç‡ç•¥ä½ä½†å¬å›ç‡50%")

    print("\n" + "=" * 60)
    print("ç»“è®ºï¼š")
    print("=" * 60)
    print(f"  åœºæ™¯2 çš„å‡†ç¡®ç‡ï¼ˆ{acc2:.1%}ï¼‰é«˜äº åœºæ™¯3ï¼ˆ{acc3:.1%}ï¼‰")
    print(f"  ä½†åœºæ™¯2çš„å¬å›ç‡ä¸º 0%ï¼Œå®Œå…¨æ¼æ‰äº†æ‰€æœ‰æµå¤±å®¢æˆ·")
    print(f"  åœºæ™¯3çš„å¬å›ç‡ä¸º {recall3:.1%}ï¼Œèƒ½è¯†åˆ«ä¸€åŠçš„æµå¤±å®¢æˆ·")
    print(f"  â†’ åœ¨ç±»åˆ«ä¸å¹³è¡¡åœºæ™¯ä¸‹ï¼Œå‡†ç¡®ç‡æ˜¯è¯¯å¯¼æ€§æŒ‡æ ‡ï¼")


def main() -> None:
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç¤ºä¾‹2: ä»å‡†ç¡®ç‡åˆ°æ··æ·†çŸ©é˜µ")
    print("=" * 60)

    # 1. ç”Ÿæˆç±»åˆ«ä¸å¹³è¡¡æ•°æ®
    df = generate_imbalanced_data(n_samples=1000, imbalance_ratio=0.15)

    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"  æµå¤±å®¢æˆ·: {df['churn'].sum()} ({df['churn'].mean():.1%})")
    print(f"  ä¸æµå¤±å®¢æˆ·: {(df['churn'] == 0).sum()} ({(df['churn'] == 0).mean():.1%})")
    print(f"  âš ï¸  è¿™æ˜¯ä¸€ä¸ªç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†ï¼")

    # 2. åˆ’åˆ†æ•°æ®
    X = df[['feature_1', 'feature_2']]
    y = df['churn']
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # 3. è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # 4. è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # 5. ç”»æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(cm, ['ä¸æµå¤±', 'æµå¤±'], 'é€»è¾‘å›å½’æ··æ·†çŸ©é˜µ')

    # 6. è§£é‡ŠæŒ‡æ ‡
    explain_confusion_matrix_metrics(tn, fp, fn, tp)

    # 7. æ‰“å°åˆ†ç±»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("åˆ†ç±»æŠ¥å‘Š (sklearn.metrics.classification_report)")
    print("=" * 60)
    print(classification_report(y_test, y_pred, target_names=['ä¸æµå¤±', 'æµå¤±']))

    # 8. ä¸åŸºçº¿å¯¹æ¯”
    print("\n" + "=" * 60)
    print("ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”")
    print("=" * 60)

    dummy = DummyClassifier(strategy='most_frequent', random_state=42)
    dummy.fit(X_train, y_train)
    y_pred_dummy = dummy.predict(X_test)

    acc_model = accuracy_score(y_test, y_pred)
    acc_dummy = accuracy_score(y_test, y_pred_dummy)
    recall_model = recall_score(y_test, y_pred)
    recall_dummy = recall_score(y_test, y_pred_dummy)

    print(f"\nåŸºçº¿æ¨¡å‹ï¼ˆæ€»æ˜¯é¢„æµ‹å¤šæ•°ç±»ï¼‰:")
    print(f"  å‡†ç¡®ç‡: {acc_dummy:.2%}")
    print(f"  å¬å›ç‡: {recall_dummy:.2%}")

    print(f"\né€»è¾‘å›å½’æ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {acc_model:.2%}")
    print(f"  å¬å›ç‡: {recall_model:.2%}")

    print(f"\næ”¹è¿›:")
    print(f"  å‡†ç¡®ç‡å˜åŒ–: {(acc_model - acc_dummy):.1%}")
    print(f"  å¬å›ç‡å˜åŒ–: {(recall_model - recall_dummy):.1%}")

    # 9. æ¼”ç¤ºå‡†ç¡®ç‡æ‚–è®º
    demonstrate_accuracy_paradox()

    print("\n" + "=" * 60)
    print("æ€»ç»“")
    print("=" * 60)
    print("""
åœ¨ç±»åˆ«ä¸å¹³è¡¡åœºæ™¯ä¸‹ï¼š
1. å‡†ç¡®ç‡ä¼šæ’’è°ï¼šæ€»æ˜¯é¢„æµ‹å¤šæ•°ç±»çš„æ¨¡å‹å‡†ç¡®ç‡å¾ˆé«˜ä½†æ¯«æ— ä»·å€¼
2. éœ€è¦çœ‹æ··æ·†çŸ©é˜µï¼šå…³æ³¨ TP, TN, FP, FN çš„åˆ†å¸ƒ
3. æ ¹æ®ä¸šåŠ¡ç›®æ ‡ä¼˜åŒ–ï¼š
   - æƒ³å‡å°‘è¯¯æŠ¥ï¼ˆæµªè´¹è¥é”€æˆæœ¬ï¼‰â†’ ä¼˜åŒ–ç²¾ç¡®ç‡
   - æƒ³å‡å°‘æ¼æŠ¥ï¼ˆæŠ“ä½æ›´å¤šæµå¤±å®¢æˆ·ï¼‰â†’ ä¼˜åŒ–å¬å›ç‡
   - éœ€è¦å¹³è¡¡ä¸¤è€… â†’ çœ‹ F1 åˆ†æ•°
    """)

    print("\n" + "=" * 60)
    print("âœ… ç¤ºä¾‹2å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
