# Week 10 作业：分类与评估实战

> "Prediction is very difficult, especially about the future."
> — Niels Bohr

本周作业要求你**完整走一遍分类评估流程**：从线性回归的局限性到逻辑回归，从准确率陷阱到混淆矩阵，从 ROC-AUC 到交叉验证，从数据泄漏到 Pipeline 工程实践。你不再只是"算个准确率"，而是学会判断"模型是否真的有效"、"评估是否诚实"、"阈值是否合理"。

---

## 作业结构

| 层级 | 内容 | 建议时间 |
|------|------|----------|
| 基础作业（必做） | 逻辑回归、混淆矩阵、ROC-AUC 分析 | 3-4 小时 |
| 进阶作业（选做） | Pipeline + ColumnTransformer、交叉验证 | 1-2 小时 |
| 挑战作业（可选） | StatLab 分类评估完整流水线 | 2-3 小时 |
| AI 协作练习（可选） | 审查 AI 生成的分类报告 | 1 小时 |

---

## 基础作业（必做）

### 任务 1：从线性回归到逻辑回归——理解分类动机（25 分）

**目标**：理解为什么不能用线性回归做分类，掌握逻辑回归的基本概念

**背景**：小北拿到了一份客户流失数据（Churn = Yes/No），他灵机一动："我用 Week 09 学过的线性回归，把 Yes 编码为 1、No 编码为 0，不就能预测了吗？"

**步骤**：

1. **尝试线性回归（感受问题）**：
   - 将 Churn 编码为数值（Yes=1, No=0）
   - 拟合线性回归模型：`流失编码 = β₀ + β₁×合同期 + β₂×月费`
   - 查看预测值范围：有没有超出 [0, 1] 的值？
   - 思考：负数或大于 1 的概率是什么意思？

2. **拟合逻辑回归**：
   - 使用 `sklearn.linear_model.LogisticRegression`
   - 使用 `predict_proba()` 获取概率预测
   - 查看预测概率范围：是否都在 [0, 1] 之间？

3. **对比两种方法**：
   - 写一段对比分析（150-200 字），回答：
     - 线性回归预测值有什么问题？
     - Sigmoid 函数的作用是什么？
     - 为什么逻辑回归更适合分类问题？

4. **提交** `linear_vs_logistic.md`，包含：
   - 线性回归预测值范围截图/输出
   - 逻辑回归预测概率范围截图/输出
   - 对比分析（150-200 字）

**输入示例**：
```
客户ID | 合同期(月) | 月费(元) | 是否流失(编码)
1      | 12         | 89       | 0
2      | 6          | 105      | 1
3      | 24         | 75       | 0
...
```

**输出示例**：
```
线性回归预测值范围: [-0.15, 1.23]
逻辑回归预测概率范围: [0.03, 0.96]

对比分析:
线性回归的预测值包含了负数(-0.15)和大于1的值(1.23)，这在概率定义上是没有意义的。
概率必须在[0,1]之间，所以线性回归不适合分类问题。逻辑回归通过Sigmoid
函数将线性预测压缩到[0,1]区间，输出的值可以解释为"流失的概率"，更适合
二分类任务。
```

**评分标准**：
- 线性回归拟合正确（5 分）
- 逻辑回归拟合正确（5 分）
- 正确识别线性回归的问题（8 分）
- 对比分析清晰准确（7 分）

---

### 任务 2：逻辑回归系数解释——优势比（30 分）

**目标**：正确解释逻辑回归系数的含义，理解优势比（Odds Ratio）

**背景**：小北拟合了逻辑回归模型，得到了这样的系数表：

```
截距: -2.50
合同期: -0.055
月费: 0.023
```

阿码问："**-0.055 代表什么？是不是说'合同期每增加 1 月，流失概率降低 0.055'？**"

**步骤**：

1. **拟合逻辑回归模型**：
   - 使用你的流失数据（或其他二分类数据）
   - 提取截距和系数

2. **计算优势比（Odds Ratio）**：
   - 对每个系数计算 `exp(β)`
   - 解释优势比的含义

3. **正确解释系数**：
   - 对每个系数写出完整解释
   - 使用"在其他变量不变的情况下"（和 Week 09 回归一样）
   - 使用"优势比"语言：
     - OR = 0.95 → "优势降低到原来的 95%（降低 5%）"
     - OR = 1.03 → "优势增加到原来的 1.03 倍（增加 3%）"

4. **对比错误解释**：
   - 小北的错误解释："合同期每增加 1 月，流失概率降低 5.5%"
   - 写出为什么这是错的（100 字）
   - 正确解释应该是什么？

5. **提交** `logistic_coefficient_interpretation.md`，包含：
   - 系数表（含优势比）
   - 每个系数的正确解释
   - 错误解释分析和纠正

**输出示例**：
```
逻辑回归系数表:

特征        | 系数(β) | 优势比(OR=exp(β)) | 95% CI
-----------|----------|-------------------|--------
截距       | -2.50    | -                 | [-3.20, -1.80]
合同期(月) | -0.055   | 0.946            | [0.934, 0.959]
月费(元)   | 0.023    | 1.023            | [1.015, 1.031]

解释:
- 截距 β₀ = -2.50：当所有特征为0时（这个场景无实际意义，只是数学基准）
- 合同期 β₁ = -0.055：在其他变量不变的情况下，合同期每增加1个月，流失的对
  数优势下降0.055，即优势降低到原来的94.6%（降低5.4%）
- 月费 β₂ = 0.023：在其他变量不变的情况下，月费每增加1元，流失的对数优
  势增加0.023，即优势增加到原来的1.023倍（增加2.3%）

错误解释分析:
小北说"合同期每增加1月，流失概率降低5.5%"是错误的。逻辑回归的系数是
"对数优势比"的变化，不是"概率"的变化。如果合同期从12月增加到13月，
流失概率的变化取决于其他特征值和当前的概率水平，不是固定的5.5%。正确的
说法是"优势降低5.4%"，不是"概率降低5.5%"。
```

**评分标准**：
- 系数提取正确（5 分）
- 优势比计算正确（8 分）
- 系数解释准确（每个 5 分，共 10 分）
- 错误解释分析合理（7 分）

**常见错误**：
- ❌ 混淆"优势比"与"概率差"
- ❌ 忘记"在其他变量不变的情况下"
- ❌ 直接说"概率增加 X%"（错误，应该说"优势增加 X%"）

---

### 任务 3：混淆矩阵与分类评估——从准确率到 F1（35 分）

**目标**：理解准确率陷阱，掌握混淆矩阵、精确率、召回率、F1

**背景**：小北训练了一个逻辑回归模型，准确率 85%。他兴冲冲地对老潘说："模型很棒！"

老潘问："**测试集里有多少客户不流失？**"

小北算了一下：170 个不流失，30 个流失。

老潘又问："**如果你的模型预测'所有人都不流失'，准确率是多少？**"

小北愣住了：170/200 = 85%。

"所以你的模型和傻瓜模型一样？"老潘问。

**步骤**：

1. **计算混淆矩阵**：
   - 使用 `confusion_matrix(y_test, y_pred)`
   - 提取 TN, FP, FN, TP
   - 解释每个数字的业务含义

2. **计算评估指标**：
   - 准确率：`(TP + TN) / (TP + TN + FP + FN)`
   - 精确率：`TP / (TP + FP)`
   - 召回率：`TP / (TP + FN)`
   - F1 分数：`2 × 精确率 × 召回率 / (精确率 + 召回率)`

3. **与基线对比**：
   - 拟合 `DummyClassifier(strategy='most_frequent')`
   - 计算基线准确率、精确率、召回率、F1
   - 对比你的模型和基线模型

4. **业务解释**：
   - 假阳性成本（误报）：把"不流失"错判为"流失"，会浪费营销成本
   - 假阴性成本（漏报）：把"流失"错判为"不流失"，会损失客户终身价值
   - 你的模型在哪个成本上更敏感？

5. **提交** `classification_evaluation_report.md`，包含：
   - 混淆矩阵（表格形式）
   - 四个评估指标（准确率、精确率、召回率、F1）
   - 与基线对比
   - 业务解释（假阳性/假阴性成本）

**输入示例**：
```
真实标签: [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, ...]
预测标签: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...]
```

**输出示例**：
```
混淆矩阵:

              | 预测不流失 | 预测流失 |
--------------|-----------|---------|
实际不流失 (TN)| 165       | 5 (FP)  |
实际流失 (FN) | 25        | 5 (TP)  |

评估指标:
- 准确率: (165 + 5) / 200 = 85.0%
- 精确率: 5 / (5 + 5) = 50.0%（预测为流失的客户中，真正流失的比例）
- 召回率: 5 / (5 + 25) = 16.7%（真实流失的客户中，被识别的比例）
- F1 分数: 2 × 0.5 × 0.167 / (0.5 + 0.167) = 0.25

基线对比:
- 基线模型（总是预测不流失）: 准确率=85.0%, 精确率=0%, 召回率=0%, F1=0
- 本模型: 准确率=85.0%, 精确率=50.0%, 召回率=16.7%, F1=0.25
- 结论: 虽然准确率相同，但本模型能识别出16.7%的真实流失客户，而基线模型的召回率为0%

业务解释:
- 假阳性（误报）: 5个客户被误判为流失，如果给每个客户发100元优惠券，浪费500元
- 假阴性（漏报）: 25个真实流失客户被遗漏，如果每个客户终身价值500元，损失12500元
- 模型价值: 虽然召回率较低(16.7%)，但相比基线(0%)有提升，至少识别出了5个流失客户
- 建议: 应该降低阈值以提高召回率（宁可误报，不能漏报），因为漏报成本(12500元)远高于误报成本(500元)
```

**评分标准**：
- 混淆矩阵计算正确（8 分）
- 四个评估指标计算正确（12 分，每个 3 分）
- 与基线对比完整（7 分）
- 业务解释合理（8 分）

---

### 任务 4：ROC-AUC 分析——阈值无关评估（30 分）

**目标**：理解 ROC-AUC 的直觉，掌握阈值无关的模型评估

**背景**：阿码问："**我们的阈值是 0.5，但谁规定的 0.5？**"

小北想了想："那……我试试 0.3？"

调整阈值后，召回率从 17% 涨到了 43%，但准确率从 85% 降到了 81%。

老潘说："**你需要一个不依赖阈值的评估指标——ROC-AUC**。"

**步骤**：

1. **绘制 ROC 曲线**：
   - 使用 `roc_curve(y_test, y_proba)`
   - 计算 FPR（假阳性率）和 TPR（真阳性率/召回率）
   - 画出 ROC 曲线

2. **计算 AUC**：
   - 使用 `roc_auc_score(y_test, y_proba)`
   - 解释 AUC 的含义

3. **AUC 直觉理解**：
   - 写一段文字（150 字），解释"AUC = 0.75 是什么意思？"
   - 使用"随机样本对"解释：随机选一个正样本和一个负样本，模型给正样本更高概率的概率

4. **阈值选择分析**：
   - 尝试至少 3 个不同阈值（0.3, 0.5, 0.7）
   - 对比每个阈值下的混淆矩阵、精确率、召回率
   - 讨论哪个阈值最符合业务目标（假设漏报成本高于误报）

5. **提交** `roc_auc_analysis.md`，包含：
   - ROC 曲线图
   - AUC 值和解释
   - 阈值对比表
   - 阈值选择建议

**输出示例**：
```
ROC-AUC 分析:

AUC: 0.812

AUC 解释:
AUC = 0.812 的意思是：如果我随机选一个"流失"客户和一个"不流失"客户，模型给"流失"
客户更高概率的概率是81.2%。AUC 衡量的是模型区分正负样本的能力，不依赖于任何
特定的分类阈值。

阈值对比:

阈值 | 准确率 | 精确率 | 召回率 | F1  | 业务解释
-----|--------|--------|--------|-----|---------
0.3  | 81.0%  | 38.3%  | 43.3%  | 0.41| 低阈值，抓更多流失客户，但误报多
0.5  | 85.0%  | 50.0%  | 16.7%  | 0.25| 默认阈值，平衡但召回率低
0.7  | 87.5%  | 66.7%  | 6.7%   | 0.12| 高阈值，很少误报但漏掉大量流失客户

建议:
- 如果业务目标是"不漏掉任何流失客户"（漏报成本高），使用阈值0.3或更低
- 如果业务目标是"不浪费营销成本"（误报成本高），使用阈值0.7或更高
- 如果要求平衡，使用阈值0.5
- 在客户流失场景中，通常漏报成本（损失客户终身价值）高于误报成本（浪费优惠券），
  因此建议使用0.3左右的阈值
```

**评分标准**：
- ROC 曲线正确绘制（8 分）
- AUC 解释准确（8 分）
- 阈值对比完整（8 分）
- 阈值选择建议合理（6 分）

---

## 进阶作业（选做）

### 任务 5：Pipeline + ColumnTransformer——防止数据泄漏（30 分）

**目标**：理解数据泄漏风险，掌握 Pipeline 工程实践

**背景**：小北学会了交叉验证，兴冲冲地写下了这样的代码：

```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

# 第一步：全局标准化（错误！）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # 在整个 X 上拟合

# 第二步：交叉验证
scores = cross_val_score(log_reg, X_scaled, y, cv=5)
```

老潘看了一眼代码，脸色立刻变了："**你的代码有严重问题——你泄漏了数据。**"

**步骤**：

1. **识别数据泄漏**：
   - 上面的代码为什么有问题？（100 字）
   - 什么是数据泄漏？它会导致什么后果？

2. **使用 Pipeline 修正**：
   - 使用 `Pipeline` 将预处理和模型串联
   - 使用 `ColumnTransformer` 处理不同类型的列（数值 + 类别）
   - 确保所有预处理都在 Pipeline 内

3. **对比两种方法**：
   - 运行错误方法（全局标准化）和正确方法（Pipeline）
   - 对比交叉验证得分
   - 解释为什么得分会变化

4. **提交** `pipeline_data_leakage.md`，包含：
   - 数据泄漏解释
   - Pipeline + ColumnTransformer 代码
   - 两种方法的交叉验证对比
   - 对比分析

**输出示例**：
```
数据泄漏分析:

错误代码的问题:
`scaler.fit_transform(X)` 在整个数据集上计算了均值和方差。交叉验证的每个折都会
"看到"其他折的统计量。模型在训练时已经"知道"测试集的分布特征，所以测试集
上的性能会被高估。这就像考试前偷看了答案。

后果:
- 交叉验证得分虚高（87.5%），但真正部署到生产环境，准确率可能只有75%
- 模型在生产环境中性能大幅下降（因为生产数据没有"未来信息"）
- 不可复现：不同数据划分下结果差异很大

Pipeline + ColumnTransformer 代码:

[完整的代码实现...]

对比结果:
- 错误方法（全局标准化）: CV准确率 = 87.5% ± 1.2%
- 正确方法（Pipeline）: CV准确率 = 81.2% ± 1.5%
- 差异: 6.3%的虚高性能

结论:
87.5%是"作弊"的成绩，81.2%是"诚实"的成绩。我们宁愿现在看到81.2%，
也不愿上线后发现只有75%。Pipeline 不是工程炫技，是避免数据泄漏的唯一方法。
```

**评分标准**：
- 数据泄漏解释准确（8 分）
- Pipeline 代码正确（10 分）
- 对比分析合理（7 分）
- ColumnTransformer 使用正确（5 分）

---

### 任务 6：Stratified K-fold 交叉验证（25 分）

**目标**：理解为什么分类问题需要分层交叉验证，掌握 StratifiedKFold

**背景**：小北使用了普通的 KFold 交叉验证，但老潘说："**分类问题应该用 StratifiedKFold**。"

**步骤**：

1. **对比 KFold 和 StratifiedKFold**：
   - 普通KFold：随机划分，每个折的类别比例可能与整体不同
   - StratifiedKFold：分层划分，每个折保持与整体相同的类别比例

2. **可视化类别分布**：
   - 画出整个训练集的类别分布
   - 画出每个 fold 的类别分布
   - 对比两种方法

3. **运行交叉验证**：
   - 使用普通 KFold 运行 5-fold CV
   - 使用 StratifiedKFold 运行 5-fold CV
   - 对比准确率、F1 的均值和标准差

4. **提交** `stratified_cv_analysis.md`，包含：
   - 两种 CV 方法的对比
   - 类别分布可视化
   - CV 结果对比表
   - 为什么 StratifiedKFold 更适合分类问题？

**评分标准**：
- 两种方法对比清晰（8 分）
- 可视化正确（7 分）
- CV 结果对比合理（6 分）
- 解释准确（4 分）

---

## 挑战作业（可选）

### 任务 7：StatLab 分类流水线（40 分）

**目标**：将你的 StatLab 报告扩展到分类问题，产出完整的分类评估章节

**背景**：到上周为止，StatLab 报告已经有了回归分析（预测连续值）。但老潘问："**你能预测'哪些客户会流失/购买'吗？**"

这正是本周"分类与评估"派上用场的地方。

**要求**：

1. **在 report.md 中添加"分类评估"章节**，包含：
   - 研究问题：哪些因素影响二分类目标（如流失/购买）？
   - 模型设定：逻辑回归，特征列表
   - 逻辑回归系数：系数表 + 优势比解释
   - 混淆矩阵：精确率、召回率、F1
   - ROC-AUC 分析：阈值无关评估
   - 交叉验证：K-fold CV 结果（均值 ± 标准差）
   - Pipeline + ColumnTransformer：防止数据泄漏的代码
   - 基线对比：与多数类分类器对比
   - 阈值选择：业务目标分析
   - 局限性：类别不平衡、因果警告

2. **使用示例代码**：
   - 参考 `examples/10_statlab_classification.py`（如果存在）
   - 或使用 `starter_code/solution.py` 作为参考

3. **可视化**：
   - ROC 曲线图
   - 混淆矩阵热力图（可选）
   - CV 稳定性图（可选）

4. **提交**：
   - 更新的 `report.md`（添加分类评估章节）
   - 分类评估代码文件 `classification_analysis.py`
   - 可视化图表文件

**输出示例结构**：
```markdown
## 分类评估

### 研究问题
哪些因素影响客户流失（Churn）？

### 模型设定
- 算法: 逻辑回归
- 特征: 合同期、月费、总消费、合同类型、支付方式
- 预处理: 数值特征标准化 + 类别特征 One-Hot 编码
- 评估方法: 5-fold 分层交叉验证

### 逻辑回归系数
[系数表 + 优势比解释]

### 混淆矩阵与评估指标
[混淆矩阵表 + 指标 + 业务解释]

### ROC-AUC 分析
[AUC 值 + ROC 曲线图 + 阈值选择讨论]

### 交叉验证结果
[CV 得分表 + 稳定性分析]

### 基线对比
[与多数类分类器对比]

### 工程实践：防止数据泄漏
[Pipeline + ColumnTransformer 代码片段 + 解释]

### 局限性
[类别不平衡、观察数据、预测≠因果]
```

**评分标准**：
- 报告结构完整（10 分）
- 分析深度充足（10 分）
- 工程实践正确（Pipeline + CV，10 分）
- 可视化清晰（6 分）
- 局限性讨论（4 分）

---

## AI 协作练习（可选）

### 任务 8：审查 AI 生成的分类报告（20 分）

**背景**：根据 `shared/ai_progression.md`，Week 10 属于**"协作期"**（Week 09-12）——AI 辅助建模与评估，但关键决策由你主导。你需要学会审查 AI 生成的分类报告。

**步骤**：

1. **获取 AI 生成的报告**：
   - 使用你喜欢的 AI 工具（如 ChatGPT、Claude 等）
   - 输入以下提示词：
     ```
     我有一份客户流失数据，包含以下变量：
     - tenure_months: 合同期（月）
     - monthly_charges: 月费（元）
     - total_charges: 总消费（元）
     - contract_type: 合同类型（月付/年付/两年付）
     - payment_method: 支付方式
     - churn: 是否流失（Yes/No）

     请帮我进行分类分析，回答"哪些因素影响客户流失"。
     ```

2. **保存 AI 原始输出**，将其保存为 `ai_original_classification_report.txt`

3. **使用审查清单**：

   检查 AI 报告是否包含以下内容：
   - [ ] 混淆矩阵（TP/TN/FP/FN）
   - [ ] 精确率、召回率、F1 分数
   - [ ] ROC-AUC 分析（或至少提到 AUC）
   - [ ] 与基线模型对比
   - [ ] 数据泄漏检查（Pipeline 提及）
   - [ ] 交叉验证（CV）结果
   - [ ] 阈值选择讨论
   - [ ] 逻辑回归系数的正确解释（优势比，不是概率）
   - [ ] 类别不平衡讨论
   - [ ] 局限性（预测≠因果）

4. **写一份审查报告** `ai_classification_review.md`，包含：
   - AI 原始报告（摘要，200 字）
   - 缺失项列表（按严重性分类：高/中/低）
   - 误解释或错误分析（如有）
   - 你的修订版结论（补充缺失诊断、修正错误解释）

5. **反思**（200 字）：
   - AI 在分类评估任务上表现如何？
   - 哪些评估项 AI 容易遗漏？
   - 数据泄漏问题是 AI 会主动提醒的吗？
   - 你认为人类必须检查哪些部分？

**评分标准**：
- 审查全面性（10 分）
- 问题分类合理（5 分）
- 修订版结论准确（5 分）

**审查清单示例**：
```
AI 报告审查结果:

缺失项（严重性：高）:
- ❌ 未与基线模型对比（无法判断模型是否优于"总是预测多数类"）
- ❌ 未检查数据泄漏（全局 StandardScaler 可能导致评估虚高）
- ❌ 未提及交叉验证（单次 train-test split 结果可能不稳定）

缺失项（严重性：中）:
- ⚠️ 未讨论阈值选择（默认 0.5 可能不是业务最优）
- ⚠️ 未分析类别不平衡（准确率在 85% 不流失的数据上会误导）

缺失项（严重性：低）:
- ℹ️ 未讨论业务成本（假阳性/假阴性成本）

可能的误解释:
- ⚠️ AI 说"合同期每增加 1 月，流失概率降低 5%"（错误：应该是优势比，不是概率）
```

---

## StatLab 集成

**要求**：本周所有基础作业的产出应整合到你的 StatLab 报告中

**具体操作**：

1. 在你的 `report.md` 中添加"分类评估"章节
2. 使用任务 7 的模板结构
3. 包含以下内容：
   - 逻辑回归系数表 + 优势比解释
   - 混淆矩阵 + 评估指标
   - ROC-AUC 分析 + 阈值选择
   - 交叉验证结果
   - Pipeline + ColumnTransformer 代码
   - 与基线对比
   - 局限性与因果警告

---

## 提交方式

1. 将所有文件放入 `chapters/week_10/assignment/` 目录
2. 文件命名规范：
   - `linear_vs_logistic.md`
   - `logistic_coefficient_interpretation.md`
   - `classification_evaluation_report.md`
   - `roc_auc_analysis.md`
   - `pipeline_data_leakage.md`（进阶）
   - `stratified_cv_analysis.md`（进阶）
   - `classification_analysis.py` + `report.md`（挑战）
   - `ai_original_classification_report.txt`（AI 协作）
   - `ai_classification_review.md`（AI 协作）
3. 确保所有 Markdown 文档使用清晰的章节结构

---

## 作业提示

1. **逻辑回归系数是优势比，不是概率变化**：永远说"优势增加/降低 X%"，不要说"概率增加/降低 X%"

2. **准确率在类别不平衡时会撒谎**：如果 85% 的样本是负类，一个"总是预测负类"的模型准确率就是 85%，但毫无价值

3. **混淆矩阵比准确率更重要**：它直接连接到业务成本（假阳性/假阴性）

4. **ROC-AUC 用于模型选择，F1 用于阈值调整**：AUC 帮你选模型，F1 帮你选阈值

5. **数据泄漏是最常见的工程陷阱**：永远用 Pipeline + ColumnTransformer，不要在交叉验证之前做全局预处理

6. **分类问题用 StratifiedKFold**：保持每个折的类别比例与整体一致

7. **永远与基线对比**：先打败"总是预测多数类"，再谈复杂模型

8. **AI 是加速器，不是决策者**：AI 可以拟合模型、计算 AUC，但防止泄漏、选择基线、解释业务价值的责任由你承担

9. **如果遇到困难**：可以参考 `starter_code/solution.py`（如果存在），但不要直接复制粘贴

---

## 常见错误

- ❌ 混淆"优势比"与"概率差"（逻辑回归系数解释错误）
- ❌ 只看准确率，忽略类别不平衡（准确率陷阱）
- ❌ 全局 StandardScaler 导致数据泄漏（评估虚高）
- ❌ 使用普通 KFold 而非 StratifiedKFold（类别分布失衡）
- ❌ 未与基线模型对比（无法判断模型价值）
- ❌ 忽略阈值选择（默认 0.5 可能不是业务最优）
- ❌ 直接把逻辑回归系数当成概率变化（应该是优势比）
- ❌ 只报告 AUC，不报告混淆矩阵（无法评估业务成本）
- ❌ 混淆"预测"与"因果"（预测≠因果）
