# Week 10：分类模型与逻辑回归——作业

> "预测很难，尤其是关于未来的预测。"
> —— 尼尔斯·玻尔

## 场景导入

小北刚完成电商平台的客户数据分析，业务方提出了一个新需求：**"能不能预测哪些客户会成为高价值客户？我们下个月要重点运营。"**

小北信心满满地说："没问题！我用逻辑回归来做二分类预测，准确率肯定很高！"

阿码在一旁追问："准确率真的靠谱吗？如果正负样本比例是 1:20，准确率 95% 也可能是陷阱啊。混淆矩阵、ROC 曲线这些都做了吗？"

老潘听到了，插了一句："分类模型不只是拟合。在公司里，我们提交分类报告时必须有完整的评估指标——精确率、查全率、F1、AUC，一个都不能少。漏了哪个，都可能让业务方做出错误决策。"

这正是本周作业的目标：用逻辑回归完成客户高价值分类任务，并产出一份**完整的分类评估报告**。

---

## 任务概述

你的任务是完成一个完整的**客户高价值分类预测**分析流程，包括数据探索、逻辑回归建模、混淆矩阵计算、分类指标评估、ROC 曲线绘制，以及针对类别不平衡问题的优化。

> **注意**：本章贯穿案例使用电商客户数据，目标变量为"是否高价值客户"（0=低价值，1=高价值）。

---

## 基础层（必做）

### 任务 1.1：逻辑回归建模（30分）

使用提供的电商客户数据集，完成以下步骤：

1. **数据准备**
   - 加载数据集 `customer_data.csv`
   - 选择特征：历史消费金额、注册时长、月均浏览次数、购物车添加次数
   - 划分训练集和测试集（分层抽样，保持类别比例）
   - 对特征进行标准化处理

2. **模型拟合**
   - 使用 `LogisticRegression` 拟合模型
   - 在测试集上进行预测（概率和类别）

3. **系数解读**
   - 输出各特征的系数
   - 解释系数的正负号含义（哪个特征最影响"高价值"概率？）

> **小北的疑问**："等等，为什么特征要做标准化？我不标准化也能跑啊……"
>
> **老潘的解释**："在公司我们从来不会跳过标准化。逻辑回归对特征尺度敏感，消费金额（几千块）和浏览次数（几十次）不在一个量级，系数大小就没法比较了。"

---

### 任务 1.2：混淆矩阵与分类指标（35分）

基于任务 1.1 的预测结果，计算并报告：

1. **混淆矩阵**
   - 计算 TP、FP、TN、FN
   - 用热力图可视化混淆矩阵

2. **分类指标计算**
   - 准确率（Accuracy）
   - 精确率（Precision）
   - 查全率（Recall）
   - F1 分数

3. **指标解读**
   - 对比准确率和 F1 分数的差异
   - 分析精确率和查全率哪个更高？为什么？

> **阿码的追问**："精确率和查全率哪个更重要？能不能两个都要？"
>
> **老潘的回答**："没有放之四海而皆准的答案。如果你的目标是'宁可多给优惠，也别漏掉高价值客户'，那就优先查全率；如果你的目标是'营销资源有限，必须精准投放'，那就优先精确率。"

---

### 任务 1.3：ROC 曲线与 AUC（35分）

1. **ROC 曲线绘制**
   - 计算不同阈值下的 TPR 和 FPR
   - 绘制 ROC 曲线
   - 标注 AUC 值

2. **AUC 解读**
   - 判断 AUC 值对应的模型表现等级
   - 给出模型改进建议

3. **阈值选择（可选）**
   - 找到最接近左上角的"最优"阈值
   - 比较默认阈值（0.5）与最优阈值的指标差异

> **老潘的点评**："没有 ROC 曲线的分类报告，是在自欺欺人。在公司里，我们提交的每份分类报告都必须包含 ROC 曲线——这是底线。"

---

## 进阶层（选做）

### 任务 2.1：类别不平衡处理（20分）

检查数据集中正负样本的比例：

1. **识别不平衡**
   - 计算正负样本数量
   - 如果不平衡比例超过 1:5，使用以下策略之一：
     - 使用 `class_weight='balanced'` 重新训练模型
     - 或使用 SMOTE 进行过采样

2. **对比评估**
   - 对比处理前后的混淆矩阵和 F1 分数
   - 分析哪种策略更适合当前场景

3. **指标选择讨论**
   - 在类别不平衡场景下，为什么 F1 比准确率更合适？

> **小北的发现**："我的数据里高价值客户只占 8%，低价值的占 92%！模型预测所有客户都是低价值，准确率也有 92%……这准确率毫无意义啊！"
>
> **老潘点头**："这就是准确率陷阱。你学会了看混淆矩阵，才不会被高准确率忽悠。"

---

### 任务 2.2：阈值优化 F1（15分）

不重新训练模型，仅通过调整决策阈值优化表现：

1. **计算不同阈值下的指标**
   - 遍历阈值 0.1 到 0.9（步长 0.05）
   - 计算每个阈值下的精确率、查全率、F1

2. **找到最优阈值**
   - 找到 F1 最大的阈值
   - 分析该阈值下的精确率和查全率

3. **业务场景讨论**
   - 如果业务目标是"高查全率"（宁可误报，不可漏报），应该选择什么阈值？
   - 如果业务目标是"高精确率"（宁可少报，不可误报），应该选择什么阈值？

> **阿码的思考**："原来阈值不是固定的 0.5！根据业务需求调整阈值，才能让模型真正'落地'。"

---

### 任务 2.3：对比不同模型的 AUC（15分）

对比逻辑回归与其他分类模型：

1. **模型对比**
   - 使用随机森林（RandomForest）训练分类模型
   - 使用决策树（DecisionTree）训练分类模型
   - 计算三个模型（逻辑回归、随机森林、决策树）的 AUC

2. **ROC 曲线对比图**
   - 在同一张图上绘制三个模型的 ROC 曲线
   - 标注各自的 AUC 值

3. **模型选择讨论**
   - 哪个模型的 AUC 最高？
   - 为什么逻辑回归在这个场景下可能更优？（可解释性、系数含义）

---

## 挑战层（选做）

### 任务 3.1：AI 协作练习——审查 AI 生成分类报告（20分）

**场景**：你让 AI（如 Kimi/ChatGPT）帮你生成了下面这份分类报告。作为数据分析师，你需要**审查**它：

```markdown
## AI 生成的分类报告

模型：逻辑回归
准确率：94.5%
结论：模型表现优秀，可以直接部署到生产环境。

建议：由于准确率很高，无需进一步优化。
```

**审查任务**：

1. **指标审查**
   - 列出这份报告缺少的关键指标（至少 3 个）
   - 解释为什么"只看准确率"可能是危险的

2. **逻辑审查**
   - 如果正负样本比例是 1:19，这份报告的结论有什么问题？
   - 你还需要哪些信息才能判断模型是否可用？

3. **改进建议**
   - 用 Markdown 重写这份报告，补充缺失的指标和诊断结论
   - 给出是否需要进一步优化的建议

> **老潘的忠告**："在公司里，我们不会直接把 AI 生成的报告提交上去。AI 可以帮你拟合模型、计算指标，但判断'这个模型是否公平、是否可用'，只有你能做。"

---

### 任务 3.2：公平性考量——设计包含公平性的分类任务（25分）

**场景**：你的模型需要预测"客户是否值得发放高额度信用卡"。你意识到，如果模型对某些群体（如性别、年龄、地域）有偏见，可能会造成不公平。

**任务**：

1. **数据拆分分析**
   - 将测试集按性别（或你选择的其他分组变量）拆分
   - 分别计算两组的：精确率、查全率、F1、AUC

2. **公平性评估**
   - 对比两组的指标差异
   - 如果某一组的 F1 明显低于另一组，说明什么问题？

3. **改进讨论**
   - 查阅资料（搜索"machine learning fairness metrics"），了解至少一种公平性指标（如 Demographic Parity、Equal Opportunity）
   - 讨论如何在实际项目中监控模型的公平性

4. **报告撰写**
   - 写一份简短的公平性评估报告（300-500 字）
   - 包含：方法、发现、建议

> **阿码的提问**："模型的公平性和准确性，哪个更重要？能不能两者兼得？"
>
> **思考提示**：参考本章"AI 时代小专栏"关于公平性的讨论。研究表明，当不同群体的基础比例不同时，没有任何模型能同时满足所有公平性定义——这是一个价值选择问题。

---

## 提交物

### 必需提交

1. **代码文件**：`starter_code/solution.py`
   - 包含基础层所有任务的完整代码
   - 代码需能直接运行，生成所有要求的图表和指标

2. **分类报告**：`output/classification_report.md`
   - 包含混淆矩阵表格
   - 包含所有分类指标（准确率、精确率、查全率、F1、AUC）
   - 包含 ROC 曲线图
   - 包含对模型表现的诊断结论

3. **运行验证**：
   ```bash
   python3 -m pytest chapters/week_10/tests -q
   python3 scripts/validate_week.py --week week_10 --mode release
   ```

### 进阶/挑战层提交（如完成）

4. **类别不平衡处理报告**（任务 2.1）：对比处理前后的指标差异
5. **阈值优化分析**（任务 2.2）：不同阈值下的指标变化图
6. **模型对比报告**（任务 2.3）：多模型 ROC 对比图
7. **AI 报告审查**（任务 3.1）：审查意见和改进版报告
8. **公平性评估报告**（任务 3.2）：分组公平性分析

---

## 评分标准

详见 `RUBRIC.md`。

| 等级 | 分数范围 | 描述 |
|------|---------|------|
| **Excellent** | 90-100 | 基础+进阶+挑战全部完成，有深度分析，代码规范 |
| **Good** | 75-89 | 基础+进阶完成，分析到位，代码可运行 |
| **Pass** | 60-74 | 基础层完成，核心指标计算正确 |
| **Needs Revision** | <60 | 基础层有缺失，需补充完善 |

---

## 检查清单（提交前自查）

- [ ] 逻辑回归模型已成功拟合，系数已解释
- [ ] 混淆矩阵已计算并可视化
- [ ] 精确率、查全率、F1 已计算
- [ ] ROC 曲线已绘制，AUC 已报告
- [ ] 分类报告 `output/classification_report.md` 已生成
- [ ] `pytest` 测试通过
- [ ] `validate_week.py` 验证通过
- [ ] （进阶）类别不平衡处理已完成并对比
- [ ] （进阶）阈值优化分析已完成
- [ ] （挑战）AI 报告审查已完成
- [ ] （挑战）公平性评估已完成（如选择）

---

## 资源与提示

### 核心库函数参考

```python
# 逻辑回归
from sklearn.linear_model import LogisticRegression

# 混淆矩阵与指标
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    precision_score, recall_score, f1_score, accuracy_score,
    classification_report
)

# ROC 曲线
from sklearn.metrics import roc_curve, auc, RocCurveDisplay

# 数据划分与预处理
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 类别不平衡处理（如需要）
from sklearn.utils.class_weight import compute_class_weight
# from imblearn.over_sampling import SMOTE  # 需安装 imblearn
```

### 常见问题

**Q1：为什么我的精确率和查全率差别很大？**
> 这是正常的。类别不平衡时，模型倾向于预测多数类，导致精确率和查全率不均衡。查看混淆矩阵，看是 FP 多还是 FN 多。

**Q2：AUC 很高但 F1 很低，怎么办？**
> 这说明模型排序能力不错（能区分正负样本），但默认阈值（0.5）不合适。尝试调整阈值，找到 F1 最大的点。

**Q3：ROC 曲线和 PR 曲线有什么区别？**
> ROC 曲线用 TPR-FPR，适合正负样本相对均衡的情况；PR 曲线用精确率-查全率，更适合类别不平衡场景。

---

## 角色寄语

> **小北**："我一开始以为分类就是跑个模型、看准确率。现在我知道，混淆矩阵才是真相，ROC 曲线才是综合能力考。分类模型不只是'准'，还要'公平'——对每一类都公平。"
>
> **阿码**："阈值调整太有意思了！原来同一个模型，换个阈值就能适应不同的业务需求。没有完美的模型，只有适合业务的模型。"
>
> **老潘**："在公司里，我们提交的每份分类报告都必须包含混淆矩阵和 ROC 曲线。这是底线。AI 可以帮你拟合模型，但只有你能判断'这个模型是否公平、是否可用'。"

---

*祝作业顺利！*
