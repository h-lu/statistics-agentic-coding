"""
ç¤ºä¾‹ï¼šStatLab é›†æˆâ€”â€”å› æœæ¨æ–­ç« èŠ‚ç”Ÿæˆ

æœ¬è„šæœ¬æ˜¯ StatLab è¶…çº§çº¿çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºåœ¨å¯å¤ç°åˆ†ææŠ¥å‘Šä¸­æ·»åŠ 
"å› æœæ¨æ–­"ç« èŠ‚ã€‚å®ƒæ‰§è¡Œå®Œæ•´çš„å› æœæ¨æ–­åˆ†ææµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- ç”»å› æœå›¾ï¼ˆDAGï¼‰
- è¯†åˆ«ç­–ç•¥ï¼ˆåé—¨å‡†åˆ™ï¼‰
- å› æœæ•ˆåº”ä¼°è®¡ï¼ˆå›å½’ + å€¾å‘è¯„åˆ†åŒ¹é…ï¼‰
- è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šç‰‡æ®µå’Œå›¾è¡¨

è¿è¡Œæ–¹å¼ï¼špython3 chapters/week_13/examples/05_statlab_causal.py
é¢„æœŸè¾“å‡ºï¼š
- æŠ¥å‘Šç‰‡æ®µï¼ˆè¿½åŠ åˆ° report.mdï¼‰
- å› æœå›¾ï¼ˆä¿å­˜åˆ° report/causal_dag.pngï¼‰
- å€¾å‘è¯„åˆ†åŒ¹é…å¯è§†åŒ–ï¼ˆä¿å­˜åˆ° report/psm_comparison.pngï¼‰

ä¾èµ–: éœ€è¦é¢„å…ˆæ¸…æ´—å¥½çš„æ•°æ®ï¼ˆå‡è®¾è·¯å¾„ä¸º data/clean_data.csvï¼‰

è¯´æ˜ï¼šæœ¬è„šæœ¬åœ¨ Week 09-12 åŸºç¡€ä¸Šå¢é‡ä¿®æ”¹ï¼Œå°†åˆ†æç›®æ ‡ä»
"é¢„æµ‹ä¸å…³è”"æ‰©å±•åˆ°"å› æœæ•ˆåº”ä¼°è®¡"ã€‚
"""
from __future__ import annotations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import NearestNeighbors
from scipy import stats
import networkx as nx

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def causal_inference_report(
    df: pd.DataFrame,
    treatment: str,
    outcome: str,
    confounders: List[str],
    output_dir: str = "report"
) -> str:
    """
    å¯¹æ•°æ®é›†è¿›è¡Œå®Œæ•´çš„å› æœæ¨æ–­åˆ†æï¼Œç”ŸæˆæŠ¥å‘Šç‰‡æ®µ

    å‚æ•°:
        df: æ¸…æ´—åçš„æ•°æ®
        treatment: å¤„ç†å˜é‡åï¼ˆå¦‚ 'coupon_used'ï¼‰
        outcome: ç»“æœå˜é‡åï¼ˆå¦‚ 'spending'ï¼‰
        confounders: æ··æ‚å˜é‡åˆ—è¡¨ï¼ˆå¦‚ ['activity', 'history_spend']ï¼‰
        output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•

    è¿”å›:
        Markdown æ ¼å¼çš„æŠ¥å‘Šç‰‡æ®µ
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    print("=" * 70)
    print("StatLab å› æœæ¨æ–­æŠ¥å‘Šç”Ÿæˆå™¨")
    print("=" * 70)
    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"  å¤„ç†å˜é‡: {treatment}")
    print(f"  ç»“æœå˜é‡: {outcome}")
    print(f"  æ··æ‚å˜é‡: {', '.join(confounders)}")

    # ========== 1. ç”»å› æœå›¾ ==========
    print(f"\nâœ… æ­¥éª¤ 1: ç”»å› æœå›¾ï¼ˆDAGï¼‰...")

    dag_path = output_path / "causal_dag.png"
    plot_causal_dag(treatment, outcome, confounders, dag_path)

    print(f"  âœ… å› æœå›¾å·²ä¿å­˜: {dag_path}")

    # ========== 2. è¯†åˆ«ç­–ç•¥ï¼ˆåé—¨å‡†åˆ™ï¼‰ ==========
    print(f"\nâœ… æ­¥éª¤ 2: è¯†åˆ«åé—¨è·¯å¾„...")

    backdoor_paths = identify_backdoor_paths(treatment, outcome, confounders)

    # ========== 3. æœªè°ƒæ•´çš„ä¼°è®¡ï¼ˆå°åŒ—çš„é”™è¯¯ï¼‰ ==========
    print(f"\nâœ… æ­¥éª¤ 3: è®¡ç®—æœªè°ƒæ•´çš„ä¼°è®¡...")

    treated = df[df[treatment] == 1][outcome].mean()
    control = df[df[treatment] == 0][outcome].mean()
    naive_effect = treated - control

    print(f"  ç”¨åˆ¸ç»„å¹³å‡: {treated:.2f}")
    print(f"  å¯¹ç…§ç»„å¹³å‡: {control:.2f}")
    print(f"  æœªè°ƒæ•´å·®å¼‚: {naive_effect:.2f}")

    # ========== 4. å›å½’ä¼°è®¡ ==========
    print(f"\nâœ… æ­¥éª¤ 4: å¸¦è°ƒæ•´é›†çš„å›å½’...")

    X = df[[treatment] + confounders]
    y = df[outcome]

    reg_model = LinearRegression()
    reg_model.fit(X, y)

    reg_coef = reg_model.coef_[0]
    reg_intercept = reg_model.intercept_

    # è®¡ç®—æ ‡å‡†è¯¯å·®å’Œç½®ä¿¡åŒºé—´
    n = len(df)
    k = len(confounders) + 1
    y_pred = reg_model.predict(X)
    residuals = y - y_pred
    mse = np.sum(residuals**2) / (n - k - 1)

    X_with_intercept = np.column_stack([np.ones(n), X.values])
    cov_matrix = mse * np.linalg.inv(X_with_intercept.T @ X_with_intercept)
    se_treatment = np.sqrt(cov_matrix[1, 1])

    t_stat = reg_coef / se_treatment
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - k - 1))
    ci_low = reg_coef - 1.96 * se_treatment
    ci_high = reg_coef + 1.96 * se_treatment

    print(f"  å›å½’ç³»æ•°: {reg_coef:.2f}")
    print(f"  95% CI: [{ci_low:.2f}, {ci_high:.2f}]")
    print(f"  p å€¼: {p_value:.4f}")

    # ========== 5. å€¾å‘è¯„åˆ†åŒ¹é… ==========
    print(f"\nâœ… æ­¥éª¤ 5: å€¾å‘è¯„åˆ†åŒ¹é…...")

    # ä¼°è®¡å€¾å‘è¯„åˆ†
    ps_model = LogisticRegression(random_state=42)
    ps_model.fit(df[confounders], df[treatment])
    df_ps = df.copy()
    df_ps['propensity_score'] = ps_model.predict_proba(df[confounders])[:, 1]

    # åŒ¹é…
    treated_df = df_ps[df_ps[treatment] == 1].copy()
    control_df = df_ps[df_ps[treatment] == 0].copy()

    nn = NearestNeighbors(n_neighbors=1)
    nn.fit(control_df[['propensity_score']])
    distances, indices = nn.kneighbors(treated_df[['propensity_score']])
    matched_control = control_df.iloc[indices.flatten()].copy()

    # è®¡ç®— ATT
    att = (treated_df[outcome].values - matched_control[outcome].values).mean()

    # Bootstrap CI
    n_boot = 500
    att_samples = []
    for i in range(n_boot):
        treated_boot = treated_df.sample(n=len(treated_df), replace=True)
        control_boot = control_df.sample(n=len(control_df), replace=True)

        nn_boot = NearestNeighbors(n_neighbors=1)
        nn_boot.fit(control_boot[['propensity_score']])
        _, indices_boot = nn_boot.kneighbors(treated_boot[['propensity_score']])
        matched_boot = control_boot.iloc[indices_boot.flatten()]

        att_boot = (treated_boot[outcome].values - matched_boot[outcome].values).mean()
        att_samples.append(att_boot)

    att_ci_low = np.percentile(att_samples, 2.5)
    att_ci_high = np.percentile(att_samples, 97.5)

    print(f"  ATT: {att:.2f}")
    print(f"  95% CI (Bootstrap): [{att_ci_low:.2f}, {att_ci_high:.2f}]")

    # ========== 6. å€¾å‘è¯„åˆ†å¯è§†åŒ– ==========
    print(f"\nâœ… æ­¥éª¤ 6: ä¿å­˜å€¾å‘è¯„åˆ†å¯è§†åŒ–...")

    psm_path = output_path / "psm_comparison.png"
    plot_psm_comparison(
        treated_df['propensity_score'].values,
        control_df['propensity_score'].values,
        matched_control['propensity_score'].values,
        psm_path
    )
    print(f"  âœ… å¯è§†åŒ–å·²ä¿å­˜: {psm_path}")

    # ========== 7. ç”ŸæˆæŠ¥å‘Š ==========
    print(f"\nâœ… æ­¥éª¤ 7: ç”ŸæˆæŠ¥å‘Šç‰‡æ®µ...")

    report = generate_report_markdown(
        treatment=treatment,
        outcome=outcome,
        confounders=confounders,
        backdoor_paths=backdoor_paths,
        naive_effect=naive_effect,
        treated=treated,
        control=control,
        reg_coef=reg_coef,
        reg_ci_low=ci_low,
        reg_ci_high=ci_high,
        reg_p=p_value,
        att=att,
        att_ci_low=att_ci_low,
        att_ci_high=att_ci_high,
        dag_path=dag_path,
        psm_path=psm_path,
        n_total=len(df)
    )

    print(f"  âœ… æŠ¥å‘Šç‰‡æ®µç”Ÿæˆå®Œæˆ")

    print("\n" + "=" * 70)
    print("âœ… å› æœæ¨æ–­åˆ†æå®Œæˆï¼")
    print("=" * 70)

    return report


def plot_causal_dag(treatment: str, outcome: str, confounders: List[str], output_path: Path):
    """ç”»å› æœå›¾"""
    G = nx.DiGraph()

    # æ·»åŠ è¾¹
    for conf in confounders:
        G.add_edge(conf, treatment)
        G.add_edge(conf, outcome)
    G.add_edge(treatment, outcome)

    # å¸ƒå±€
    pos = {}
    pos[treatment] = (1, 0)
    pos[outcome] = (2, 0)

    for i, conf in enumerate(confounders):
        pos[conf] = (0, (i - len(confounders) / 2) * 0.5)

    # ç”»å›¾
    plt.figure(figsize=(10, 6))
    nx.draw_networkx_nodes(G, pos, node_color='lightblue',
                          node_size=3000, alpha=0.9)
    nx.draw_networkx_edges(G, pos, edge_color='gray',
                          arrowsize=20, width=2, alpha=0.7)
    nx.draw_networkx_labels(G, pos, font_size=12,
                            font_family='sans-serif')
    plt.title("å› æœå›¾ï¼ˆDAGï¼‰", fontsize=14, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()


def plot_psm_comparison(treated_ps, control_ps, matched_ps, output_path: Path):
    """ç”»å€¾å‘è¯„åˆ†åŒ¹é…å‰åçš„å¯¹æ¯”å›¾"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # åŒ¹é…å‰
    axes[0].hist(treated_ps, alpha=0.5, label='å¤„ç†ç»„', bins=20, color='blue')
    axes[0].hist(control_ps, alpha=0.5, label='å¯¹ç…§ç»„', bins=20, color='red')
    axes[0].set_xlabel('å€¾å‘è¯„åˆ†', fontsize=12)
    axes[0].set_ylabel('æ ·æœ¬æ•°', fontsize=12)
    axes[0].set_title('åŒ¹é…å‰ï¼šå€¾å‘è¯„åˆ†åˆ†å¸ƒå·®å¼‚å¤§', fontsize=12, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)

    # åŒ¹é…å
    axes[1].hist(treated_ps, alpha=0.5, label='å¤„ç†ç»„', bins=20, color='blue')
    axes[1].hist(matched_ps, alpha=0.5, label='åŒ¹é…çš„å¯¹ç…§ç»„', bins=20, color='green')
    axes[1].set_xlabel('å€¾å‘è¯„åˆ†', fontsize=12)
    axes[1].set_ylabel('æ ·æœ¬æ•°', fontsize=12)
    axes[1].set_title('åŒ¹é…åï¼šå€¾å‘è¯„åˆ†åˆ†å¸ƒæ¥è¿‘', fontsize=12, fontweight='bold')
    axes[1].legend(fontsize=11)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()


def identify_backdoor_paths(treatment: str, outcome: str, confounders: List[str]) -> List[str]:
    """è¯†åˆ«åé—¨è·¯å¾„"""
    paths = []
    for conf in confounders:
        path = f"{treatment} â† {conf} â†’ {outcome}"
        paths.append(path)
    return paths


def generate_report_markdown(
    treatment: str, outcome: str, confounders: List[str],
    backdoor_paths: List[str],
    naive_effect: float, treated: float, control: float,
    reg_coef: float, reg_ci_low: float, reg_ci_high: float, reg_p: float,
    att: float, att_ci_low: float, att_ci_high: float,
    dag_path: Path, psm_path: Path,
    n_total: int
) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Šç‰‡æ®µ"""

    confounder_list = "ã€".join(confounders)

    report = f"""
## å› æœæ¨æ–­

### ç ”ç©¶é—®é¢˜

æœ¬ç« å›ç­”çš„å› æœé—®é¢˜æ˜¯ï¼š

**"å¦‚æœç»™ç”¨æˆ·å‘æ”¾{treatment}ï¼Œ{outcome}ä¼šæé«˜å¤šå°‘ï¼Ÿ"**

æ³¨æ„ï¼šè¿™ä¸å…³è”é—®é¢˜ä¸åŒã€‚å…³è”é—®é¢˜æ˜¯"ç”¨åˆ¸ç”¨æˆ·å’Œæœªç”¨åˆ¸ç”¨æˆ·çš„{outcome}å·®å¼‚"ï¼Œè€Œå› æœé—®é¢˜æ˜¯"å‘åˆ¸è¿™ä¸ªè¡Œä¸ºçš„å› æœæ•ˆåº”"ã€‚

### å› æœå‡è®¾

æˆ‘ä»¬ç”¨å› æœå›¾ï¼ˆDAGï¼‰è¡¨è¾¾å› æœå‡è®¾ï¼š

![å› æœå›¾]({dag_path.name})

**å›¾è§£**ï¼š
- **å¤„ç†å˜é‡ï¼ˆXï¼‰**ï¼š{treatment}ï¼ˆ0=æœªä½¿ç”¨ï¼Œ1=ä½¿ç”¨ï¼‰
- **ç»“æœå˜é‡ï¼ˆYï¼‰**ï¼š{outcome}
- **æ··æ‚å˜é‡ï¼ˆZï¼‰**ï¼š{confounder_list}ï¼ˆåŒæ—¶å½±å“ç”¨åˆ¸å’Œæ¶ˆè´¹ï¼‰
- **å› æœè·¯å¾„**ï¼š{treatment} â†’ {outcome}ï¼ˆæˆ‘ä»¬æƒ³ä¼°è®¡çš„æ•ˆåº”ï¼‰

### è¯†åˆ«ç­–ç•¥

æ ¹æ®**åé—¨å‡†åˆ™ï¼ˆBackdoor Criterionï¼‰**ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´ä»¥ä¸‹æ··æ‚å˜é‡ï¼š

"""

    for path in backdoor_paths:
        report += f"- **{path}**ï¼šè™šå‡å…³è”è·¯å¾„\n"

    report += f"""
**è°ƒæ•´ç†ç”±**ï¼š
- {confounder_list}åŒæ—¶å½±å“"{treatment}"å’Œ"{outcome}"
- ä¸è°ƒæ•´è¿™äº›å˜é‡ï¼Œä¼šæŠŠæ··æ‚å˜é‡çš„æ•ˆåº”å½’åŠŸäºå¤„ç†

**ä¸è°ƒæ•´çš„å˜é‡**ï¼š
- ä¸­ä»‹å˜é‡ï¼ˆå¦‚ä½¿ç”¨é¢‘ç‡ï¼‰ï¼š{treatment}é€šè¿‡å½±å“ä¸­ä»‹å˜é‡å½±å“{outcome}ï¼Œè°ƒæ•´å®ƒä¼šåˆ‡æ–­å› æœè·¯å¾„

### å› æœæ•ˆåº”ä¼°è®¡

æˆ‘ä»¬ç”¨ä¸¤ç§æ–¹æ³•ä¼°è®¡å› æœæ•ˆåº”ï¼Œä»¥æ£€æŸ¥ç¨³å¥æ€§ã€‚

#### æ–¹æ³• 1ï¼šå¸¦è°ƒæ•´é›†çš„å›å½’

| æŒ‡æ ‡ | ä¼°è®¡å€¼ | 95% CI | p å€¼ |
|------|--------|--------|------|
| {treatment} çš„å› æœæ•ˆåº” | **{reg_coef:.2f}** | [{reg_ci_low:.2f}, {reg_ci_high:.2f}] | {reg_p:.4f} |

**è§£è¯»**ï¼šåœ¨æ§åˆ¶äº†{confounder_list}åï¼Œ{treatment}å¯¹{outcome}çš„å› æœæ•ˆåº”ä¸º**{reg_coef:.2f}**ï¼ˆ95% CI [{reg_ci_low:.2f}, {reg_ci_high:.2f}]ï¼‰ã€‚

#### æ–¹æ³• 2ï¼šå€¾å‘è¯„åˆ†åŒ¹é…

åŒ¹é…è´¨é‡æ£€æŸ¥ï¼š

![å€¾å‘è¯„åˆ†åˆ†å¸ƒï¼ˆåŒ¹é…å‰åï¼‰]({psm_path.name})

åŒ¹é…åçš„å› æœæ•ˆåº”ï¼š

| æŒ‡æ ‡ | ä¼°è®¡å€¼ | 95% CI |
|------|--------|--------|
| **ATTï¼ˆå¤„ç†ç»„å¹³å‡å¤„ç†æ•ˆåº”ï¼‰** | **{att:.2f}** | [{att_ci_low:.2f}, {att_ci_high:.2f}] |

**è§£è¯»**ï¼šå€¾å‘è¯„åˆ†åŒ¹é…ä¼°è®¡çš„å› æœæ•ˆåº”ä¸º**{att:.2f}**ï¼ˆ95% CI [{att_ci_low:.2f}, {att_ci_high:.2f}]ï¼‰ï¼Œä¸å›å½’ç»“æœæ¥è¿‘ï¼Œç»“è®ºç¨³å¥ã€‚

### æ··æ‚åå·®å¯¹æ¯”

| æ–¹æ³• | ä¼°è®¡å€¼ | è¯´æ˜ |
|------|--------|------|
| **æœªè°ƒæ•´ï¼ˆå°åŒ—çš„é”™è¯¯ï¼‰** | {naive_effect:.2f} | ç›´æ¥æ¯”è¾ƒå‡å€¼ï¼Œè¢«æ··æ‚å¤¸å¤§ |
| **å›å½’ï¼ˆå¸¦è°ƒæ•´é›†ï¼‰** | {reg_coef:.2f} | æ§åˆ¶æ··æ‚åçš„å› æœæ•ˆåº” |
| **å€¾å‘è¯„åˆ†åŒ¹é…** | {att:.2f} | åŒ¹é…ç›¸ä¼¼æ ·æœ¬åçš„å› æœæ•ˆåº” |

**ç»“è®º**ï¼šæœªè°ƒæ•´çš„ä¼°è®¡è¢«å¤¸å¤§äº†{naive_effect - reg_coef:.2f}å…ƒï¼Œè°ƒæ•´åçœŸå®çš„å› æœæ•ˆåº”çº¦ä¸º{(reg_coef + att) / 2:.2f}å…ƒã€‚

### ç»“è®ºè¾¹ç•Œ

**æˆ‘ä»¬èƒ½å›ç­”çš„ï¼ˆå› æœç»“è®ºï¼‰**ï¼š
- {treatment}å¯¹{outcome}çš„å› æœæ•ˆåº”çº¦ä¸º**{(reg_coef + att) / 2:.2f} Â± {abs(reg_coef - att) / 2:.2f}**å…ƒï¼ˆä¸¤ç§æ–¹æ³•çš„å¹³å‡ï¼‰
- è¿™ä¸ªç»“è®ºåœ¨è°ƒæ•´äº†æ··æ‚å˜é‡ï¼ˆ{confounder_list}ï¼‰åæˆç«‹
- 95% ç½®ä¿¡åŒºé—´ä¸åŒ…å«é›¶ï¼Œæ•ˆåº”ç»Ÿè®¡æ˜¾è‘—

**æˆ‘ä»¬ä¸èƒ½å›ç­”çš„ï¼ˆåªæ˜¯ç›¸å…³æˆ–æœªçŸ¥ï¼‰**ï¼š
- ä¸ªä½“å› æœæ•ˆåº”ï¼ˆåäº‹å®ï¼‰ï¼š"å¦‚æœå¼ ä¸‰æ²¡ç”¨åˆ¸ï¼Œä»–ä¼šæ¶ˆè´¹å¤šå°‘"æ˜¯ä¸ªä½“åäº‹å®ï¼Œæ— æ³•ç›´æ¥è§‚æµ‹
- é•¿æœŸæ•ˆåº”ï¼šæ•°æ®åªæœ‰å½“å‰æ—¶é—´èŒƒå›´ï¼Œæ— æ³•å›ç­”æ›´é•¿æœŸçš„æ•ˆåº”
- æ•ˆåº”å¼‚è´¨æ€§ï¼šæˆ‘ä»¬ä¼°è®¡çš„æ˜¯å¹³å‡æ•ˆåº”ï¼Œä¸åŒäººç¾¤çš„æ•ˆåº”å¯èƒ½ä¸åŒ

**é™åˆ¶**ï¼š
- å­˜åœ¨æœªè§‚å¯Ÿæ··æ‚çš„å¯èƒ½ï¼ˆå¦‚ç”¨æˆ·æ”¶å…¥ï¼Œå¦‚æœæ•°æ®ä¸­æ²¡æœ‰ï¼‰
- å€¾å‘è¯„åˆ†åŒ¹é…ä¼šä¸¢å¼ƒæ— æ³•åŒ¹é…çš„æ ·æœ¬ï¼ˆå¯èƒ½å½±å“å¤–æ¨æ€§ï¼‰
- å›å½’å‡è®¾çº¿æ€§å…³ç³»ï¼Œå¦‚æœçœŸå®å…³ç³»éçº¿æ€§ï¼Œä¼°è®¡å¯èƒ½æœ‰åå·®

### å·¥ç¨‹å®è·µ

æœ¬åˆ†æä½¿ç”¨äº†ä»¥ä¸‹æœ€ä½³å®è·µï¼š
- **å…ˆç”»å› æœå›¾**ï¼šæ˜ç¡®å‡è®¾ï¼Œå¯è§†åŒ–å˜é‡å…³ç³»
- **åé—¨å‡†åˆ™**ï¼šç§‘å­¦åœ°é€‰æ‹©è°ƒæ•´é›†ï¼Œä¸ç›²ç›®è°ƒæ•´ä¸€åˆ‡
- **ä¸¤ç§æ–¹æ³•éªŒè¯**ï¼šå›å½’ + åŒ¹é…ï¼Œç»“æœæ¥è¿‘åˆ™ç»“è®ºç¨³å¥
- **Bootstrap ç½®ä¿¡åŒºé—´**ï¼šé‡åŒ–ä¸ç¡®å®šæ€§ï¼Œä¸ä¾èµ–åˆ†å¸ƒå‡è®¾

### æ•°æ®æ¥æº
- **æ ·æœ¬é‡**ï¼šn = {n_total}
- **åˆ†ææ—¥æœŸ**ï¼š2026-02-13
- **éšæœºç§å­**ï¼š42ï¼ˆä¿è¯å¯å¤ç°ï¼‰

---

"""
    return report


# ============================================================================
# ç¤ºä¾‹ä½¿ç”¨ï¼ˆç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºï¼‰
# ============================================================================

def demo_with_mock_data():
    """ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºå®Œæ•´æµç¨‹"""
    print("\n" + "=" * 70)
    print("StatLab å› æœæ¨æ–­æŠ¥å‘Šç”Ÿæˆå™¨ - æ¼”ç¤ºæ¨¡å¼")
    print("=" * 70)

    # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆä¼˜æƒ åˆ¸æ¡ˆä¾‹ï¼‰
    np.random.seed(42)
    n = 1000

    # æ··æ‚å˜é‡
    activity = np.random.normal(50, 15, n)
    history_spend = np.random.normal(100, 30, n)

    # å¤„ç†å˜é‡
    coupon_prob = 0.2 + 0.006 * activity + 0.002 * history_spend
    coupon = np.random.binomial(1, np.clip(coupon_prob, 0, 1))

    # ç»“æœå˜é‡ï¼ˆçœŸå®æ•ˆåº” = 30 å…ƒï¼‰
    spending = (
        50 + 1.5 * activity + 0.3 * history_spend +
        30 * coupon + np.random.normal(0, 15, n)
    )

    df = pd.DataFrame({
        'ç”¨æˆ·æ´»è·ƒåº¦': activity,
        'å†å²æ¶ˆè´¹': history_spend,
        'ä¼˜æƒ åˆ¸ä½¿ç”¨': coupon,
        'æ¶ˆè´¹é‡‘é¢': spending
    })

    print(f"\nğŸ“Š æ¨¡æ‹Ÿæ•°æ®æ¦‚è§ˆ:")
    print(df.head(10))
    print(f"\nç”¨åˆ¸ç‡: {coupon.mean():.1%}")
    print(f"å¹³å‡æ¶ˆè´¹: {spending.mean():.2f} å…ƒ")

    # 2. è¿è¡Œå› æœæ¨æ–­åˆ†æ
    report = causal_inference_report(
        df=df,
        treatment='ä¼˜æƒ åˆ¸ä½¿ç”¨',
        outcome='æ¶ˆè´¹é‡‘é¢',
        confounders=['ç”¨æˆ·æ´»è·ƒåº¦', 'å†å²æ¶ˆè´¹'],
        output_dir='report'
    )

    # 3. æ‰“å°æŠ¥å‘Š
    print("\n" + "=" * 70)
    print("ç”Ÿæˆçš„æŠ¥å‘Šç‰‡æ®µ:")
    print("=" * 70)
    print(report)

    # 4. ä¿å­˜åˆ°æ–‡ä»¶
    output_path = Path("report")
    output_path.mkdir(exist_ok=True)

    report_file = output_path / "causal_inference_report.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("# å› æœæ¨æ–­ - StatLab ç¤ºä¾‹æŠ¥å‘Š\n\n")
        f.write(report)

    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(f"âœ… å› æœå›¾å·²ä¿å­˜åˆ°: {output_path}/causal_dag.png")
    print(f"âœ… å€¾å‘è¯„åˆ†åŒ¹é…å›¾å·²ä¿å­˜åˆ°: {output_path}/psm_comparison.png")

    return report


def main():
    """ä¸»å‡½æ•°"""
    # æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    report = demo_with_mock_data()

    print("\n" + "=" * 70)
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜")
    print("=" * 70)
    print("""
åœ¨ä½ çš„ StatLab é¡¹ç›®ä¸­ä½¿ç”¨æœ¬è„šæœ¬çš„æ­¥éª¤ï¼š

1. æ›¿æ¢æ•°æ®æº:
   df = pd.read_csv("data/clean_data.csv")

2. æŒ‡å®šä½ çš„å¤„ç†å˜é‡ã€ç»“æœå˜é‡å’Œæ··æ‚å˜é‡:
   treatment = "your_treatment_variable"    # å¦‚ 'coupon_used'
   outcome = "your_outcome_variable"        # å¦‚ 'spending'
   confounders = ["conf1", "conf2", ...]   # æ··æ‚å˜é‡åˆ—è¡¨

3. è¿è¡Œå‡½æ•°ç”ŸæˆæŠ¥å‘Š:
   report = causal_inference_report(
       df, treatment, outcome, confounders, "report"
   )

4. å°†ç”Ÿæˆçš„æŠ¥å‘Šç‰‡æ®µè¿½åŠ åˆ° report.md

æœ¬å‘¨ StatLab çš„æ”¹è¿›ï¼ˆç›¸æ¯”ä¸Šå‘¨ï¼‰:
- æ–°å¢å› æœå›¾ï¼ˆDAGï¼‰å¯è§†åŒ–
- æ–°å¢è¯†åˆ«ç­–ç•¥ï¼ˆåé—¨å‡†åˆ™ï¼‰
- æ–°å¢å› æœæ•ˆåº”ä¼°è®¡ï¼ˆå›å½’ + åŒ¹é…ï¼‰
- æ˜ç¡®åŒºåˆ†"å› æœç»“è®º"å’Œ"ç›¸å…³å‘ç°"
- å†™æ¸…ç»“è®ºè¾¹ç•Œï¼ˆèƒ½å›ç­”ä»€ä¹ˆã€ä¸èƒ½å›ç­”ä»€ä¹ˆï¼‰

è¿™æ˜¯ä»"é¢„æµ‹/å…³è”"åˆ°"å› æœæ¨æ–­"çš„å…³é”®è·ƒè¿ï¼
    """)


if __name__ == "__main__":
    main()
