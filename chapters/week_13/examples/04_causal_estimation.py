"""
ç¤ºä¾‹ï¼šå› æœæ•ˆåº”ä¼°è®¡â€”â€”ä»è¯†åˆ«åˆ°æ•°å€¼

æœ¬ä¾‹æ¼”ç¤ºä¸¤ç§å¸¸ç”¨çš„å› æœæ•ˆåº”ä¼°è®¡æ–¹æ³•ï¼š
1. å¸¦åé—¨è°ƒæ•´é›†çš„å›å½’ï¼ˆç®€å•å¿«é€Ÿï¼‰
2. å€¾å‘è¯„åˆ†åŒ¹é…ï¼ˆPropensity Score Matchingï¼Œçµæ´»å¯æ£€æŸ¥ï¼‰

è¿è¡Œæ–¹å¼ï¼špython3 chapters/week_13/examples/04_causal_estimation.py
é¢„æœŸè¾“å‡ºï¼š
- stdout è¾“å‡ºä¸¤ç§æ–¹æ³•çš„ä¼°è®¡ç»“æœ
- ä¿å­˜å€¾å‘è¯„åˆ†åŒ¹é…å¯è§†åŒ–å›¾ï¼ˆpsm_comparison.pngï¼‰
"""
from __future__ import annotations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import NearestNeighbors

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def generate_coupon_data(n=1000, seed=42):
    """
    ç”Ÿæˆä¼˜æƒ åˆ¸æ¨¡æ‹Ÿæ•°æ®

    çœŸå®å› æœæ•ˆåº”ï¼š30 å…ƒ

    ç»“æ„:
      æ´»è·ƒåº¦ â†’ ä¼˜æƒ åˆ¸ â†’ æ¶ˆè´¹
           â†˜          â†—
           å†å²æ¶ˆè´¹
    """
    np.random.seed(seed)

    # æ··æ‚å˜é‡
    activity = np.random.normal(50, 15, n)
    history_spend = np.random.normal(100, 30, n)

    # å¤„ç†å˜é‡ï¼ˆå—æ··æ‚å½±å“ï¼‰
    coupon_prob = 0.2 + 0.006 * activity + 0.002 * history_spend
    coupon = np.random.binomial(1, np.clip(coupon_prob, 0, 1))

    # ç»“æœå˜é‡ï¼ˆå—æ··æ‚å’Œå¤„ç†å½±å“ï¼‰
    # çœŸå®å› æœæ•ˆåº” = 30 å…ƒ
    spending = (
        50 +                      # åŸºç¡€æ¶ˆè´¹
        1.5 * activity +         # æ´»è·ƒåº¦å½±å“
        0.3 * history_spend +    # å†å²æ¶ˆè´¹å½±å“
        30 * coupon +             # ä¼˜æƒ åˆ¸å› æœæ•ˆåº”
        np.random.normal(0, 15, n)  # å™ªå£°
    )

    df = pd.DataFrame({
        'ç”¨æˆ·æ´»è·ƒåº¦': activity,
        'å†å²æ¶ˆè´¹': history_spend,
        'ä¼˜æƒ åˆ¸ä½¿ç”¨': coupon,
        'æ¶ˆè´¹é‡‘é¢': spending
    })

    return df


def method_1_regression_adjustment(df):
    """
    æ–¹æ³• 1ï¼šå¸¦åé—¨è°ƒæ•´é›†çš„å›å½’

    æœ€ç®€å•çš„æ–¹æ³•â€”â€”ç”¨å›å½’æ§åˆ¶æ··æ‚å˜é‡
    """
    print("\n" + "=" * 70)
    print("ğŸ“Š æ–¹æ³• 1ï¼šå¸¦åé—¨è°ƒæ•´é›†çš„å›å½’")
    print("=" * 70)

    print("\nåŸç†:")
    print("  åœ¨å›å½’æ–¹ç¨‹ä¸­åŒ…å«æ··æ‚å˜é‡ï¼Œ")
    print("  å¤„ç†å˜é‡çš„ç³»æ•°å°±æ˜¯è°ƒæ•´åçš„å› æœæ•ˆåº”")

    print("\nå›å½’æ–¹ç¨‹:")
    print("  æ¶ˆè´¹é‡‘é¢ = Î²0 + Î²1Ã—ä¼˜æƒ åˆ¸ + Î²2Ã—æ´»è·ƒåº¦ + Î²3Ã—å†å²æ¶ˆè´¹ + Îµ")

    # æ‹Ÿåˆæ¨¡å‹
    X = df[['ä¼˜æƒ åˆ¸ä½¿ç”¨', 'ç”¨æˆ·æ´»è·ƒåº¦', 'å†å²æ¶ˆè´¹']]
    y = df['æ¶ˆè´¹é‡‘é¢']

    model = LinearRegression()
    model.fit(X, y)

    # æå–ç»“æœ
    coef_coupon = model.coef_[0]
    coef_activity = model.coef_[1]
    coef_history = model.coef_[2]
    intercept = model.intercept_

    # è®¡ç®—æ ‡å‡†è¯¯å·®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    from scipy import stats
    n = len(df)
    k = 3  # è‡ªå˜é‡æ•°é‡
    y_pred = model.predict(X)
    residuals = y - y_pred
    mse = np.sum(residuals**2) / (n - k - 1)

    # ç³»æ•°çš„åæ–¹å·®çŸ©é˜µï¼ˆç®€åŒ–ï¼‰
    X_with_intercept = np.column_stack([np.ones(n), X.values])
    cov_matrix = mse * np.linalg.inv(X_with_intercept.T @ X_with_intercept)
    se_coupon = np.sqrt(cov_matrix[1, 1])

    # t æ£€éªŒ
    t_stat = coef_coupon / se_coupon
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - k - 1))

    # 95% ç½®ä¿¡åŒºé—´
    ci_low = coef_coupon - 1.96 * se_coupon
    ci_high = coef_coupon + 1.96 * se_coupon

    print("\nç»“æœ:")
    print("-" * 70)
    print(f"æˆªè·: {intercept:.2f}")
    print(f"ä¼˜æƒ åˆ¸ç³»æ•°: {coef_coupon:.2f} å…ƒ (SE: {se_coupon:.2f})")
    print(f"æ´»è·ƒåº¦ç³»æ•°: {coef_activity:.2f}")
    print(f"å†å²æ¶ˆè´¹ç³»æ•°: {coef_history:.2f}")
    print(f"\nå› æœæ•ˆåº”ä¼°è®¡:")
    print(f"  ä¼˜æƒ åˆ¸ â†’ æ¶ˆè´¹é‡‘é¢: {coef_coupon:.2f} å…ƒ")
    print(f"  95% CI: [{ci_low:.2f}, {ci_high:.2f}]")
    print(f"  t å€¼: {t_stat:.2f}")
    print(f"  p å€¼: {p_value:.4f}")

    # å¯¹æ¯”çœŸå®å€¼
    true_effect = 30
    print(f"\nå¯¹æ¯”çœŸå®å€¼:")
    print(f"  çœŸå®æ•ˆåº”: {true_effect:.2f} å…ƒ")
    print(f"  ä¼°è®¡è¯¯å·®: {abs(coef_coupon - true_effect):.2f} å…ƒ")

    return {
        'method': 'å›å½’ï¼ˆå¸¦è°ƒæ•´é›†ï¼‰',
        'effect': coef_coupon,
        'ci_low': ci_low,
        'ci_high': ci_high,
        'p_value': p_value
    }


def method_2_psm(df):
    """
    æ–¹æ³• 2ï¼šå€¾å‘è¯„åˆ†åŒ¹é…ï¼ˆPropensity Score Matchingï¼‰

    æ ¸å¿ƒæ€æƒ³ï¼šå¦‚æœä¸¤ä¸ªç”¨æˆ·å€¾å‘è¯„åˆ†ç›¸è¿‘ï¼ˆç‰¹å¾ç›¸ä¼¼ï¼‰ï¼Œä½†ä¸€ä¸ªç”¨åˆ¸ã€ä¸€ä¸ªä¸ç”¨ï¼Œ
    é‚£ä»–ä»¬çš„å·®å¼‚å°±æ˜¯å› æœæ•ˆåº”ã€‚
    """
    print("\n" + "=" * 70)
    print("ğŸ¯ æ–¹æ³• 2ï¼šå€¾å‘è¯„åˆ†åŒ¹é…ï¼ˆPSMï¼‰")
    print("=" * 70)

    print("\nåŸç†:")
    print("  1. ä¼°è®¡å€¾å‘è¯„åˆ†ï¼šP(ç”¨åˆ¸ | æ´»è·ƒåº¦, å†å²æ¶ˆè´¹)")
    print("  2. ä¸ºæ¯ä¸ªç”¨åˆ¸ç”¨æˆ·æ‰¾æœªç”¨åˆ¸çš„'ç›¸ä¼¼ç”¨æˆ·'ï¼ˆ1:1 åŒ¹é…ï¼‰")
    print("  3. è®¡ç®—åŒ¹é…åçš„æ¶ˆè´¹å·®å¼‚ï¼ˆATTï¼‰")

    # ========== ç¬¬ 1 æ­¥ï¼šä¼°è®¡å€¾å‘è¯„åˆ† ==========
    print("\nç¬¬ 1 æ­¥ï¼šä¼°è®¡å€¾å‘è¯„åˆ†...")
    print("-" * 70)

    confounders = ['ç”¨æˆ·æ´»è·ƒåº¦', 'å†å²æ¶ˆè´¹']
    treatment = 'ä¼˜æƒ åˆ¸ä½¿ç”¨'

    ps_model = LogisticRegression(random_state=42)
    ps_model.fit(df[confounders], df[treatment])

    df['propensity_score'] = ps_model.predict_proba(df[confounders])[:, 1]

    print(f"å€¾å‘è¯„åˆ†æ¨¡å‹:")
    print(f"  ç‰¹å¾: {confounders}")
    print(f"  ç®—æ³•: Logistic Regression")
    print(f"  å¹³å‡å€¾å‘è¯„åˆ†: {df['propensity_score'].mean():.3f}")

    # ========== ç¬¬ 2 æ­¥ï¼šåŒ¹é… ==========
    print("\nç¬¬ 2 æ­¥ï¼šåŒ¹é…ï¼ˆ1:1 æœ€è¿‘é‚»ï¼‰...")
    print("-" * 70)

    treated = df[df[treatment] == 1].copy()
    control = df[df[treatment] == 0].copy()

    print(f"å¤„ç†ç»„ï¼ˆç”¨åˆ¸ï¼‰: {len(treated)} äºº")
    print(f"å¯¹ç…§ç»„ï¼ˆæœªç”¨åˆ¸ï¼‰: {len(control)} äºº")

    # 1:1 æœ€è¿‘é‚»åŒ¹é…
    nn = NearestNeighbors(n_neighbors=1)
    nn.fit(control[['propensity_score']])

    distances, indices = nn.kneighbors(treated[['propensity_score']])
    matched_control = control.iloc[indices.flatten()].copy()

    # è®¡ç®—åŒ¹é…è´¨é‡
    mean_distance = distances.mean()

    print(f"\nåŒ¹é…è´¨é‡:")
    print(f"  å¹³å‡å€¾å‘è¯„åˆ†è·ç¦»: {mean_distance:.4f}")
    print(f"  åŒ¹é…æˆåŠŸ: {len(matched_control)} å¯¹")

    # ========== ç¬¬ 3 æ­¥ï¼šè®¡ç®— ATT ==========
    print("\nç¬¬ 3 æ­¥ï¼šè®¡ç®— ATTï¼ˆå¤„ç†ç»„å¹³å‡å¤„ç†æ•ˆåº”ï¼‰...")
    print("-" * 70)

    treated_outcome = treated['æ¶ˆè´¹é‡‘é¢'].values
    control_outcome = matched_control['æ¶ˆè´¹é‡‘é¢'].values

    att = (treated_outcome - control_outcome).mean()

    # Bootstrap ç½®ä¿¡åŒºé—´
    print("\nBootstrap 95% CI (500 æ¬¡é‡é‡‡æ ·)...")

    n_boot = 500
    att_samples = []

    for i in range(n_boot):
        # é‡é‡‡æ ·
        treated_boot = treated.sample(n=len(treated), replace=True)
        control_boot = control.sample(n=len(control), replace=True)

        # é‡æ–°åŒ¹é…
        nn_boot = NearestNeighbors(n_neighbors=1)
        nn_boot.fit(control_boot[['propensity_score']])
        _, indices_boot = nn_boot.kneighbors(treated_boot[['propensity_score']])
        matched_boot = control_boot.iloc[indices_boot.flatten()]

        # è®¡ç®— ATT
        att_boot = (treated_boot['æ¶ˆè´¹é‡‘é¢'].values - matched_boot['æ¶ˆè´¹é‡‘é¢'].values).mean()
        att_samples.append(att_boot)

    att_ci_low = np.percentile(att_samples, 2.5)
    att_ci_high = np.percentile(att_samples, 97.5)

    print(f"\nå› æœæ•ˆåº”ä¼°è®¡:")
    print(f"  ATTï¼ˆå¤„ç†ç»„å¹³å‡å¤„ç†æ•ˆåº”ï¼‰: {att:.2f} å…ƒ")
    print(f"  95% CI (Bootstrap): [{att_ci_low:.2f}, {att_ci_high:.2f}]")

    # å¯¹æ¯”çœŸå®å€¼
    true_effect = 30
    print(f"\nå¯¹æ¯”çœŸå®å€¼:")
    print(f"  çœŸå®æ•ˆåº”: {true_effect:.2f} å…ƒ")
    print(f"  ä¼°è®¡è¯¯å·®: {abs(att - true_effect):.2f} å…ƒ")

    # ========== ç¬¬ 4 æ­¥ï¼šå¯è§†åŒ– ==========
    print("\nç¬¬ 4 æ­¥ï¼šä¿å­˜åŒ¹é…å‰åå¯è§†åŒ–...")

    plot_psm_comparison(
        treated['propensity_score'].values,
        control['propensity_score'].values,
        matched_control['propensity_score'].values,
        output_path='psm_comparison.png'
    )

    return {
        'method': 'å€¾å‘è¯„åˆ†åŒ¹é…ï¼ˆPSMï¼‰',
        'effect': att,
        'ci_low': att_ci_low,
        'ci_high': att_ci_high,
        'p_value': None  # PSM ä¸ç›´æ¥ç»™å‡º p å€¼
    }


def plot_psm_comparison(treated_ps, control_ps, matched_ps, output_path='psm_comparison.png'):
    """
    ç”»å€¾å‘è¯„åˆ†åŒ¹é…å‰åçš„å¯¹æ¯”å›¾
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # åŒ¹é…å‰
    axes[0].hist(treated_ps, alpha=0.5, label='ç”¨åˆ¸ï¼ˆå¤„ç†ç»„ï¼‰', bins=20, color='blue')
    axes[0].hist(control_ps, alpha=0.5, label='æœªç”¨åˆ¸ï¼ˆå¯¹ç…§ç»„ï¼‰', bins=20, color='red')
    axes[0].set_xlabel('å€¾å‘è¯„åˆ†', fontsize=12)
    axes[0].set_ylabel('äººæ•°', fontsize=12)
    axes[0].set_title('åŒ¹é…å‰ï¼šå€¾å‘è¯„åˆ†åˆ†å¸ƒå·®å¼‚å¤§', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)

    # åŒ¹é…å
    axes[1].hist(treated_ps, alpha=0.5, label='ç”¨åˆ¸ï¼ˆå¤„ç†ç»„ï¼‰', bins=20, color='blue')
    axes[1].hist(matched_ps, alpha=0.5, label='åŒ¹é…çš„æœªç”¨åˆ¸ï¼ˆå¯¹ç…§ç»„ï¼‰', bins=20, color='green')
    axes[1].set_xlabel('å€¾å‘è¯„åˆ†', fontsize=12)
    axes[1].set_ylabel('äººæ•°', fontsize=12)
    axes[1].set_title('åŒ¹é…åï¼šå€¾å‘è¯„åˆ†åˆ†å¸ƒæ¥è¿‘', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=11)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"  âœ… å›¾å·²ä¿å­˜: {output_path}")


def compare_methods(result_reg, result_psm):
    """
    å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„ç»“æœ
    """
    print("\n" + "=" * 70)
    print("ğŸ“‹ ä¸¤ç§æ–¹æ³•çš„å¯¹æ¯”")
    print("=" * 70)

    print(f"\n{'æ–¹æ³•':<20} {'ä¼°è®¡å€¼':<12} {'95% CI':<20} {'ç»“è®º'}")
    print("-" * 70)

    print(f"{result_reg['method']:<20} "
          f"{result_reg['effect']:>8.2f} å…ƒ  "
          f"[{result_reg['ci_low']:.2f}, {result_reg['ci_high']:.2f}]  "
          f"{'æ˜¾è‘—' if result_reg['p_value'] < 0.05 else 'ä¸æ˜¾è‘—'}")

    print(f"{result_psm['method']:<20} "
          f"{result_psm['effect']:8.2f} å…ƒ  "
          f"[{result_psm['ci_low']:.2f}, {result_psm['ci_high']:.2f}]  "
          f"Bootstrap")

    # ä¸€è‡´æ€§æ£€æŸ¥
    effect_diff = abs(result_reg['effect'] - result_psm['effect'])
    ci_overlap = not (result_reg['ci_high'] < result_psm['ci_low'] or
                     result_psm['ci_high'] < result_reg['ci_low'])

    print("\nä¸€è‡´æ€§è¯„ä¼°:")
    print(f"  ä¼°è®¡å€¼å·®å¼‚: {effect_diff:.2f} å…ƒ")
    print(f"  ç½®ä¿¡åŒºé—´é‡å : {'âœ… æ˜¯' if ci_overlap else 'âŒ å¦'}")

    if effect_diff < 5 and ci_overlap:
        print(f"\nâœ… ä¸¤ç§æ–¹æ³•ç»“æœæ¥è¿‘ï¼Œç»“è®ºç¨³å¥ï¼")
    else:
        print(f"\nâš ï¸  ä¸¤ç§æ–¹æ³•å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦æ£€æŸ¥å‡è®¾")

    # å®è·µå»ºè®®
    print("\nå®è·µå»ºè®®:")
    print("-" * 70)
    print("1. å…ˆç”¨å›å½’ï¼ˆå¿«é€Ÿå¾—åˆ°åŸºçº¿ï¼‰")
    print("2. å†ç”¨åŒ¹é…ï¼ˆæ£€æŸ¥ç¨³å¥æ€§ï¼‰")
    print("3. å¦‚æœä¸¤è€…æ¥è¿‘ï¼Œç»“è®ºå¯é ")
    print("4. å¦‚æœå·®å¼‚å¤§ï¼Œæ£€æŸ¥å‡è®¾ï¼ˆæ¨¡å‹å½¢å¼ã€åŒ¹é…è´¨é‡ï¼‰")


def bad_example_naive_comparison(df):
    """
    åä¾‹ï¼šç®€å•å‡å€¼æ¯”è¾ƒï¼ˆå°åŒ—çš„é”™è¯¯ï¼‰
    """
    print("\n" + "=" * 70)
    print("âŒ åä¾‹ï¼šå°åŒ—çš„é”™è¯¯â€”â€”ç®€å•å‡å€¼æ¯”è¾ƒ")
    print("=" * 70)

    print("\nå°åŒ—çš„åšæ³•:")
    print("  ç›´æ¥æ¯”è¾ƒç”¨åˆ¸å’Œæœªç”¨åˆ¸ç”¨æˆ·çš„å¹³å‡æ¶ˆè´¹")

    treated_mean = df[df['ä¼˜æƒ åˆ¸ä½¿ç”¨'] == 1]['æ¶ˆè´¹é‡‘é¢'].mean()
    control_mean = df[df['ä¼˜æƒ åˆ¸ä½¿ç”¨'] == 0]['æ¶ˆè´¹é‡‘é¢'].mean()
    naive_effect = treated_mean - control_mean

    print(f"\nç»“æœ:")
    print(f"  ç”¨åˆ¸ç”¨æˆ·å¹³å‡æ¶ˆè´¹: {treated_mean:.2f} å…ƒ")
    print(f"  æœªç”¨åˆ¸ç”¨æˆ·å¹³å‡æ¶ˆè´¹: {control_mean:.2f} å…ƒ")
    print(f"  å·®å¼‚: {naive_effect:.2f} å…ƒ")

    print(f"\nâš ï¸ é—®é¢˜:")
    print(f"  çœŸå®å› æœæ•ˆåº”: 30.00 å…ƒ")
    print(f"  å°åŒ—çš„ä¼°è®¡: {naive_effect:.2f} å…ƒ")
    print(f"  æ··æ‚åå·®: {naive_effect - 30:.2f} å…ƒ")

    print(f"\nåŸå› :")
    print(f"  æ´»è·ƒç”¨æˆ·æ—¢æ›´å¯èƒ½ç”¨åˆ¸ï¼Œä¹Ÿæ¶ˆè´¹æ›´é«˜")
    print(f"  ä¸è°ƒæ•´æ´»è·ƒåº¦ï¼Œä¼šæŠŠæ´»è·ƒåº¦çš„æ•ˆåº”å½’åŠŸäºä¼˜æƒ åˆ¸")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("å› æœæ•ˆåº”ä¼°è®¡ï¼šå›å½’ vs å€¾å‘è¯„åˆ†åŒ¹é…")
    print("=" * 70)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("report")
    output_dir.mkdir(exist_ok=True)

    # ç”Ÿæˆæ•°æ®
    print("\nğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
    df = generate_coupon_data(n=1000, seed=42)

    print(f"æ•°æ®è§„æ¨¡: {len(df)} ç”¨æˆ·")
    print(f"ç”¨åˆ¸æ¯”ä¾‹: {df['ä¼˜æƒ åˆ¸ä½¿ç”¨'].mean():.1%}")
    print(f"å¹³å‡æ¶ˆè´¹: {df['æ¶ˆè´¹é‡‘é¢'].mean():.2f} å…ƒ")

    # åä¾‹ï¼šå°åŒ—çš„é”™è¯¯
    bad_example_naive_comparison(df)

    # æ–¹æ³• 1ï¼šå›å½’
    result_reg = method_1_regression_adjustment(df)

    # æ–¹æ³• 2ï¼šå€¾å‘è¯„åˆ†åŒ¹é…
    result_psm = method_2_psm(df)

    # å¯¹æ¯”
    compare_methods(result_reg, result_psm)

    print("\n" + "=" * 70)
    print("ğŸ’¡ å…³é”®è¦ç‚¹")
    print("=" * 70)
    print("""
1. å›å½’ï¼ˆå¸¦è°ƒæ•´é›†ï¼‰:
   - ç®€å•ã€å¿«é€Ÿã€å¯ç”¨æ ‡å‡†è¯¯å·®
   - å‡è®¾çº¿æ€§ã€å®¹æ˜“æ¨¡å‹é”™è®¾
   - é€‚ç”¨äºæ··æ‚å˜é‡å°‘ã€å…³ç³»ç®€å•çš„åœºæ™¯

2. å€¾å‘è¯„åˆ†åŒ¹é…:
   - ä¸å‡è®¾çº¿æ€§ã€å¯è§†åŒ–å¼ºã€ç›´è§‰æ¸…æ™°
   - ä¸¢å¼ƒæ— æ³•åŒ¹é…çš„æ ·æœ¬ã€æ•ˆç‡ä½
   - é€‚ç”¨äºéçº¿æ€§å…³ç³»ã€éœ€è¦å¯æ¯”æ€§æ£€æŸ¥çš„åœºæ™¯

3. å®è·µå»ºè®®:
   - å…ˆç”¨å›å½’ï¼ˆå¿«é€Ÿå¾—åˆ°åŸºçº¿ï¼‰
   - å†ç”¨åŒ¹é…ï¼ˆæ£€æŸ¥ç¨³å¥æ€§ï¼‰
   - å¦‚æœä¸¤è€…æ¥è¿‘ï¼Œç»“è®ºå¯é 

4. å°åŒ—çš„é”™è¯¯:
   - ç›´æ¥æ¯”è¾ƒå‡å€¼ï¼ˆæœªè°ƒæ•´æ··æ‚ï¼‰
   - ç»“æœè¢«å¤¸å¤§ï¼ˆ50 vs 30 å…ƒï¼‰
   - æ­£ç¡®åšæ³•ï¼šå…ˆç”»å› æœå›¾ï¼Œç”¨åé—¨å‡†åˆ™é€‰æ‹©è°ƒæ•´é›†
    """)


if __name__ == "__main__":
    main()
